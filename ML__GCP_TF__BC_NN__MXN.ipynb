{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning (ML): Google Cloud Platform (GCP): TensorFlow (TF): Financial Time-Series\n",
    "# Model 01 = Binary Classifier\n",
    "# Model 02 = Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import StringIO\n",
    "\n",
    "import pandas as pd\n",
    "from pandas.tools.plotting import autocorrelation_plot\n",
    "from pandas.tools.plotting import scatter_matrix\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import gcp\n",
    "import gcp.bigquery as bq\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from IPython.core.display import HTML\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Install stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\r\n",
      "  File \"/usr/local/bin/pip\", line 9, in <module>\r\n",
      "    load_entry_point('pip==1.5.6', 'console_scripts', 'pip')()\r\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/pkg_resources/__init__.py\", line 542, in load_entry_point\r\n",
      "    return get_distribution(dist).load_entry_point(group, name)\r\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/pkg_resources/__init__.py\", line 2569, in load_entry_point\r\n",
      "    return ep.load()\r\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/pkg_resources/__init__.py\", line 2229, in load\r\n",
      "    return self.resolve()\r\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/pkg_resources/__init__.py\", line 2235, in resolve\r\n",
      "    module = __import__(self.module_name, fromlist=['__name__'], level=0)\r\n",
      "  File \"/usr/lib/python2.7/dist-packages/pip/__init__.py\", line 74, in <module>\r\n",
      "    from pip.vcs import git, mercurial, subversion, bazaar  # noqa\r\n",
      "  File \"/usr/lib/python2.7/dist-packages/pip/vcs/mercurial.py\", line 9, in <module>\r\n",
      "    from pip.download import path_to_url\r\n",
      "  File \"/usr/lib/python2.7/dist-packages/pip/download.py\", line 25, in <module>\r\n",
      "    from requests.compat import IncompleteRead\r\n",
      "ImportError: cannot import name IncompleteRead\r\n"
     ]
    }
   ],
   "source": [
    "# Install xlrd - required for reading excel files\n",
    "# !pip install xlrd\n",
    "!pip install xlrd\n",
    "# -----------------------------------------------------------------------------------------------\n",
    "# Downloading/unpacking xlrd\n",
    "#   Downloading xlrd-1.0.0.tar.gz (2.6MB): 2.6MB downloaded\n",
    "#   Running setup.py (path:/tmp/pip-build-brL6Op/xlrd/setup.py) egg_info for package xlrd\n",
    "    \n",
    "#     warning: no files found matching 'README.html'\n",
    "# Installing collected packages: xlrd\n",
    "#   Running setup.py install for xlrd\n",
    "#     changing mode of build/scripts-2.7/runxlrd.py from 644 to 755\n",
    "    \n",
    "#     warning: no files found matching 'README.html'\n",
    "#     changing mode of /usr/local/bin/runxlrd.py to 755\n",
    "#   Could not find .egg-info directory in install record for xlrd\n",
    "# Successfully installed xlrd\n",
    "# Cleaning up...\n",
    "# -----------------------------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\r\n",
      "  File \"/usr/local/bin/pip\", line 9, in <module>\r\n",
      "    load_entry_point('pip==1.5.6', 'console_scripts', 'pip')()\r\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/pkg_resources/__init__.py\", line 542, in load_entry_point\r\n",
      "    return get_distribution(dist).load_entry_point(group, name)\r\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/pkg_resources/__init__.py\", line 2569, in load_entry_point\r\n",
      "    return ep.load()\r\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/pkg_resources/__init__.py\", line 2229, in load\r\n",
      "    return self.resolve()\r\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/pkg_resources/__init__.py\", line 2235, in resolve\r\n",
      "    module = __import__(self.module_name, fromlist=['__name__'], level=0)\r\n",
      "  File \"/usr/lib/python2.7/dist-packages/pip/__init__.py\", line 74, in <module>\r\n",
      "    from pip.vcs import git, mercurial, subversion, bazaar  # noqa\r\n",
      "  File \"/usr/lib/python2.7/dist-packages/pip/vcs/mercurial.py\", line 9, in <module>\r\n",
      "    from pip.download import path_to_url\r\n",
      "  File \"/usr/lib/python2.7/dist-packages/pip/download.py\", line 25, in <module>\r\n",
      "    from requests.compat import IncompleteRead\r\n",
      "ImportError: cannot import name IncompleteRead\r\n"
     ]
    }
   ],
   "source": [
    "# Install dropbox - required for writing files to dropbox, not required for reading files from dropbox\n",
    "!pip install dropbox\n",
    "# -----------------------------------------------------------------------------------------------\n",
    "# Downloading/unpacking dropbox\n",
    "#   Downloading dropbox-6.6.2-py2-none-any.whl (261kB): 261kB downloaded\n",
    "# Requirement already satisfied (use --upgrade to upgrade): six>=1.3.0 in /usr/local/lib/python2.7/dist-packages (from dropbox)\n",
    "# Requirement already satisfied (use --upgrade to upgrade): urllib3 in /usr/lib/python2.7/dist-packages (from dropbox)\n",
    "# Downloading/unpacking requests!=2.6.1,>=2.5.1 (from dropbox)\n",
    "#   Downloading requests-2.11.0-py2.py3-none-any.whl (514kB): 514kB downloaded\n",
    "# Downloading/unpacking typing>=3.5.2 (from dropbox)\n",
    "#   Downloading typing-3.5.2.2.tar.gz (51kB): 51kB downloaded\n",
    "#   Running setup.py (path:/tmp/pip-build-xSBpJ2/typing/setup.py) egg_info for package typing\n",
    "    \n",
    "# Installing collected packages: dropbox, requests, typing\n",
    "#   Found existing installation: requests 2.4.3\n",
    "#     Not uninstalling requests at /usr/lib/python2.7/dist-packages, owned by OS\n",
    "#   Running setup.py install for typing\n",
    "    \n",
    "#   Could not find .egg-info directory in install record for typing>=3.5.2 (from dropbox)\n",
    "# Successfully installed dropbox requests typing\n",
    "# Cleaning up...\n",
    "# -----------------------------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Data File: From dropbox to GCP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* s_data_dropbox_file_path_name_ext = https://www.dropbox.com/s/foso5ckzl9vrui0/fx_mxn_data_raw.xlsm\n",
      "  * s_data_dropbox_file_path_name = https://www.dropbox.com/s/foso5ckzl9vrui0/fx_mxn_data_raw\n",
      "  * s_data_dropbox_file_path      = https://www.dropbox.com/s/foso5ckzl9vrui0\n",
      "  * s_data_dropbox_file_name      = fx_mxn_data_raw\n",
      "  * s_data_dropbox_file_ext       = .xlsm\n",
      "* force = 1\n",
      "* s_data_google_file_path_name_ext = fx_mxn_data_raw.xlsm\n",
      "===== Display Current Directory ========================================\n",
      "total 2892\n",
      "-rw-r--r-- 1 root root  144612 Aug 29 05:15 ML__GCP_TF__BC_NN__MXN.ipynb\n",
      "drwxr-xr-x 5 root root    4096 Aug 26 21:27 datalab\n",
      "-rw-r--r-- 1 root root 2807130 Aug 29 05:14 fx_mxn_data_raw.xlsm\n",
      "===== Check if file exists on GCP =============================================\n",
      "* already_downloaded = False\n",
      "===== Delete the current data file on GCP =============================================\n",
      "===== Get latest data file from dropbox to GCP ========================================\n",
      "converted 'https://www.dropbox.com/s/foso5ckzl9vrui0/fx_mxn_data_raw.xlsm' (ANSI_X3.4-1968) -> 'https://www.dropbox.com/s/foso5ckzl9vrui0/fx_mxn_data_raw.xlsm' (UTF-8)\n",
      "--2016-08-29 05:16:39--  https://www.dropbox.com/s/foso5ckzl9vrui0/fx_mxn_data_raw.xlsm\n",
      "Resolving www.dropbox.com (www.dropbox.com)... 108.160.172.238\n",
      "Connecting to www.dropbox.com (www.dropbox.com)|108.160.172.238|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://dl.dropboxusercontent.com/content_link/fepkdy8BKazxc06Ez9rjli3A6PxGCeJMMFQ4tiofPIu5iEanUb3GcT7ReNlFrDIE/file [following]\n",
      "converted 'https://dl.dropboxusercontent.com/content_link/fepkdy8BKazxc06Ez9rjli3A6PxGCeJMMFQ4tiofPIu5iEanUb3GcT7ReNlFrDIE/file' (ANSI_X3.4-1968) -> 'https://dl.dropboxusercontent.com/content_link/fepkdy8BKazxc06Ez9rjli3A6PxGCeJMMFQ4tiofPIu5iEanUb3GcT7ReNlFrDIE/file' (UTF-8)\n",
      "--2016-08-29 05:16:39--  https://dl.dropboxusercontent.com/content_link/fepkdy8BKazxc06Ez9rjli3A6PxGCeJMMFQ4tiofPIu5iEanUb3GcT7ReNlFrDIE/file\n",
      "Resolving dl.dropboxusercontent.com (dl.dropboxusercontent.com)... 45.58.69.101\n",
      "Connecting to dl.dropboxusercontent.com (dl.dropboxusercontent.com)|45.58.69.101|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 2807130 (2.7M) [application/vnd.ms-excel.sheet.macroEnabled.12]\n",
      "Saving to: 'fx_mxn_data_raw.xlsm'\n",
      "\n",
      "fx_mxn_data_raw.xls 100%[=====================>]   2.68M  9.34MB/s   in 0.3s   \n",
      "\n",
      "2016-08-29 05:16:40 (9.34 MB/s) - 'fx_mxn_data_raw.xlsm' saved [2807130/2807130]\n",
      "\n",
      "===== Display Current Directory ========================================\n",
      "total 2892\n",
      "-rw-r--r-- 1 root root  144612 Aug 29 05:15 ML__GCP_TF__BC_NN__MXN.ipynb\n",
      "drwxr-xr-x 5 root root    4096 Aug 26 21:27 datalab\n",
      "-rw-r--r-- 1 root root 2807130 Aug 29 05:16 fx_mxn_data_raw.xlsm\n"
     ]
    }
   ],
   "source": [
    "# Save file on dropbox, and put the URL here\n",
    "s_data_dropbox_file_path_name_ext = 'https://www.dropbox.com/s/foso5ckzl9vrui0/fx_mxn_data_raw.xlsm'\n",
    "print '* s_data_dropbox_file_path_name_ext = ' + s_data_dropbox_file_path_name_ext\n",
    "\n",
    "# Store file name meta data\n",
    "import os\n",
    "s_data_dropbox_file_path_name, s_data_dropbox_file_ext  = os.path.splitext(s_data_dropbox_file_path_name_ext)\n",
    "s_data_dropbox_file_path,      s_data_dropbox_file_name = os.path.split   (s_data_dropbox_file_path_name)\n",
    "print '  * s_data_dropbox_file_path_name = ' + s_data_dropbox_file_path_name\n",
    "print '  * s_data_dropbox_file_path      = ' + s_data_dropbox_file_path\n",
    "print '  * s_data_dropbox_file_name      = ' + s_data_dropbox_file_name\n",
    "print '  * s_data_dropbox_file_ext       = ' + s_data_dropbox_file_ext\n",
    "\n",
    "# Select force = 0 or 1\n",
    "# force = 0 => Just use existing data on GCP\n",
    "# force = 1 => Always grab from dropbox and copy to GCP, overwriting exisiting if any.\n",
    "force = 1\n",
    "print '* force = ' + str(force)\n",
    "\n",
    "# # Import file using url to file on dropbox\n",
    "from urlparse import urlparse\n",
    "from os.path import basename\n",
    "\n",
    "s_data_google_file_path_name_ext = basename(urlparse(s_data_dropbox_file_path_name_ext).path)\n",
    "print '* s_data_google_file_path_name_ext = ' + s_data_google_file_path_name_ext\n",
    "\n",
    "print '===== Display Current Directory ========================================'\n",
    "!ls -l  \n",
    "\n",
    "print '===== Check if file exists on GCP ============================================='\n",
    "try:\n",
    "    already_downloaded\n",
    "except:\n",
    "    already_downloaded = False\n",
    "print '* already_downloaded = ' + str(already_downloaded)\n",
    "    \n",
    "if not already_downloaded or force:    \n",
    "    print '===== Delete the current data file on GCP ============================================='\n",
    "    !rm $s_data_google_file_path_name_ext    # Delete the current data file on GCP\n",
    "    print '===== Get latest data file from dropbox to GCP ========================================'\n",
    "    !wget $s_data_dropbox_file_path_name_ext # Get latest data file from dropbox to GCP\n",
    "    already_downloaded = True\n",
    "    \n",
    "print '===== Display Current Directory ========================================'\n",
    "!ls -l  \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Read the Excel file raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>77</th>\n",
       "      <th>78</th>\n",
       "      <th>79</th>\n",
       "      <th>80</th>\n",
       "      <th>81</th>\n",
       "      <th>82</th>\n",
       "      <th>83</th>\n",
       "      <th>84</th>\n",
       "      <th>85</th>\n",
       "      <th>86</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ticker</td>\n",
       "      <td>USDMXN Curncy</td>\n",
       "      <td>ASURB MM Equity</td>\n",
       "      <td>BIMBOA MM Equity</td>\n",
       "      <td>CN1 Comdty</td>\n",
       "      <td>CT350188 Curncy</td>\n",
       "      <td>DU1 Comdty</td>\n",
       "      <td>ED749713@BVAL Corp</td>\n",
       "      <td>EMRUEMRU Index</td>\n",
       "      <td>EURMXN3Y Curncy</td>\n",
       "      <td>...</td>\n",
       "      <td>USSA25 ICPL Curncy</td>\n",
       "      <td>USSA30 ICPL Curncy</td>\n",
       "      <td>USSP15 CMPN Curncy</td>\n",
       "      <td>USSP20 CMPN Curncy</td>\n",
       "      <td>USSP25 CMPN Curncy</td>\n",
       "      <td>USSW15 CMPN Curncy</td>\n",
       "      <td>USSW20 CMPN Curncy</td>\n",
       "      <td>USSW25 CMPN Curncy</td>\n",
       "      <td>USSW30 CMPN Curncy</td>\n",
       "      <td>XQ1 Comdty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Date</td>\n",
       "      <td>USDMXN Curncy</td>\n",
       "      <td>ASURB MM Equity</td>\n",
       "      <td>BIMBOA MM Equity</td>\n",
       "      <td>CN1 Comdty</td>\n",
       "      <td>CT350188 Curncy</td>\n",
       "      <td>DU1 Comdty</td>\n",
       "      <td>ED749713@BVAL Corp</td>\n",
       "      <td>EMRUEMRU Index</td>\n",
       "      <td>EURMXN3Y Curncy</td>\n",
       "      <td>...</td>\n",
       "      <td>USSA25 ICPL Curncy</td>\n",
       "      <td>USSA30 ICPL Curncy</td>\n",
       "      <td>USSP15 CMPN Curncy</td>\n",
       "      <td>USSP20 CMPN Curncy</td>\n",
       "      <td>USSP25 CMPN Curncy</td>\n",
       "      <td>USSW15 CMPN Curncy</td>\n",
       "      <td>USSW20 CMPN Curncy</td>\n",
       "      <td>USSW25 CMPN Curncy</td>\n",
       "      <td>USSW30 CMPN Curncy</td>\n",
       "      <td>XQ1 Comdty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016-05-27 00:00:00</td>\n",
       "      <td>18.4751</td>\n",
       "      <td>289.95</td>\n",
       "      <td>55.4</td>\n",
       "      <td>141.28</td>\n",
       "      <td>424.14</td>\n",
       "      <td>111.85</td>\n",
       "      <td>118.813</td>\n",
       "      <td>66.0342</td>\n",
       "      <td>35645.2</td>\n",
       "      <td>...</td>\n",
       "      <td>2.102</td>\n",
       "      <td>2.138</td>\n",
       "      <td>-10.63</td>\n",
       "      <td>-18</td>\n",
       "      <td>-31.63</td>\n",
       "      <td>1.943</td>\n",
       "      <td>2.074</td>\n",
       "      <td>2.133</td>\n",
       "      <td>2.1738</td>\n",
       "      <td>124.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016-05-26 00:00:00</td>\n",
       "      <td>18.4544</td>\n",
       "      <td>284.68</td>\n",
       "      <td>55.29</td>\n",
       "      <td>141.83</td>\n",
       "      <td>427.999</td>\n",
       "      <td>111.845</td>\n",
       "      <td>118.75</td>\n",
       "      <td>65.282</td>\n",
       "      <td>35316.9</td>\n",
       "      <td>...</td>\n",
       "      <td>2.104</td>\n",
       "      <td>2.139</td>\n",
       "      <td>-10.46</td>\n",
       "      <td>-17.65</td>\n",
       "      <td>-31.43</td>\n",
       "      <td>1.928</td>\n",
       "      <td>2.0575</td>\n",
       "      <td>2.125</td>\n",
       "      <td>2.1598</td>\n",
       "      <td>124.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016-05-25 00:00:00</td>\n",
       "      <td>18.4852</td>\n",
       "      <td>281.4</td>\n",
       "      <td>56.33</td>\n",
       "      <td>140.98</td>\n",
       "      <td>5392.86</td>\n",
       "      <td>111.825</td>\n",
       "      <td>118.438</td>\n",
       "      <td>65.9242</td>\n",
       "      <td>35680.2</td>\n",
       "      <td>...</td>\n",
       "      <td>2.107</td>\n",
       "      <td>2.14</td>\n",
       "      <td>-11.11</td>\n",
       "      <td>-18.4</td>\n",
       "      <td>-32.03</td>\n",
       "      <td>1.955</td>\n",
       "      <td>2.0825</td>\n",
       "      <td>2.1453</td>\n",
       "      <td>2.1785</td>\n",
       "      <td>124.11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 87 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    0              1                2                 3   \\\n",
       "0               Ticker  USDMXN Curncy  ASURB MM Equity  BIMBOA MM Equity   \n",
       "1                 Date  USDMXN Curncy  ASURB MM Equity  BIMBOA MM Equity   \n",
       "2  2016-05-27 00:00:00        18.4751           289.95              55.4   \n",
       "3  2016-05-26 00:00:00        18.4544           284.68             55.29   \n",
       "4  2016-05-25 00:00:00        18.4852            281.4             56.33   \n",
       "\n",
       "           4                5           6                   7   \\\n",
       "0  CN1 Comdty  CT350188 Curncy  DU1 Comdty  ED749713@BVAL Corp   \n",
       "1  CN1 Comdty  CT350188 Curncy  DU1 Comdty  ED749713@BVAL Corp   \n",
       "2      141.28           424.14      111.85             118.813   \n",
       "3      141.83          427.999     111.845              118.75   \n",
       "4      140.98          5392.86     111.825             118.438   \n",
       "\n",
       "               8                9      ...                      77  \\\n",
       "0  EMRUEMRU Index  EURMXN3Y Curncy     ...      USSA25 ICPL Curncy   \n",
       "1  EMRUEMRU Index  EURMXN3Y Curncy     ...      USSA25 ICPL Curncy   \n",
       "2         66.0342          35645.2     ...                   2.102   \n",
       "3          65.282          35316.9     ...                   2.104   \n",
       "4         65.9242          35680.2     ...                   2.107   \n",
       "\n",
       "                   78                  79                  80  \\\n",
       "0  USSA30 ICPL Curncy  USSP15 CMPN Curncy  USSP20 CMPN Curncy   \n",
       "1  USSA30 ICPL Curncy  USSP15 CMPN Curncy  USSP20 CMPN Curncy   \n",
       "2               2.138              -10.63                 -18   \n",
       "3               2.139              -10.46              -17.65   \n",
       "4                2.14              -11.11               -18.4   \n",
       "\n",
       "                   81                  82                  83  \\\n",
       "0  USSP25 CMPN Curncy  USSW15 CMPN Curncy  USSW20 CMPN Curncy   \n",
       "1  USSP25 CMPN Curncy  USSW15 CMPN Curncy  USSW20 CMPN Curncy   \n",
       "2              -31.63               1.943               2.074   \n",
       "3              -31.43               1.928              2.0575   \n",
       "4              -32.03               1.955              2.0825   \n",
       "\n",
       "                   84                  85          86  \n",
       "0  USSW25 CMPN Curncy  USSW30 CMPN Curncy  XQ1 Comdty  \n",
       "1  USSW25 CMPN Curncy  USSW30 CMPN Curncy  XQ1 Comdty  \n",
       "2               2.133              2.1738      124.13  \n",
       "3               2.125              2.1598       124.3  \n",
       "4              2.1453              2.1785      124.11  \n",
       "\n",
       "[5 rows x 87 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import xlrd\n",
    "\n",
    "# Read the excel data that is on GCP\n",
    "df = pd.read_excel(s_data_google_file_path_name_ext, 'data_raw')\n",
    "# data = pd.read_excel(bn_data_raw, 'data_raw', header=0, index_col=0, parse_cols=None)\n",
    "  # parse_cols=None: parse all columns\n",
    "  # header=0:    sets the row 0 as col labels (ie. headers)\n",
    "  # index_col=0: sets the col 0 as row labels (ie. index)\n",
    "df.head() # display the first few lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>77</th>\n",
       "      <th>78</th>\n",
       "      <th>79</th>\n",
       "      <th>80</th>\n",
       "      <th>81</th>\n",
       "      <th>82</th>\n",
       "      <th>83</th>\n",
       "      <th>84</th>\n",
       "      <th>85</th>\n",
       "      <th>86</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4277</th>\n",
       "      <td>2000-01-07 00:00:00</td>\n",
       "      <td>9.565</td>\n",
       "      <td>14.23</td>\n",
       "      <td>4.83</td>\n",
       "      <td>117.45</td>\n",
       "      <td>382.997</td>\n",
       "      <td>102.71</td>\n",
       "      <td>132.35</td>\n",
       "      <td>27.321</td>\n",
       "      <td>22485</td>\n",
       "      <td>...</td>\n",
       "      <td>7.475</td>\n",
       "      <td>7.465</td>\n",
       "      <td>87</td>\n",
       "      <td>89</td>\n",
       "      <td>84</td>\n",
       "      <td>7.39</td>\n",
       "      <td>7.41</td>\n",
       "      <td>6.461</td>\n",
       "      <td>7.4</td>\n",
       "      <td>98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4278</th>\n",
       "      <td>2000-01-06 00:00:00</td>\n",
       "      <td>9.58</td>\n",
       "      <td>14.23</td>\n",
       "      <td>4.68</td>\n",
       "      <td>117.09</td>\n",
       "      <td>382.997</td>\n",
       "      <td>102.5</td>\n",
       "      <td>132.35</td>\n",
       "      <td>27.321</td>\n",
       "      <td>22485</td>\n",
       "      <td>...</td>\n",
       "      <td>7.505</td>\n",
       "      <td>7.48</td>\n",
       "      <td>89</td>\n",
       "      <td>90</td>\n",
       "      <td>84</td>\n",
       "      <td>7.43</td>\n",
       "      <td>7.44</td>\n",
       "      <td>6.461</td>\n",
       "      <td>7.43</td>\n",
       "      <td>97.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4279</th>\n",
       "      <td>2000-01-05 00:00:00</td>\n",
       "      <td>9.571</td>\n",
       "      <td>14.23</td>\n",
       "      <td>4.83</td>\n",
       "      <td>116.57</td>\n",
       "      <td>382.997</td>\n",
       "      <td>102.49</td>\n",
       "      <td>132.35</td>\n",
       "      <td>27.321</td>\n",
       "      <td>22485</td>\n",
       "      <td>...</td>\n",
       "      <td>7.475</td>\n",
       "      <td>7.455</td>\n",
       "      <td>88</td>\n",
       "      <td>89</td>\n",
       "      <td>84</td>\n",
       "      <td>7.47</td>\n",
       "      <td>7.48</td>\n",
       "      <td>6.461</td>\n",
       "      <td>7.46</td>\n",
       "      <td>97.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4280</th>\n",
       "      <td>2000-01-04 00:00:00</td>\n",
       "      <td>9.5713</td>\n",
       "      <td>14.23</td>\n",
       "      <td>4.83</td>\n",
       "      <td>117.5</td>\n",
       "      <td>382.997</td>\n",
       "      <td>102.49</td>\n",
       "      <td>132.35</td>\n",
       "      <td>27.54</td>\n",
       "      <td>22485</td>\n",
       "      <td>...</td>\n",
       "      <td>7.425</td>\n",
       "      <td>7.425</td>\n",
       "      <td>90</td>\n",
       "      <td>91</td>\n",
       "      <td>84</td>\n",
       "      <td>7.4</td>\n",
       "      <td>7.41</td>\n",
       "      <td>6.461</td>\n",
       "      <td>7.4</td>\n",
       "      <td>98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4281</th>\n",
       "      <td>2000-01-03 00:00:00</td>\n",
       "      <td>9.505</td>\n",
       "      <td>14.23</td>\n",
       "      <td>5.03</td>\n",
       "      <td>118.5</td>\n",
       "      <td>382.997</td>\n",
       "      <td>102.47</td>\n",
       "      <td>132.35</td>\n",
       "      <td>27.54</td>\n",
       "      <td>22485</td>\n",
       "      <td>...</td>\n",
       "      <td>7.455</td>\n",
       "      <td>7.425</td>\n",
       "      <td>86</td>\n",
       "      <td>87</td>\n",
       "      <td>84</td>\n",
       "      <td>7.46</td>\n",
       "      <td>7.47</td>\n",
       "      <td>6.461</td>\n",
       "      <td>7.44</td>\n",
       "      <td>98.56</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 87 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       0       1      2     3       4        5       6   \\\n",
       "4277  2000-01-07 00:00:00   9.565  14.23  4.83  117.45  382.997  102.71   \n",
       "4278  2000-01-06 00:00:00    9.58  14.23  4.68  117.09  382.997   102.5   \n",
       "4279  2000-01-05 00:00:00   9.571  14.23  4.83  116.57  382.997  102.49   \n",
       "4280  2000-01-04 00:00:00  9.5713  14.23  4.83   117.5  382.997  102.49   \n",
       "4281  2000-01-03 00:00:00   9.505  14.23  5.03   118.5  382.997  102.47   \n",
       "\n",
       "          7       8      9   ...       77     78  79  80  81    82    83  \\\n",
       "4277  132.35  27.321  22485  ...    7.475  7.465  87  89  84  7.39  7.41   \n",
       "4278  132.35  27.321  22485  ...    7.505   7.48  89  90  84  7.43  7.44   \n",
       "4279  132.35  27.321  22485  ...    7.475  7.455  88  89  84  7.47  7.48   \n",
       "4280  132.35   27.54  22485  ...    7.425  7.425  90  91  84   7.4  7.41   \n",
       "4281  132.35   27.54  22485  ...    7.455  7.425  86  87  84  7.46  7.47   \n",
       "\n",
       "         84    85     86  \n",
       "4277  6.461   7.4     98  \n",
       "4278  6.461  7.43   97.7  \n",
       "4279  6.461  7.46  97.55  \n",
       "4280  6.461   7.4     98  \n",
       "4281  6.461  7.44  98.56  \n",
       "\n",
       "[5 rows x 87 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store the number of columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* df_num_rows=4282\n",
      "* df_num_cols=87\n",
      "  * df_num_cols_Date = 1\n",
      "  * df_num_cols_XTp0 = 1\n",
      "  * df_num_cols_var  = 85\n"
     ]
    }
   ],
   "source": [
    "[df_num_rows, df_num_cols] = df.shape\n",
    "print '* df_num_rows='+str(df_num_rows)\n",
    "print '* df_num_cols='+str(df_num_cols)\n",
    "\n",
    "df_num_cols_Date = 1\n",
    "df_num_cols_XTp0 = 1\n",
    "df_num_cols_var = df_num_cols - df_num_cols_Date - df_num_cols_XTp0\n",
    "print '  * df_num_cols_Date = ' + str(df_num_cols_Date)\n",
    "print '  * df_num_cols_XTp0 = ' + str(df_num_cols_XTp0)\n",
    "print '  * df_num_cols_var  = ' + str(df_num_cols_var)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "04282\n",
      "00087\n"
     ]
    }
   ],
   "source": [
    "s_rows = '{0:05d}'.format(df_num_rows)\n",
    "print s_rows\n",
    "s_cols = '{0:05d}'.format(df_num_cols)\n",
    "print s_cols\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1) Write the Excel file back to dropbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Include the Dropbox SDK\n",
    "import dropbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:2: DeprecationWarning: You are using a deprecated client. Please use the new v2 client located at dropbox.Dropbox.\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linked account:  {u'referral_link': u'https://db.tt/wTxYqge4', u'display_name': u'Eyup Saltik', u'uid': 576395988, u'locale': u'en', u'email_verified': True, u'email': u'eyup.saltik.2016@gmail.com', u'is_paired': False, u'team': None, u'name_details': {u'familiar_name': u'Eyup', u'surname': u'Saltik', u'given_name': u'Eyup'}, u'country': u'SG', u'quota_info': {u'datastores': 0, u'shared': 0, u'quota': 2147483648, u'normal': 48507221}}\n"
     ]
    }
   ],
   "source": [
    "s_dropbox_access_token = 'Z0hHmWIPD4AAAAAAAAAADKBYofFVGTUnVcva8HKUpKCj6SDaY15WNHIZcRcEknoO'\n",
    "client = dropbox.client.DropboxClient(s_dropbox_access_token)\n",
    "print 'linked account: ', client.account_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uploaded: {u'icon': u'page_white_excel', u'bytes': 2672933, u'thumb_exists': False, u'rev': u'894a1e9d47', u'modified': u'Mon, 29 Aug 2016 05:14:11 +0000', u'shareable': False, u'client_mtime': u'Mon, 29 Aug 2016 05:14:11 +0000', u'path': u'/fx_mxn_data_raw_from_google_r04282_c00087_.csv', u'is_dir': False, u'size': u'2.5 MB', u'root': u'dropbox', u'mime_type': u'text/csv', u'revision': 137}\n"
     ]
    }
   ],
   "source": [
    "s_data_dropbox_file_path_name_ext = '/' + s_data_dropbox_file_name + '_from_google_' + 'r' + s_rows + '_' + 'c' + s_cols + '_' + '.csv'\n",
    "response = client.put_file(s_data_dropbox_file_path_name_ext, str(df.to_csv()), overwrite=True)\n",
    "print \"uploaded:\", response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Select data df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>77</th>\n",
       "      <th>78</th>\n",
       "      <th>79</th>\n",
       "      <th>80</th>\n",
       "      <th>81</th>\n",
       "      <th>82</th>\n",
       "      <th>83</th>\n",
       "      <th>84</th>\n",
       "      <th>85</th>\n",
       "      <th>86</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ticker</td>\n",
       "      <td>USDMXN Curncy</td>\n",
       "      <td>ASURB MM Equity</td>\n",
       "      <td>BIMBOA MM Equity</td>\n",
       "      <td>CN1 Comdty</td>\n",
       "      <td>CT350188 Curncy</td>\n",
       "      <td>DU1 Comdty</td>\n",
       "      <td>ED749713@BVAL Corp</td>\n",
       "      <td>EMRUEMRU Index</td>\n",
       "      <td>EURMXN3Y Curncy</td>\n",
       "      <td>...</td>\n",
       "      <td>USSA25 ICPL Curncy</td>\n",
       "      <td>USSA30 ICPL Curncy</td>\n",
       "      <td>USSP15 CMPN Curncy</td>\n",
       "      <td>USSP20 CMPN Curncy</td>\n",
       "      <td>USSP25 CMPN Curncy</td>\n",
       "      <td>USSW15 CMPN Curncy</td>\n",
       "      <td>USSW20 CMPN Curncy</td>\n",
       "      <td>USSW25 CMPN Curncy</td>\n",
       "      <td>USSW30 CMPN Curncy</td>\n",
       "      <td>XQ1 Comdty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Date</td>\n",
       "      <td>USDMXN Curncy</td>\n",
       "      <td>ASURB MM Equity</td>\n",
       "      <td>BIMBOA MM Equity</td>\n",
       "      <td>CN1 Comdty</td>\n",
       "      <td>CT350188 Curncy</td>\n",
       "      <td>DU1 Comdty</td>\n",
       "      <td>ED749713@BVAL Corp</td>\n",
       "      <td>EMRUEMRU Index</td>\n",
       "      <td>EURMXN3Y Curncy</td>\n",
       "      <td>...</td>\n",
       "      <td>USSA25 ICPL Curncy</td>\n",
       "      <td>USSA30 ICPL Curncy</td>\n",
       "      <td>USSP15 CMPN Curncy</td>\n",
       "      <td>USSP20 CMPN Curncy</td>\n",
       "      <td>USSP25 CMPN Curncy</td>\n",
       "      <td>USSW15 CMPN Curncy</td>\n",
       "      <td>USSW20 CMPN Curncy</td>\n",
       "      <td>USSW25 CMPN Curncy</td>\n",
       "      <td>USSW30 CMPN Curncy</td>\n",
       "      <td>XQ1 Comdty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016-05-27 00:00:00</td>\n",
       "      <td>18.4751</td>\n",
       "      <td>289.95</td>\n",
       "      <td>55.4</td>\n",
       "      <td>141.28</td>\n",
       "      <td>424.14</td>\n",
       "      <td>111.85</td>\n",
       "      <td>118.813</td>\n",
       "      <td>66.0342</td>\n",
       "      <td>35645.2</td>\n",
       "      <td>...</td>\n",
       "      <td>2.102</td>\n",
       "      <td>2.138</td>\n",
       "      <td>-10.63</td>\n",
       "      <td>-18</td>\n",
       "      <td>-31.63</td>\n",
       "      <td>1.943</td>\n",
       "      <td>2.074</td>\n",
       "      <td>2.133</td>\n",
       "      <td>2.1738</td>\n",
       "      <td>124.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016-05-26 00:00:00</td>\n",
       "      <td>18.4544</td>\n",
       "      <td>284.68</td>\n",
       "      <td>55.29</td>\n",
       "      <td>141.83</td>\n",
       "      <td>427.999</td>\n",
       "      <td>111.845</td>\n",
       "      <td>118.75</td>\n",
       "      <td>65.282</td>\n",
       "      <td>35316.9</td>\n",
       "      <td>...</td>\n",
       "      <td>2.104</td>\n",
       "      <td>2.139</td>\n",
       "      <td>-10.46</td>\n",
       "      <td>-17.65</td>\n",
       "      <td>-31.43</td>\n",
       "      <td>1.928</td>\n",
       "      <td>2.0575</td>\n",
       "      <td>2.125</td>\n",
       "      <td>2.1598</td>\n",
       "      <td>124.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016-05-25 00:00:00</td>\n",
       "      <td>18.4852</td>\n",
       "      <td>281.4</td>\n",
       "      <td>56.33</td>\n",
       "      <td>140.98</td>\n",
       "      <td>5392.86</td>\n",
       "      <td>111.825</td>\n",
       "      <td>118.438</td>\n",
       "      <td>65.9242</td>\n",
       "      <td>35680.2</td>\n",
       "      <td>...</td>\n",
       "      <td>2.107</td>\n",
       "      <td>2.14</td>\n",
       "      <td>-11.11</td>\n",
       "      <td>-18.4</td>\n",
       "      <td>-32.03</td>\n",
       "      <td>1.955</td>\n",
       "      <td>2.0825</td>\n",
       "      <td>2.1453</td>\n",
       "      <td>2.1785</td>\n",
       "      <td>124.11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 87 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    0              1                2                 3   \\\n",
       "0               Ticker  USDMXN Curncy  ASURB MM Equity  BIMBOA MM Equity   \n",
       "1                 Date  USDMXN Curncy  ASURB MM Equity  BIMBOA MM Equity   \n",
       "2  2016-05-27 00:00:00        18.4751           289.95              55.4   \n",
       "3  2016-05-26 00:00:00        18.4544           284.68             55.29   \n",
       "4  2016-05-25 00:00:00        18.4852            281.4             56.33   \n",
       "\n",
       "           4                5           6                   7   \\\n",
       "0  CN1 Comdty  CT350188 Curncy  DU1 Comdty  ED749713@BVAL Corp   \n",
       "1  CN1 Comdty  CT350188 Curncy  DU1 Comdty  ED749713@BVAL Corp   \n",
       "2      141.28           424.14      111.85             118.813   \n",
       "3      141.83          427.999     111.845              118.75   \n",
       "4      140.98          5392.86     111.825             118.438   \n",
       "\n",
       "               8                9      ...                      77  \\\n",
       "0  EMRUEMRU Index  EURMXN3Y Curncy     ...      USSA25 ICPL Curncy   \n",
       "1  EMRUEMRU Index  EURMXN3Y Curncy     ...      USSA25 ICPL Curncy   \n",
       "2         66.0342          35645.2     ...                   2.102   \n",
       "3          65.282          35316.9     ...                   2.104   \n",
       "4         65.9242          35680.2     ...                   2.107   \n",
       "\n",
       "                   78                  79                  80  \\\n",
       "0  USSA30 ICPL Curncy  USSP15 CMPN Curncy  USSP20 CMPN Curncy   \n",
       "1  USSA30 ICPL Curncy  USSP15 CMPN Curncy  USSP20 CMPN Curncy   \n",
       "2               2.138              -10.63                 -18   \n",
       "3               2.139              -10.46              -17.65   \n",
       "4                2.14              -11.11               -18.4   \n",
       "\n",
       "                   81                  82                  83  \\\n",
       "0  USSP25 CMPN Curncy  USSW15 CMPN Curncy  USSW20 CMPN Curncy   \n",
       "1  USSP25 CMPN Curncy  USSW15 CMPN Curncy  USSW20 CMPN Curncy   \n",
       "2              -31.63               1.943               2.074   \n",
       "3              -31.43               1.928              2.0575   \n",
       "4              -32.03               1.955              2.0825   \n",
       "\n",
       "                   84                  85          86  \n",
       "0  USSW25 CMPN Curncy  USSW30 CMPN Curncy  XQ1 Comdty  \n",
       "1  USSW25 CMPN Curncy  USSW30 CMPN Curncy  XQ1 Comdty  \n",
       "2               2.133              2.1738      124.13  \n",
       "3               2.125              2.1598       124.3  \n",
       "4              2.1453              2.1785      124.11  \n",
       "\n",
       "[5 rows x 87 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6) df_ticker_desc = Just a table to map column id to the ticker and name of the variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>77</th>\n",
       "      <th>78</th>\n",
       "      <th>79</th>\n",
       "      <th>80</th>\n",
       "      <th>81</th>\n",
       "      <th>82</th>\n",
       "      <th>83</th>\n",
       "      <th>84</th>\n",
       "      <th>85</th>\n",
       "      <th>86</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ticker</td>\n",
       "      <td>USDMXN Curncy</td>\n",
       "      <td>ASURB MM Equity</td>\n",
       "      <td>BIMBOA MM Equity</td>\n",
       "      <td>CN1 Comdty</td>\n",
       "      <td>CT350188 Curncy</td>\n",
       "      <td>DU1 Comdty</td>\n",
       "      <td>ED749713@BVAL Corp</td>\n",
       "      <td>EMRUEMRU Index</td>\n",
       "      <td>EURMXN3Y Curncy</td>\n",
       "      <td>...</td>\n",
       "      <td>USSA25 ICPL Curncy</td>\n",
       "      <td>USSA30 ICPL Curncy</td>\n",
       "      <td>USSP15 CMPN Curncy</td>\n",
       "      <td>USSP20 CMPN Curncy</td>\n",
       "      <td>USSP25 CMPN Curncy</td>\n",
       "      <td>USSW15 CMPN Curncy</td>\n",
       "      <td>USSW20 CMPN Curncy</td>\n",
       "      <td>USSW25 CMPN Curncy</td>\n",
       "      <td>USSW30 CMPN Curncy</td>\n",
       "      <td>XQ1 Comdty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Date</td>\n",
       "      <td>USDMXN Curncy</td>\n",
       "      <td>ASURB MM Equity</td>\n",
       "      <td>BIMBOA MM Equity</td>\n",
       "      <td>CN1 Comdty</td>\n",
       "      <td>CT350188 Curncy</td>\n",
       "      <td>DU1 Comdty</td>\n",
       "      <td>ED749713@BVAL Corp</td>\n",
       "      <td>EMRUEMRU Index</td>\n",
       "      <td>EURMXN3Y Curncy</td>\n",
       "      <td>...</td>\n",
       "      <td>USSA25 ICPL Curncy</td>\n",
       "      <td>USSA30 ICPL Curncy</td>\n",
       "      <td>USSP15 CMPN Curncy</td>\n",
       "      <td>USSP20 CMPN Curncy</td>\n",
       "      <td>USSP25 CMPN Curncy</td>\n",
       "      <td>USSW15 CMPN Curncy</td>\n",
       "      <td>USSW20 CMPN Curncy</td>\n",
       "      <td>USSW25 CMPN Curncy</td>\n",
       "      <td>USSW30 CMPN Curncy</td>\n",
       "      <td>XQ1 Comdty</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 87 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0              1                2                 3           4   \\\n",
       "0  Ticker  USDMXN Curncy  ASURB MM Equity  BIMBOA MM Equity  CN1 Comdty   \n",
       "1    Date  USDMXN Curncy  ASURB MM Equity  BIMBOA MM Equity  CN1 Comdty   \n",
       "\n",
       "                5           6                   7               8   \\\n",
       "0  CT350188 Curncy  DU1 Comdty  ED749713@BVAL Corp  EMRUEMRU Index   \n",
       "1  CT350188 Curncy  DU1 Comdty  ED749713@BVAL Corp  EMRUEMRU Index   \n",
       "\n",
       "                9      ...                      77                  78  \\\n",
       "0  EURMXN3Y Curncy     ...      USSA25 ICPL Curncy  USSA30 ICPL Curncy   \n",
       "1  EURMXN3Y Curncy     ...      USSA25 ICPL Curncy  USSA30 ICPL Curncy   \n",
       "\n",
       "                   79                  80                  81  \\\n",
       "0  USSP15 CMPN Curncy  USSP20 CMPN Curncy  USSP25 CMPN Curncy   \n",
       "1  USSP15 CMPN Curncy  USSP20 CMPN Curncy  USSP25 CMPN Curncy   \n",
       "\n",
       "                   82                  83                  84  \\\n",
       "0  USSW15 CMPN Curncy  USSW20 CMPN Curncy  USSW25 CMPN Curncy   \n",
       "1  USSW15 CMPN Curncy  USSW20 CMPN Curncy  USSW25 CMPN Curncy   \n",
       "\n",
       "                   85          86  \n",
       "0  USSW30 CMPN Curncy  XQ1 Comdty  \n",
       "1  USSW30 CMPN Curncy  XQ1 Comdty  \n",
       "\n",
       "[2 rows x 87 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ticker_desc = df.ix[:1] # 1st 2 rows\n",
    "df_ticker_desc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7) df_values(0:87) = date(0) + col(1) + col(2) + col(3:87)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>77</th>\n",
       "      <th>78</th>\n",
       "      <th>79</th>\n",
       "      <th>80</th>\n",
       "      <th>81</th>\n",
       "      <th>82</th>\n",
       "      <th>83</th>\n",
       "      <th>84</th>\n",
       "      <th>85</th>\n",
       "      <th>86</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016-05-27 00:00:00</td>\n",
       "      <td>18.4751</td>\n",
       "      <td>289.95</td>\n",
       "      <td>55.4</td>\n",
       "      <td>141.28</td>\n",
       "      <td>424.14</td>\n",
       "      <td>111.85</td>\n",
       "      <td>118.813</td>\n",
       "      <td>66.0342</td>\n",
       "      <td>35645.2</td>\n",
       "      <td>...</td>\n",
       "      <td>2.102</td>\n",
       "      <td>2.138</td>\n",
       "      <td>-10.63</td>\n",
       "      <td>-18</td>\n",
       "      <td>-31.63</td>\n",
       "      <td>1.943</td>\n",
       "      <td>2.074</td>\n",
       "      <td>2.133</td>\n",
       "      <td>2.1738</td>\n",
       "      <td>124.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016-05-26 00:00:00</td>\n",
       "      <td>18.4544</td>\n",
       "      <td>284.68</td>\n",
       "      <td>55.29</td>\n",
       "      <td>141.83</td>\n",
       "      <td>427.999</td>\n",
       "      <td>111.845</td>\n",
       "      <td>118.75</td>\n",
       "      <td>65.282</td>\n",
       "      <td>35316.9</td>\n",
       "      <td>...</td>\n",
       "      <td>2.104</td>\n",
       "      <td>2.139</td>\n",
       "      <td>-10.46</td>\n",
       "      <td>-17.65</td>\n",
       "      <td>-31.43</td>\n",
       "      <td>1.928</td>\n",
       "      <td>2.0575</td>\n",
       "      <td>2.125</td>\n",
       "      <td>2.1598</td>\n",
       "      <td>124.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016-05-25 00:00:00</td>\n",
       "      <td>18.4852</td>\n",
       "      <td>281.4</td>\n",
       "      <td>56.33</td>\n",
       "      <td>140.98</td>\n",
       "      <td>5392.86</td>\n",
       "      <td>111.825</td>\n",
       "      <td>118.438</td>\n",
       "      <td>65.9242</td>\n",
       "      <td>35680.2</td>\n",
       "      <td>...</td>\n",
       "      <td>2.107</td>\n",
       "      <td>2.14</td>\n",
       "      <td>-11.11</td>\n",
       "      <td>-18.4</td>\n",
       "      <td>-32.03</td>\n",
       "      <td>1.955</td>\n",
       "      <td>2.0825</td>\n",
       "      <td>2.1453</td>\n",
       "      <td>2.1785</td>\n",
       "      <td>124.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2016-05-24 00:00:00</td>\n",
       "      <td>18.487</td>\n",
       "      <td>282.81</td>\n",
       "      <td>56.06</td>\n",
       "      <td>141.16</td>\n",
       "      <td>5392.86</td>\n",
       "      <td>111.815</td>\n",
       "      <td>118.5</td>\n",
       "      <td>66.8658</td>\n",
       "      <td>36154.1</td>\n",
       "      <td>...</td>\n",
       "      <td>2.126</td>\n",
       "      <td>2.158</td>\n",
       "      <td>-11.18</td>\n",
       "      <td>-18.43</td>\n",
       "      <td>-31.94</td>\n",
       "      <td>1.9469</td>\n",
       "      <td>2.0703</td>\n",
       "      <td>2.1305</td>\n",
       "      <td>2.1615</td>\n",
       "      <td>124.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2016-05-23 00:00:00</td>\n",
       "      <td>18.5161</td>\n",
       "      <td>282.02</td>\n",
       "      <td>55.27</td>\n",
       "      <td>141.31</td>\n",
       "      <td>5392.86</td>\n",
       "      <td>111.815</td>\n",
       "      <td>120.188</td>\n",
       "      <td>67.0104</td>\n",
       "      <td>36684.3</td>\n",
       "      <td>...</td>\n",
       "      <td>2.092</td>\n",
       "      <td>2.125</td>\n",
       "      <td>-10.81</td>\n",
       "      <td>-18.14</td>\n",
       "      <td>-31.55</td>\n",
       "      <td>1.9245</td>\n",
       "      <td>2.0503</td>\n",
       "      <td>2.112</td>\n",
       "      <td>2.1435</td>\n",
       "      <td>124.27</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 87 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    0        1       2      3       4        5        6   \\\n",
       "2  2016-05-27 00:00:00  18.4751  289.95   55.4  141.28   424.14   111.85   \n",
       "3  2016-05-26 00:00:00  18.4544  284.68  55.29  141.83  427.999  111.845   \n",
       "4  2016-05-25 00:00:00  18.4852   281.4  56.33  140.98  5392.86  111.825   \n",
       "5  2016-05-24 00:00:00   18.487  282.81  56.06  141.16  5392.86  111.815   \n",
       "6  2016-05-23 00:00:00  18.5161  282.02  55.27  141.31  5392.86  111.815   \n",
       "\n",
       "        7        8        9    ...       77     78     79     80     81  \\\n",
       "2  118.813  66.0342  35645.2   ...    2.102  2.138 -10.63    -18 -31.63   \n",
       "3   118.75   65.282  35316.9   ...    2.104  2.139 -10.46 -17.65 -31.43   \n",
       "4  118.438  65.9242  35680.2   ...    2.107   2.14 -11.11  -18.4 -32.03   \n",
       "5    118.5  66.8658  36154.1   ...    2.126  2.158 -11.18 -18.43 -31.94   \n",
       "6  120.188  67.0104  36684.3   ...    2.092  2.125 -10.81 -18.14 -31.55   \n",
       "\n",
       "       82      83      84      85      86  \n",
       "2   1.943   2.074   2.133  2.1738  124.13  \n",
       "3   1.928  2.0575   2.125  2.1598   124.3  \n",
       "4   1.955  2.0825  2.1453  2.1785  124.11  \n",
       "5  1.9469  2.0703  2.1305  2.1615  124.24  \n",
       "6  1.9245  2.0503   2.112  2.1435  124.27  \n",
       "\n",
       "[5 rows x 87 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_values = df.ix[2:] # row 2 onwards\n",
    "dv = df_values\n",
    "dv.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8) Set the dates as index of the dataframe = 2000-01-03 to 2016-05-27 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>77</th>\n",
       "      <th>78</th>\n",
       "      <th>79</th>\n",
       "      <th>80</th>\n",
       "      <th>81</th>\n",
       "      <th>82</th>\n",
       "      <th>83</th>\n",
       "      <th>84</th>\n",
       "      <th>85</th>\n",
       "      <th>86</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2016-05-27</th>\n",
       "      <td>18.4751</td>\n",
       "      <td>289.95</td>\n",
       "      <td>55.4</td>\n",
       "      <td>141.28</td>\n",
       "      <td>424.14</td>\n",
       "      <td>111.85</td>\n",
       "      <td>118.813</td>\n",
       "      <td>66.0342</td>\n",
       "      <td>35645.2</td>\n",
       "      <td>161.89</td>\n",
       "      <td>...</td>\n",
       "      <td>2.102</td>\n",
       "      <td>2.138</td>\n",
       "      <td>-10.63</td>\n",
       "      <td>-18</td>\n",
       "      <td>-31.63</td>\n",
       "      <td>1.943</td>\n",
       "      <td>2.074</td>\n",
       "      <td>2.133</td>\n",
       "      <td>2.1738</td>\n",
       "      <td>124.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-05-26</th>\n",
       "      <td>18.4544</td>\n",
       "      <td>284.68</td>\n",
       "      <td>55.29</td>\n",
       "      <td>141.83</td>\n",
       "      <td>427.999</td>\n",
       "      <td>111.845</td>\n",
       "      <td>118.75</td>\n",
       "      <td>65.282</td>\n",
       "      <td>35316.9</td>\n",
       "      <td>161.8</td>\n",
       "      <td>...</td>\n",
       "      <td>2.104</td>\n",
       "      <td>2.139</td>\n",
       "      <td>-10.46</td>\n",
       "      <td>-17.65</td>\n",
       "      <td>-31.43</td>\n",
       "      <td>1.928</td>\n",
       "      <td>2.0575</td>\n",
       "      <td>2.125</td>\n",
       "      <td>2.1598</td>\n",
       "      <td>124.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-05-25</th>\n",
       "      <td>18.4852</td>\n",
       "      <td>281.4</td>\n",
       "      <td>56.33</td>\n",
       "      <td>140.98</td>\n",
       "      <td>5392.86</td>\n",
       "      <td>111.825</td>\n",
       "      <td>118.438</td>\n",
       "      <td>65.9242</td>\n",
       "      <td>35680.2</td>\n",
       "      <td>161.61</td>\n",
       "      <td>...</td>\n",
       "      <td>2.107</td>\n",
       "      <td>2.14</td>\n",
       "      <td>-11.11</td>\n",
       "      <td>-18.4</td>\n",
       "      <td>-32.03</td>\n",
       "      <td>1.955</td>\n",
       "      <td>2.0825</td>\n",
       "      <td>2.1453</td>\n",
       "      <td>2.1785</td>\n",
       "      <td>124.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-05-24</th>\n",
       "      <td>18.487</td>\n",
       "      <td>282.81</td>\n",
       "      <td>56.06</td>\n",
       "      <td>141.16</td>\n",
       "      <td>5392.86</td>\n",
       "      <td>111.815</td>\n",
       "      <td>118.5</td>\n",
       "      <td>66.8658</td>\n",
       "      <td>36154.1</td>\n",
       "      <td>161.58</td>\n",
       "      <td>...</td>\n",
       "      <td>2.126</td>\n",
       "      <td>2.158</td>\n",
       "      <td>-11.18</td>\n",
       "      <td>-18.43</td>\n",
       "      <td>-31.94</td>\n",
       "      <td>1.9469</td>\n",
       "      <td>2.0703</td>\n",
       "      <td>2.1305</td>\n",
       "      <td>2.1615</td>\n",
       "      <td>124.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-05-23</th>\n",
       "      <td>18.5161</td>\n",
       "      <td>282.02</td>\n",
       "      <td>55.27</td>\n",
       "      <td>141.31</td>\n",
       "      <td>5392.86</td>\n",
       "      <td>111.815</td>\n",
       "      <td>120.188</td>\n",
       "      <td>67.0104</td>\n",
       "      <td>36684.3</td>\n",
       "      <td>161.55</td>\n",
       "      <td>...</td>\n",
       "      <td>2.092</td>\n",
       "      <td>2.125</td>\n",
       "      <td>-10.81</td>\n",
       "      <td>-18.14</td>\n",
       "      <td>-31.55</td>\n",
       "      <td>1.9245</td>\n",
       "      <td>2.0503</td>\n",
       "      <td>2.112</td>\n",
       "      <td>2.1435</td>\n",
       "      <td>124.27</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 86 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 1       2      3       4        5        6        7   \\\n",
       "0                                                                       \n",
       "2016-05-27  18.4751  289.95   55.4  141.28   424.14   111.85  118.813   \n",
       "2016-05-26  18.4544  284.68  55.29  141.83  427.999  111.845   118.75   \n",
       "2016-05-25  18.4852   281.4  56.33  140.98  5392.86  111.825  118.438   \n",
       "2016-05-24   18.487  282.81  56.06  141.16  5392.86  111.815    118.5   \n",
       "2016-05-23  18.5161  282.02  55.27  141.31  5392.86  111.815  120.188   \n",
       "\n",
       "                 8        9       10   ...       77     78     79     80  \\\n",
       "0                                      ...                                 \n",
       "2016-05-27  66.0342  35645.2  161.89   ...    2.102  2.138 -10.63    -18   \n",
       "2016-05-26   65.282  35316.9   161.8   ...    2.104  2.139 -10.46 -17.65   \n",
       "2016-05-25  65.9242  35680.2  161.61   ...    2.107   2.14 -11.11  -18.4   \n",
       "2016-05-24  66.8658  36154.1  161.58   ...    2.126  2.158 -11.18 -18.43   \n",
       "2016-05-23  67.0104  36684.3  161.55   ...    2.092  2.125 -10.81 -18.14   \n",
       "\n",
       "               81      82      83      84      85      86  \n",
       "0                                                          \n",
       "2016-05-27 -31.63   1.943   2.074   2.133  2.1738  124.13  \n",
       "2016-05-26 -31.43   1.928  2.0575   2.125  2.1598   124.3  \n",
       "2016-05-25 -32.03   1.955  2.0825  2.1453  2.1785  124.11  \n",
       "2016-05-24 -31.94  1.9469  2.0703  2.1305  2.1615  124.24  \n",
       "2016-05-23 -31.55  1.9245  2.0503   2.112  2.1435  124.27  \n",
       "\n",
       "[5 rows x 86 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_values_indexed = df_values.set_index([0])\n",
    "dvi = df_values_indexed\n",
    "dvi.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>77</th>\n",
       "      <th>78</th>\n",
       "      <th>79</th>\n",
       "      <th>80</th>\n",
       "      <th>81</th>\n",
       "      <th>82</th>\n",
       "      <th>83</th>\n",
       "      <th>84</th>\n",
       "      <th>85</th>\n",
       "      <th>86</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2000-01-07</th>\n",
       "      <td>9.565</td>\n",
       "      <td>14.23</td>\n",
       "      <td>4.83</td>\n",
       "      <td>117.45</td>\n",
       "      <td>382.997</td>\n",
       "      <td>102.71</td>\n",
       "      <td>132.35</td>\n",
       "      <td>27.321</td>\n",
       "      <td>22485</td>\n",
       "      <td>119.16</td>\n",
       "      <td>...</td>\n",
       "      <td>7.475</td>\n",
       "      <td>7.465</td>\n",
       "      <td>87</td>\n",
       "      <td>89</td>\n",
       "      <td>84</td>\n",
       "      <td>7.39</td>\n",
       "      <td>7.41</td>\n",
       "      <td>6.461</td>\n",
       "      <td>7.4</td>\n",
       "      <td>98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-06</th>\n",
       "      <td>9.58</td>\n",
       "      <td>14.23</td>\n",
       "      <td>4.68</td>\n",
       "      <td>117.09</td>\n",
       "      <td>382.997</td>\n",
       "      <td>102.5</td>\n",
       "      <td>132.35</td>\n",
       "      <td>27.321</td>\n",
       "      <td>22485</td>\n",
       "      <td>118.7</td>\n",
       "      <td>...</td>\n",
       "      <td>7.505</td>\n",
       "      <td>7.48</td>\n",
       "      <td>89</td>\n",
       "      <td>90</td>\n",
       "      <td>84</td>\n",
       "      <td>7.43</td>\n",
       "      <td>7.44</td>\n",
       "      <td>6.461</td>\n",
       "      <td>7.43</td>\n",
       "      <td>97.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-05</th>\n",
       "      <td>9.571</td>\n",
       "      <td>14.23</td>\n",
       "      <td>4.83</td>\n",
       "      <td>116.57</td>\n",
       "      <td>382.997</td>\n",
       "      <td>102.49</td>\n",
       "      <td>132.35</td>\n",
       "      <td>27.321</td>\n",
       "      <td>22485</td>\n",
       "      <td>119.15</td>\n",
       "      <td>...</td>\n",
       "      <td>7.475</td>\n",
       "      <td>7.455</td>\n",
       "      <td>88</td>\n",
       "      <td>89</td>\n",
       "      <td>84</td>\n",
       "      <td>7.47</td>\n",
       "      <td>7.48</td>\n",
       "      <td>6.461</td>\n",
       "      <td>7.46</td>\n",
       "      <td>97.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-04</th>\n",
       "      <td>9.5713</td>\n",
       "      <td>14.23</td>\n",
       "      <td>4.83</td>\n",
       "      <td>117.5</td>\n",
       "      <td>382.997</td>\n",
       "      <td>102.49</td>\n",
       "      <td>132.35</td>\n",
       "      <td>27.54</td>\n",
       "      <td>22485</td>\n",
       "      <td>119.2</td>\n",
       "      <td>...</td>\n",
       "      <td>7.425</td>\n",
       "      <td>7.425</td>\n",
       "      <td>90</td>\n",
       "      <td>91</td>\n",
       "      <td>84</td>\n",
       "      <td>7.4</td>\n",
       "      <td>7.41</td>\n",
       "      <td>6.461</td>\n",
       "      <td>7.4</td>\n",
       "      <td>98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-03</th>\n",
       "      <td>9.505</td>\n",
       "      <td>14.23</td>\n",
       "      <td>5.03</td>\n",
       "      <td>118.5</td>\n",
       "      <td>382.997</td>\n",
       "      <td>102.47</td>\n",
       "      <td>132.35</td>\n",
       "      <td>27.54</td>\n",
       "      <td>22485</td>\n",
       "      <td>119.85</td>\n",
       "      <td>...</td>\n",
       "      <td>7.455</td>\n",
       "      <td>7.425</td>\n",
       "      <td>86</td>\n",
       "      <td>87</td>\n",
       "      <td>84</td>\n",
       "      <td>7.46</td>\n",
       "      <td>7.47</td>\n",
       "      <td>6.461</td>\n",
       "      <td>7.44</td>\n",
       "      <td>98.56</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 86 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                1      2     3       4        5       6       7       8   \\\n",
       "0                                                                          \n",
       "2000-01-07   9.565  14.23  4.83  117.45  382.997  102.71  132.35  27.321   \n",
       "2000-01-06    9.58  14.23  4.68  117.09  382.997   102.5  132.35  27.321   \n",
       "2000-01-05   9.571  14.23  4.83  116.57  382.997  102.49  132.35  27.321   \n",
       "2000-01-04  9.5713  14.23  4.83   117.5  382.997  102.49  132.35   27.54   \n",
       "2000-01-03   9.505  14.23  5.03   118.5  382.997  102.47  132.35   27.54   \n",
       "\n",
       "               9       10  ...       77     78  79  80  81    82    83     84  \\\n",
       "0                          ...                                                  \n",
       "2000-01-07  22485  119.16  ...    7.475  7.465  87  89  84  7.39  7.41  6.461   \n",
       "2000-01-06  22485   118.7  ...    7.505   7.48  89  90  84  7.43  7.44  6.461   \n",
       "2000-01-05  22485  119.15  ...    7.475  7.455  88  89  84  7.47  7.48  6.461   \n",
       "2000-01-04  22485   119.2  ...    7.425  7.425  90  91  84   7.4  7.41  6.461   \n",
       "2000-01-03  22485  119.85  ...    7.455  7.425  86  87  84  7.46  7.47  6.461   \n",
       "\n",
       "              85     86  \n",
       "0                        \n",
       "2000-01-07   7.4     98  \n",
       "2000-01-06  7.43   97.7  \n",
       "2000-01-05  7.46  97.55  \n",
       "2000-01-04   7.4     98  \n",
       "2000-01-03  7.44  98.56  \n",
       "\n",
       "[5 rows x 86 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dvi.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4280, 86)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dvi.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9) Fill any gaps in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>77</th>\n",
       "      <th>78</th>\n",
       "      <th>79</th>\n",
       "      <th>80</th>\n",
       "      <th>81</th>\n",
       "      <th>82</th>\n",
       "      <th>83</th>\n",
       "      <th>84</th>\n",
       "      <th>85</th>\n",
       "      <th>86</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2016-05-27</th>\n",
       "      <td>18.4751</td>\n",
       "      <td>289.95</td>\n",
       "      <td>55.40</td>\n",
       "      <td>141.28</td>\n",
       "      <td>424.140</td>\n",
       "      <td>111.850</td>\n",
       "      <td>118.813</td>\n",
       "      <td>66.0342</td>\n",
       "      <td>35645.2148</td>\n",
       "      <td>161.89</td>\n",
       "      <td>...</td>\n",
       "      <td>2.102</td>\n",
       "      <td>2.138</td>\n",
       "      <td>-10.63</td>\n",
       "      <td>-18.00</td>\n",
       "      <td>-31.63</td>\n",
       "      <td>1.9430</td>\n",
       "      <td>2.0740</td>\n",
       "      <td>2.1330</td>\n",
       "      <td>2.1738</td>\n",
       "      <td>124.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-05-26</th>\n",
       "      <td>18.4544</td>\n",
       "      <td>284.68</td>\n",
       "      <td>55.29</td>\n",
       "      <td>141.83</td>\n",
       "      <td>427.999</td>\n",
       "      <td>111.845</td>\n",
       "      <td>118.750</td>\n",
       "      <td>65.2820</td>\n",
       "      <td>35316.8828</td>\n",
       "      <td>161.80</td>\n",
       "      <td>...</td>\n",
       "      <td>2.104</td>\n",
       "      <td>2.139</td>\n",
       "      <td>-10.46</td>\n",
       "      <td>-17.65</td>\n",
       "      <td>-31.43</td>\n",
       "      <td>1.9280</td>\n",
       "      <td>2.0575</td>\n",
       "      <td>2.1250</td>\n",
       "      <td>2.1598</td>\n",
       "      <td>124.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-05-25</th>\n",
       "      <td>18.4852</td>\n",
       "      <td>281.40</td>\n",
       "      <td>56.33</td>\n",
       "      <td>140.98</td>\n",
       "      <td>5392.863</td>\n",
       "      <td>111.825</td>\n",
       "      <td>118.438</td>\n",
       "      <td>65.9242</td>\n",
       "      <td>35680.2227</td>\n",
       "      <td>161.61</td>\n",
       "      <td>...</td>\n",
       "      <td>2.107</td>\n",
       "      <td>2.140</td>\n",
       "      <td>-11.11</td>\n",
       "      <td>-18.40</td>\n",
       "      <td>-32.03</td>\n",
       "      <td>1.9550</td>\n",
       "      <td>2.0825</td>\n",
       "      <td>2.1453</td>\n",
       "      <td>2.1785</td>\n",
       "      <td>124.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-05-24</th>\n",
       "      <td>18.4870</td>\n",
       "      <td>282.81</td>\n",
       "      <td>56.06</td>\n",
       "      <td>141.16</td>\n",
       "      <td>5392.863</td>\n",
       "      <td>111.815</td>\n",
       "      <td>118.500</td>\n",
       "      <td>66.8658</td>\n",
       "      <td>36154.1172</td>\n",
       "      <td>161.58</td>\n",
       "      <td>...</td>\n",
       "      <td>2.126</td>\n",
       "      <td>2.158</td>\n",
       "      <td>-11.18</td>\n",
       "      <td>-18.43</td>\n",
       "      <td>-31.94</td>\n",
       "      <td>1.9469</td>\n",
       "      <td>2.0703</td>\n",
       "      <td>2.1305</td>\n",
       "      <td>2.1615</td>\n",
       "      <td>124.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-05-23</th>\n",
       "      <td>18.5161</td>\n",
       "      <td>282.02</td>\n",
       "      <td>55.27</td>\n",
       "      <td>141.31</td>\n",
       "      <td>5392.863</td>\n",
       "      <td>111.815</td>\n",
       "      <td>120.188</td>\n",
       "      <td>67.0104</td>\n",
       "      <td>36684.3047</td>\n",
       "      <td>161.55</td>\n",
       "      <td>...</td>\n",
       "      <td>2.092</td>\n",
       "      <td>2.125</td>\n",
       "      <td>-10.81</td>\n",
       "      <td>-18.14</td>\n",
       "      <td>-31.55</td>\n",
       "      <td>1.9245</td>\n",
       "      <td>2.0503</td>\n",
       "      <td>2.1120</td>\n",
       "      <td>2.1435</td>\n",
       "      <td>124.27</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 86 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 1       2      3       4         5        6        7   \\\n",
       "0                                                                        \n",
       "2016-05-27  18.4751  289.95  55.40  141.28   424.140  111.850  118.813   \n",
       "2016-05-26  18.4544  284.68  55.29  141.83   427.999  111.845  118.750   \n",
       "2016-05-25  18.4852  281.40  56.33  140.98  5392.863  111.825  118.438   \n",
       "2016-05-24  18.4870  282.81  56.06  141.16  5392.863  111.815  118.500   \n",
       "2016-05-23  18.5161  282.02  55.27  141.31  5392.863  111.815  120.188   \n",
       "\n",
       "                 8           9       10   ...       77     78     79     80  \\\n",
       "0                                         ...                                 \n",
       "2016-05-27  66.0342  35645.2148  161.89   ...    2.102  2.138 -10.63 -18.00   \n",
       "2016-05-26  65.2820  35316.8828  161.80   ...    2.104  2.139 -10.46 -17.65   \n",
       "2016-05-25  65.9242  35680.2227  161.61   ...    2.107  2.140 -11.11 -18.40   \n",
       "2016-05-24  66.8658  36154.1172  161.58   ...    2.126  2.158 -11.18 -18.43   \n",
       "2016-05-23  67.0104  36684.3047  161.55   ...    2.092  2.125 -10.81 -18.14   \n",
       "\n",
       "               81      82      83      84      85      86  \n",
       "0                                                          \n",
       "2016-05-27 -31.63  1.9430  2.0740  2.1330  2.1738  124.13  \n",
       "2016-05-26 -31.43  1.9280  2.0575  2.1250  2.1598  124.30  \n",
       "2016-05-25 -32.03  1.9550  2.0825  2.1453  2.1785  124.11  \n",
       "2016-05-24 -31.94  1.9469  2.0703  2.1305  2.1615  124.24  \n",
       "2016-05-23 -31.55  1.9245  2.0503  2.1120  2.1435  124.27  \n",
       "\n",
       "[5 rows x 86 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pandas includes a very convenient function for filling gaps in the data.\n",
    "df_values_indexed = df_values_indexed.fillna(method='ffill')\n",
    "df_values_indexed.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory data analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploratory Data Analysis (EDA) is foundational to working with machine learning, and any other sort of analysis. EDA means getting to know your data, getting your fingers dirty with your data, feeling it and seeing it. The end result is you know your data very well, so when you build models you build them based on an actual, practical, physical understanding of the data, not assumptions or vaguely held notions. You can still make assumptions of course, but EDA means you will understand your assumptions and why you're making those assumptions. \n",
    "\n",
    "First, take a look at the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10) Describe the data briefly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_values_indexed.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that the various indices operate on scales differing by orders of magnitude. It's best to scale the data so that, for example, operations involving multiple indices aren't unduly influenced by a single, massive index.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11) Plot the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# N.B. A super-useful trick-ette is to assign the return value of plot to _ \n",
    "# so that you don't get text printed before the plot itself.\n",
    "\n",
    "# _ = pd.concat([data_values_indexed[1],\n",
    "#   data_values_indexed[1070],\n",
    "#   data_values_indexed[788],\n",
    "#   data_values_indexed[926]], axis=1).plot(figsize=(20, 15))\n",
    "\n",
    "_ = df_values_indexed.plot(figsize=(20, 15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the structure isn't uniformly visible for the indices. Divide each value in an individual index by the maximum value for that index., and then replot. The maximum value of all indices will be 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12) Calculate the max value for each column, prepare to scale data for each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_values_indexed_max = df_values_indexed.max(axis=0) # max across axis 0 = rows\n",
    "df_values_indexed_max.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13) Scale the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_values_indexed_scaled = df_values_indexed / df_values_indexed_max\n",
    "df_values_indexed_scaled.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14) Plot the scaled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "_ = df_values_indexed_scaled.plot(figsize=(25, 20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shortform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dvis = df_values_indexed_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15) Auto-correlations\n",
    "\n",
    "Next, plot autocorrelations for each of the indices. The autocorrelations determine correlations between current values of the index and lagged values of the same index. The goal is to determine whether the lagged values are reliable indicators of the current values. If they are, then we've identified a correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "fig.set_figwidth(20)\n",
    "fig.set_figheight(10)\n",
    "\n",
    "_ = autocorrelation_plot(dvis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For 2300 lagged days, we observe positive auto-correlations.\n",
    "This suggests that as the variables increase, they tend to keep on increasing. Momentum.\n",
    "\n",
    "After 2300 lagged days, we observe negative auto-correlations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16) Skip: Just a reminder of the PCA columns we selected earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# list_cols_pca = [ 0,1,\n",
    "#                   1070,788,926,112,69,574,654,1160,527,323,\n",
    "#                   397,118,774,1028,1034,655,907,736,251,388,\n",
    "#                   327,243,705,303,1146,467,136,1006,600,15,\n",
    "#                   231,290,131,782,20,1048,630,1173,431,856,\n",
    "#                   67,299,838,639,53,932,870,938,1061\n",
    "#                 ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 17) Scatter plots of the first 20 of our 86 variables vs USDMXN(varid=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# _ = scatter_matrix(data_values_indexed_scaled)\n",
    "# dvis = df_values_indexed_scaled\n",
    "\n",
    "_ = scatter_matrix(\n",
    "      pd.concat(\n",
    "      [ \n",
    "        dvis[ 1],dvis[ 2],dvis[ 3],dvis[ 4],dvis[ 5],dvis[ 6],dvis[ 7],dvis[ 8],dvis[ 9],dvis[10],\n",
    "        dvis[11],dvis[12],dvis[13],dvis[14],dvis[15],dvis[16],dvis[17],dvis[18],dvis[19],dvis[20],\n",
    "      ], axis=1), figsize=(15, 15), diagonal='kde')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 18) Remind ourselves what our scaled data (of price or index LEVELS) looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dvis.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 19) Calculate Log Returns on our scaled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_values_indexed_scaled_logret = pd.DataFrame()\n",
    "df_values_indexed_scaled_logret = np.log(dvis/dvis.shift(-1)) # note dates are reverse-chrono\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shortform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dvislr = df_values_indexed_scaled_logret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dvislr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dvislr.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dvislr.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 2000/01/07 to 2016/05/27 = 1 header row + 4279 data rows\n",
    "# print dvislr.head()\n",
    "# print dvislr.tail()\n",
    "print 'NumRowsIncludeHeader = len(dvislr) = ' + str(len(dvislr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 20) Replace inf, NaN in data with Zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dvislr = dvislr.replace([np.inf, -np.inf, np.nan], 0)\n",
    "dvislr.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dvislr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dvislr.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 21) Skip: Fill the Gaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Pandas includes a very convenient function for filling gaps in the data.\n",
    "# dvislr = dvislr.fillna(method='ffill')\n",
    "# dvislr.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 22) Plot log returns of scaled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "_ = dvislr.plot(figsize=(20, 15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 23) Auto-Correlations of log returns of scaled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "fig.set_figwidth(20)\n",
    "fig.set_figheight(10)\n",
    "\n",
    "_ = autocorrelation_plot(dvislr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no auto-correlations, so we are good."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 24) Skip: Just a reminder of the PCA50 Columns we selected earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# list_cols_pca = [ 0,1,\n",
    "#                   1070,788,926,112,69,574,654,1160,527,323,\n",
    "#                   397,118,774,1028,1034,655,907,736,251,388,\n",
    "#                   327,243,705,303,1146,467,136,1006,600,15,\n",
    "#                   231,290,131,782,20,1048,630,1173,431,856,\n",
    "#                   67,299,838,639,53,932,870,938,1061\n",
    "#                 ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 25) Scatter plots of log returns of first 20 of the 86 variables where log-returns of USDMXN(varid=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# _ = scatter_matrix(data_values_indexed_scaled_logret)   # takes long time, becareful, save work first\n",
    "# dvislr = df_values_indexed_scaled_logret\n",
    "\n",
    "_ = scatter_matrix(\n",
    "      pd.concat(\n",
    "      [ \n",
    "        dvislr[ 1],dvislr[ 2],dvislr[ 3],dvislr[ 4],dvislr[ 5],dvislr[ 6],dvislr[ 7],dvislr[ 8],dvislr[ 9],dvislr[10],\n",
    "        dvislr[11],dvislr[12],dvislr[13],dvislr[14],dvislr[15],dvislr[16],dvislr[17],dvislr[18],dvislr[19],dvislr[20],\n",
    "      ], axis=1), figsize=(15, 15), diagonal='kde')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summing up the EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, you've done a good enough job of exploratory data analysis. You've visualized our data and come to know it better. \n",
    "You've transformed it into a form that is useful for modelling, log returns, and looked at how indices relate to each other. \n",
    "\n",
    "What should we think so far?\n",
    "\n",
    "Cloud Datalab is working great. With just a few lines of code, you were able to munge the data, visualize the changes, and make decisions. You could easily analyze and iterate. This is a common feature of iPython, but the advantage here is that Cloud Datalab is a managed service that you can simply click and use, so you can focus on your analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we can see a model:\n",
    "\n",
    "* We'll predict whether the USDMXN close today will be higher or lower than yesterday.\n",
    "\n",
    "Predicting whether the log return of the USDMXN is positive or negative is a classification problem. \n",
    "That is, we want to choose one option from a finite set of options, in this case positive or negative. \n",
    "This is the base case of classification where we have only two values to choose from, known as binary classification, or logistic regression.\n",
    "\n",
    "Machine learning models are very good at finding weak signals from data.\n",
    "In machine learning, as in most things, there are subtle tradeoffs happening, but in general good data is better than good algorithms, which are better than good frameworks. \n",
    "You need all three pillars but in that order of importance: data, algorithms, frameworks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[TensorFlow](https://tensorflow.org) is an open source software library, initiated by Google, for numerical computation using data flow graphs. TensorFlow is based on Google's machine learning expertise and is the next generation framework used internally at Google for tasks such as translation and image recognition. It's a wonderful framework for machine learning because it's expressive, efficient, and easy to use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature engineering for TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From a training and testing perspective, time series data is easy. Training data should come from events that happened before test data events, and be contiguous in time.  Otherwise,  your model would be trained on events from \"the future\", at least as compared to the test data. It would then likely perform badly in practice, because you can’t really have access to data from the future. That means random sampling or cross validation don't apply to time series data. Decide on a training-versus-testing split, and divide your data into training and test datasets.\n",
    "\n",
    "In this case, you'll create the features together with two additional columns:\n",
    "\n",
    "* usdmxn_logret_positive, which is 1 if the log return of the USDMXN close is positive, and 0 otherwise. \n",
    "* usdmxn_logret_negative, which is 1 if the log return of the USDMXN close is negative, and 1 otherwise. \n",
    "\n",
    "We'll use 80% of our data for training and 20% for testing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 1: Binary Classification (BC) = Logistic Regression (LR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1: Step 01: Indicator Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 00: Original: Initialize indicator columns to 0\n",
    "#    0,   1,   2,  3, ..,  87\n",
    "# date, Tp1, Tp0, v1, .., v85\n",
    "\n",
    "#    0,   1,   2,  3, ..,  82\n",
    "# date, Tp1, Tp0, v1, .., v80\n",
    "\n",
    "dvislr['usdmxn_Tp1_logret_positive'] = 0 # col 88  ---> col (82+1)\n",
    "dvislr['usdmxn_Tp1_logret_negative'] = 0 # col 89\n",
    "\n",
    "dvislr['usdmxn_Tp0_logret_positive'] = 0 # col 90\n",
    "dvislr['usdmxn_Tp0_logret_negative'] = 0 # col 91\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dvislr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 00: Populate results columns according to actual usdmxn returns (positive or negative)\n",
    "\n",
    "# Predicting T+1: use col 1\n",
    "dvislr.ix[dvislr[1] >= 0, 'usdmxn_Tp1_logret_positive'] = 1\n",
    "dvislr.ix[dvislr[1] <  0, 'usdmxn_Tp1_logret_negative'] = 1\n",
    "\n",
    "# Predicting T+0: use col 2\n",
    "dvislr.ix[dvislr[2] >= 0, 'usdmxn_Tp0_logret_positive'] = 1\n",
    "dvislr.ix[dvislr[2] <  0, 'usdmxn_Tp0_logret_negative'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 01: Noise Cancellation\n",
    "# dvislr['usdmxn_Tp0_logret_positive_0050bp'] = 0\n",
    "# dvislr['usdmxn_Tp0_logret_negative_0050bp'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 01: Populate results columns according to actual usdmxn returns (outside noise zone above or below)\n",
    "# dvislr.ix[dvislr[2] >= 0.0050, 'usdmxn_Tp0_logret_positive_0050bp'] = 1\n",
    "# dvislr.ix[dvislr[2] <  -0.0050, 'usdmxn_Tp0_logret_negative_0050bp'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # 02: Stratification\n",
    "# dvislr['usdmxn_Tp0_logret_+0150bp_and_above'] = 0; dvislr['usdmxn_Tp0_logret_+0150bp_and_below'] = 0;\n",
    "# dvislr['usdmxn_Tp0_logret_+0140bp_and_above'] = 0; dvislr['usdmxn_Tp0_logret_+0140bp_and_below'] = 0;\n",
    "# dvislr['usdmxn_Tp0_logret_+0130bp_and_above'] = 0; dvislr['usdmxn_Tp0_logret_+0130bp_and_below'] = 0;\n",
    "# dvislr['usdmxn_Tp0_logret_+0120bp_and_above'] = 0; dvislr['usdmxn_Tp0_logret_+0120bp_and_below'] = 0;\n",
    "# dvislr['usdmxn_Tp0_logret_+0110bp_and_above'] = 0; dvislr['usdmxn_Tp0_logret_+0110bp_and_below'] = 0;\n",
    "# dvislr['usdmxn_Tp0_logret_+0100bp_and_above'] = 0; dvislr['usdmxn_Tp0_logret_+0100bp_and_below'] = 0;\n",
    "# dvislr['usdmxn_Tp0_logret_+0090bp_and_above'] = 0; dvislr['usdmxn_Tp0_logret_+0090bp_and_below'] = 0;\n",
    "# dvislr['usdmxn_Tp0_logret_+0080bp_and_above'] = 0; dvislr['usdmxn_Tp0_logret_+0080bp_and_below'] = 0;\n",
    "# dvislr['usdmxn_Tp0_logret_+0070bp_and_above'] = 0; dvislr['usdmxn_Tp0_logret_+0070bp_and_below'] = 0;\n",
    "# dvislr['usdmxn_Tp0_logret_+0060bp_and_above'] = 0; dvislr['usdmxn_Tp0_logret_+0060bp_and_below'] = 0;\n",
    "# dvislr['usdmxn_Tp0_logret_+0050bp_and_above'] = 0; dvislr['usdmxn_Tp0_logret_+0050bp_and_below'] = 0;\n",
    "# dvislr['usdmxn_Tp0_logret_+0040bp_and_above'] = 0; dvislr['usdmxn_Tp0_logret_+0040bp_and_below'] = 0;\n",
    "# dvislr['usdmxn_Tp0_logret_+0030bp_and_above'] = 0; dvislr['usdmxn_Tp0_logret_+0030bp_and_below'] = 0;\n",
    "# dvislr['usdmxn_Tp0_logret_+0020bp_and_above'] = 0; dvislr['usdmxn_Tp0_logret_+0020bp_and_below'] = 0;\n",
    "# dvislr['usdmxn_Tp0_logret_+0010bp_and_above'] = 0; dvislr['usdmxn_Tp0_logret_+0010bp_and_below'] = 0;\n",
    "# dvislr['usdmxn_Tp0_logret_+0000bp_and_above'] = 0; dvislr['usdmxn_Tp0_logret_+0000bp_and_below'] = 0;\n",
    "# dvislr['usdmxn_Tp0_logret_-0010bp_and_above'] = 0; dvislr['usdmxn_Tp0_logret_-0010bp_and_below'] = 0;\n",
    "# dvislr['usdmxn_Tp0_logret_-0020bp_and_above'] = 0; dvislr['usdmxn_Tp0_logret_-0020bp_and_below'] = 0;\n",
    "# dvislr['usdmxn_Tp0_logret_-0030bp_and_above'] = 0; dvislr['usdmxn_Tp0_logret_-0030bp_and_below'] = 0;\n",
    "# dvislr['usdmxn_Tp0_logret_-0040bp_and_above'] = 0; dvislr['usdmxn_Tp0_logret_-0040bp_and_below'] = 0;\n",
    "# dvislr['usdmxn_Tp0_logret_-0050bp_and_above'] = 0; dvislr['usdmxn_Tp0_logret_-0050bp_and_below'] = 0;\n",
    "# dvislr['usdmxn_Tp0_logret_-0060bp_and_above'] = 0; dvislr['usdmxn_Tp0_logret_-0060bp_and_below'] = 0;\n",
    "# dvislr['usdmxn_Tp0_logret_-0070bp_and_above'] = 0; dvislr['usdmxn_Tp0_logret_-0070bp_and_below'] = 0;\n",
    "# dvislr['usdmxn_Tp0_logret_-0080bp_and_above'] = 0; dvislr['usdmxn_Tp0_logret_-0080bp_and_below'] = 0;\n",
    "# dvislr['usdmxn_Tp0_logret_-0090bp_and_above'] = 0; dvislr['usdmxn_Tp0_logret_-0090bp_and_below'] = 0;\n",
    "# dvislr['usdmxn_Tp0_logret_-0100bp_and_above'] = 0; dvislr['usdmxn_Tp0_logret_-0100bp_and_below'] = 0;\n",
    "# dvislr['usdmxn_Tp0_logret_-0110bp_and_above'] = 0; dvislr['usdmxn_Tp0_logret_-0110bp_and_below'] = 0;\n",
    "# dvislr['usdmxn_Tp0_logret_-0120bp_and_above'] = 0; dvislr['usdmxn_Tp0_logret_-0120bp_and_below'] = 0;\n",
    "# dvislr['usdmxn_Tp0_logret_-0130bp_and_above'] = 0; dvislr['usdmxn_Tp0_logret_-0130bp_and_below'] = 0;\n",
    "# dvislr['usdmxn_Tp0_logret_-0140bp_and_above'] = 0; dvislr['usdmxn_Tp0_logret_-0140bp_and_below'] = 0;\n",
    "# dvislr['usdmxn_Tp0_logret_-0150bp_and_above'] = 0; dvislr['usdmxn_Tp0_logret_-0150bp_and_below'] = 0;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 02: Tp1 Stratification Initialize\n",
    "# WIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 02: Tp0 Stratification: Populate \n",
    "# WIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 02: Tp0 Stratification: Initialize\n",
    "for d_bp in xrange(-150,160,10): # -150:10:+150\n",
    "  d = d_bp / 10000.00\n",
    "  s_Tp0_above = 'usdmxn_Tp0_logret_' + '{0:+05d}'.format(d_bp) + 'bp_and_above'\n",
    "  s_Tp0_below = 'usdmxn_Tp0_logret_' + '{0:+05d}'.format(d_bp) + 'bp_and_below'\n",
    "  # print d\n",
    "  # print s_above\n",
    "  # print s_below\n",
    "  dvislr[s_Tp0_above] = 0;\n",
    "  dvislr[s_Tp0_below] = 0;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 02: Tp0 Stratification: Populate results columns according to actual usdmxn returns (above or below each level)\n",
    "for d_bp in xrange(-150,160,10): # -150:10:+150\n",
    "  d = d_bp / 10000.00\n",
    "  s_Tp0_above = 'usdmxn_Tp0_logret_' + '{0:+05d}'.format(d_bp) + 'bp_and_above'\n",
    "  s_Tp0_below = 'usdmxn_Tp0_logret_' + '{0:+05d}'.format(d_bp) + 'bp_and_below'\n",
    "  # print d\n",
    "  # print s_above\n",
    "  # print s_below\n",
    "  dvislr.ix[dvislr[1] >= d, s_Tp0_above] = 1\n",
    "  dvislr.ix[dvislr[1] <  d, s_Tp0_below] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dvislr.head()\n",
    "# 87 inputs + 4 user cols + (15+1+15)*2=62   = 155 cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dvislr.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1: Step 01.01: Remove data noise in a stratified way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# dvislr = data_values_indexed_scaled_logret\n",
    "# dvislrnr = data_values_indexed_scaled_logret (noise removed)\n",
    "\n",
    "# Example Filter: df[(df.A == 1) & (df.D == 6)]\n",
    "dvislr_nr_m0010_z0000_p0010 = dvislr[ (dvislr['usdmxn_Tp0_logret_-0010bp_and_below'] == 1) | (dvislr['usdmxn_Tp0_logret_+0010bp_and_above'] == 1) ] # remove data between [-0.10%, +0.10%], center at +0.00%\n",
    "dvislr_nr_m0020_z0000_p0020 = dvislr[ (dvislr['usdmxn_Tp0_logret_-0020bp_and_below'] == 1) | (dvislr['usdmxn_Tp0_logret_+0020bp_and_above'] == 1) ] # remove data between [-0.10%, +0.10%], center at +0.00%\n",
    "dvislr_nr_m0030_z0000_p0030 = dvislr[ (dvislr['usdmxn_Tp0_logret_-0030bp_and_below'] == 1) | (dvislr['usdmxn_Tp0_logret_+0030bp_and_above'] == 1) ] # remove data between [-0.10%, +0.10%], center at +0.00%\n",
    "dvislr_nr_m0040_z0000_p0040 = dvislr[ (dvislr['usdmxn_Tp0_logret_-0040bp_and_below'] == 1) | (dvislr['usdmxn_Tp0_logret_+0040bp_and_above'] == 1) ] # remove data between [-0.10%, +0.10%], center at +0.00%\n",
    "dvislr_nr_m0050_z0000_p0050 = dvislr[ (dvislr['usdmxn_Tp0_logret_-0050bp_and_below'] == 1) | (dvislr['usdmxn_Tp0_logret_+0050bp_and_above'] == 1) ] # remove data between [-0.50%, +0.50%], center at +0.00%\n",
    "dvislr_nr_m0060_z0000_p0060 = dvislr[ (dvislr['usdmxn_Tp0_logret_-0060bp_and_below'] == 1) | (dvislr['usdmxn_Tp0_logret_+0060bp_and_above'] == 1) ] # remove data between [-0.50%, +0.50%], center at +0.00%\n",
    "dvislr_nr_m0070_z0000_p0070 = dvislr[ (dvislr['usdmxn_Tp0_logret_-0070bp_and_below'] == 1) | (dvislr['usdmxn_Tp0_logret_+0070bp_and_above'] == 1) ] # remove data between [-0.50%, +0.50%], center at +0.00%\n",
    "dvislr_nr_m0080_z0000_p0080 = dvislr[ (dvislr['usdmxn_Tp0_logret_-0080bp_and_below'] == 1) | (dvislr['usdmxn_Tp0_logret_+0080bp_and_above'] == 1) ] # remove data between [-0.50%, +0.50%], center at +0.00%\n",
    "dvislr_nr_m0090_z0000_p0090 = dvislr[ (dvislr['usdmxn_Tp0_logret_-0090bp_and_below'] == 1) | (dvislr['usdmxn_Tp0_logret_+0090bp_and_above'] == 1) ] # remove data between [-0.50%, +0.50%], center at +0.00%\n",
    "dvislr_nr_m0100_z0000_p0100 = dvislr[ (dvislr['usdmxn_Tp0_logret_-0100bp_and_below'] == 1) | (dvislr['usdmxn_Tp0_logret_+0100bp_and_above'] == 1) ] # remove data between [-0.50%, +0.50%], center at +0.00%\n",
    "\n",
    "# check rows\n",
    "print 'dvislr_nr_m0010_z0000_p0010 rows =' + str(len(dvislr_nr_m0010_z0000_p0010))\n",
    "print 'dvislr_nr_m0020_z0000_p0020 rows =' + str(len(dvislr_nr_m0020_z0000_p0020))\n",
    "print 'dvislr_nr_m0030_z0000_p0030 rows =' + str(len(dvislr_nr_m0030_z0000_p0030))\n",
    "print 'dvislr_nr_m0040_z0000_p0040 rows =' + str(len(dvislr_nr_m0040_z0000_p0040))\n",
    "print 'dvislr_nr_m0050_z0000_p0050 rows =' + str(len(dvislr_nr_m0050_z0000_p0050))\n",
    "print 'dvislr_nr_m0060_z0000_p0060 rows =' + str(len(dvislr_nr_m0060_z0000_p0060))\n",
    "print 'dvislr_nr_m0070_z0000_p0070 rows =' + str(len(dvislr_nr_m0070_z0000_p0070))\n",
    "print 'dvislr_nr_m0080_z0000_p0080 rows =' + str(len(dvislr_nr_m0080_z0000_p0080))\n",
    "print 'dvislr_nr_m0090_z0000_p0090 rows =' + str(len(dvislr_nr_m0090_z0000_p0090))\n",
    "print 'dvislr_nr_m0100_z0000_p0100 rows =' + str(len(dvislr_nr_m0100_z0000_p0100))\n",
    "\n",
    "# check rows results\n",
    "# dvislr_nr_m0010_z0000_p0010 rows =3553\n",
    "# dvislr_nr_m0020_z0000_p0020 rows =2903\n",
    "# dvislr_nr_m0030_z0000_p0030 rows =2315\n",
    "# dvislr_nr_m0040_z0000_p0040 rows =1836\n",
    "# dvislr_nr_m0050_z0000_p0050 rows =1436\n",
    "# dvislr_nr_m0060_z0000_p0060 rows =1134\n",
    "# dvislr_nr_m0070_z0000_p0070 rows =893\n",
    "# dvislr_nr_m0080_z0000_p0080 rows =697\n",
    "# dvislr_nr_m0090_z0000_p0090 rows =559\n",
    "# dvislr_nr_m0100_z0000_p0100 rows =439"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1: Step 02: Split data into training, test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### training test data empty shell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# training_test_data\n",
    "# col 001-002 = 02 cols = Tp1 up dn indicators of results\n",
    "# col 003-004 = 02 cols = Tp0 up dn indicators of results\n",
    "# col 005-066 = 62 cols = (15+1+15)*2\n",
    "# col 067-153 = 87 cols = 87 inputs\n",
    "training_test_data = pd.DataFrame(\n",
    "  columns=[\n",
    "    # 'usdmxn_Tp0_logret_positive',        'usdmxn_Tp0_logret_negative',\n",
    "    # 'usdmxn_Tp0_logret_positive_0050bp', 'usdmxn_Tp0_logret_negative_0050bp',\n",
    "\n",
    "    'bc_Tp1_up', \n",
    "    'bc_Tp1_dn',\n",
    "    \n",
    "    'bc_Tp0_up', \n",
    "    'bc_Tp0_dn',\n",
    "    \n",
    "    'usdmxn_Tp0_logret_-0150bp_and_above', 'usdmxn_Tp0_logret_-0150bp_and_below',\n",
    "    'usdmxn_Tp0_logret_-0140bp_and_above', 'usdmxn_Tp0_logret_-0140bp_and_below',\n",
    "    'usdmxn_Tp0_logret_-0130bp_and_above', 'usdmxn_Tp0_logret_-0130bp_and_below',\n",
    "    'usdmxn_Tp0_logret_-0120bp_and_above', 'usdmxn_Tp0_logret_-0120bp_and_below',\n",
    "    'usdmxn_Tp0_logret_-0110bp_and_above', 'usdmxn_Tp0_logret_-0110bp_and_below',\n",
    "    'usdmxn_Tp0_logret_-0100bp_and_above', 'usdmxn_Tp0_logret_-0100bp_and_below',\n",
    "    'usdmxn_Tp0_logret_-0090bp_and_above', 'usdmxn_Tp0_logret_-0090bp_and_below',\n",
    "    'usdmxn_Tp0_logret_-0080bp_and_above', 'usdmxn_Tp0_logret_-0080bp_and_below',\n",
    "    'usdmxn_Tp0_logret_-0070bp_and_above', 'usdmxn_Tp0_logret_-0070bp_and_below',\n",
    "    'usdmxn_Tp0_logret_-0060bp_and_above', 'usdmxn_Tp0_logret_-0060bp_and_below',\n",
    "    'usdmxn_Tp0_logret_-0050bp_and_above', 'usdmxn_Tp0_logret_-0050bp_and_below',\n",
    "    'usdmxn_Tp0_logret_-0040bp_and_above', 'usdmxn_Tp0_logret_-0040bp_and_below',\n",
    "    'usdmxn_Tp0_logret_-0030bp_and_above', 'usdmxn_Tp0_logret_-0030bp_and_below',\n",
    "    'usdmxn_Tp0_logret_-0020bp_and_above', 'usdmxn_Tp0_logret_-0020bp_and_below',\n",
    "    'usdmxn_Tp0_logret_-0010bp_and_above', 'usdmxn_Tp0_logret_-0010bp_and_below',\n",
    "    'usdmxn_Tp0_logret_+0000bp_and_above', 'usdmxn_Tp0_logret_+0000bp_and_below',\n",
    "    'usdmxn_Tp0_logret_+0010bp_and_above', 'usdmxn_Tp0_logret_+0010bp_and_below',\n",
    "    'usdmxn_Tp0_logret_+0020bp_and_above', 'usdmxn_Tp0_logret_+0020bp_and_below',\n",
    "    'usdmxn_Tp0_logret_+0030bp_and_above', 'usdmxn_Tp0_logret_+0030bp_and_below',\n",
    "    'usdmxn_Tp0_logret_+0040bp_and_above', 'usdmxn_Tp0_logret_+0040bp_and_below',\n",
    "    'usdmxn_Tp0_logret_+0050bp_and_above', 'usdmxn_Tp0_logret_+0050bp_and_below',\n",
    "    'usdmxn_Tp0_logret_+0060bp_and_above', 'usdmxn_Tp0_logret_+0060bp_and_below',\n",
    "    'usdmxn_Tp0_logret_+0070bp_and_above', 'usdmxn_Tp0_logret_+0070bp_and_below',\n",
    "    'usdmxn_Tp0_logret_+0080bp_and_above', 'usdmxn_Tp0_logret_+0080bp_and_below',\n",
    "    'usdmxn_Tp0_logret_+0090bp_and_above', 'usdmxn_Tp0_logret_+0090bp_and_below',\n",
    "    'usdmxn_Tp0_logret_+0100bp_and_above', 'usdmxn_Tp0_logret_+0100bp_and_below',\n",
    "    'usdmxn_Tp0_logret_+0110bp_and_above', 'usdmxn_Tp0_logret_+0110bp_and_below',\n",
    "    'usdmxn_Tp0_logret_+0120bp_and_above', 'usdmxn_Tp0_logret_+0120bp_and_below',\n",
    "    'usdmxn_Tp0_logret_+0130bp_and_above', 'usdmxn_Tp0_logret_+0130bp_and_below',\n",
    "    'usdmxn_Tp0_logret_+0140bp_and_above', 'usdmxn_Tp0_logret_+0140bp_and_below',\n",
    "    'usdmxn_Tp0_logret_+0150bp_and_above', 'usdmxn_Tp0_logret_+0150bp_and_below',\n",
    "\n",
    "    # XXX 1st - change this\n",
    "    '1','2','3','4','5','6','7','8','9','10',\n",
    "    '11','12','13','14','15','16','17','18','19','20',\n",
    "    '21','22','23','24','25','26','27','28','29','30',\n",
    "    '31','32','33','34','35','36','37','38','39','40',\n",
    "    '41','42','43','44','45','46','47','48','49','50',\n",
    "    '51','52','53','54','55','56','57','58','59','60',\n",
    "    '61','62','63','64','65','66','67','68','69','70',\n",
    "    '71','72','73','74','75','76','77','78','79','80',\n",
    "    '81','82','83','84','85','86','87'\n",
    "  ])\n",
    "\n",
    "training_test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(dvislr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data into variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# row 0      = header\n",
    "# row 1-4279 = data\n",
    "# NumRowsIncludeHeader = len(data_values_indexed_scaled_logret) = 4280 = 0-4279\n",
    "# Start from row 7, so we can have up to (7-1) lookback days\n",
    "\n",
    "# check rows results\n",
    "# dvislr_nr_m0010_z0000_p0010 rows =3554\n",
    "# dvislr_nr_m0020_z0000_p0020 rows =2904\n",
    "# dvislr_nr_m0030_z0000_p0030 rows =2316\n",
    "# dvislr_nr_m0040_z0000_p0040 rows =1837\n",
    "# dvislr_nr_m0050_z0000_p0050 rows =1437\n",
    "# dvislr_nr_m0060_z0000_p0060 rows =1135\n",
    "# dvislr_nr_m0070_z0000_p0070 rows =893\n",
    "# dvislr_nr_m0080_z0000_p0080 rows =697\n",
    "# dvislr_nr_m0090_z0000_p0090 rows =559\n",
    "# dvislr_nr_m0100_z0000_p0100 rows =439\n",
    "\n",
    "#==========================================================================================\n",
    "# Select the data\n",
    "#==========================================================================================\n",
    "# dvislr =  dvislr                          # default\n",
    "# dvislr =  dvislr_nr_m0010_z0000_p0010\n",
    "# dvislr =  dvislr_nr_m0020_z0000_p0020\n",
    "# dvislr =  dvislr_nr_m0030_z0000_p0030\n",
    "# dvislr =  dvislr_nr_m0040_z0000_p0040\n",
    "# dvislr =  dvislr_nr_m0050_z0000_p0050\n",
    "# dvislr =  dvislr_nr_m0060_z0000_p0060\n",
    "# dvislr =  dvislr_nr_m0070_z0000_p0070\n",
    "# dvislr =  dvislr_nr_m0080_z0000_p0080\n",
    "# dvislr =  dvislr_nr_m0090_z0000_p0090\n",
    "# dvislr =  dvislr_nr_m0100_z0000_p0100\n",
    "\n",
    "# for row i\n",
    "for i in range(7, len(dvislr)): # = range(7, 4280) =  [7, 4280) = [7,4279] \n",
    "\n",
    "  #==========================================================================================\n",
    "  usdmxn_Tp1_logret_positive = dvislr['usdmxn_Tp1_logret_positive'].ix[i] # col 1\n",
    "  usdmxn_Tp1_logret_negative = dvislr['usdmxn_Tp1_logret_negative'].ix[i] # col 2\n",
    "  usdmxn_Tp0_logret_positive = dvislr['usdmxn_Tp0_logret_positive'].ix[i] # col 3\n",
    "  usdmxn_Tp0_logret_negative = dvislr['usdmxn_Tp0_logret_negative'].ix[i] # col 4\n",
    "  #==========================================================================================\n",
    "  bc_Tp1_up = usdmxn_Tp1_logret_positive; # Tp1 up # col 1\n",
    "  bc_Tp1_dn = usdmxn_Tp1_logret_negative; # Tp1 dn # col 2\n",
    "  bc_Tp0_up = usdmxn_Tp0_logret_positive; # Tp0 up # col 3\n",
    "  bc_Tp0_dn = usdmxn_Tp0_logret_negative; # Tp0 dn # col 4\n",
    "  #==========================================================================================\n",
    "\n",
    "  # 02: stratification  \n",
    "  usdmxn_Tp0_logret_p0150bp_and_below = dvislr['usdmxn_Tp0_logret_+0150bp_and_below'].ix[i]; usdmxn_Tp0_logret_p0150bp_and_above = dvislr['usdmxn_Tp0_logret_+0150bp_and_above'].ix[i];\n",
    "  usdmxn_Tp0_logret_p0140bp_and_below = dvislr['usdmxn_Tp0_logret_+0140bp_and_below'].ix[i]; usdmxn_Tp0_logret_p0140bp_and_above = dvislr['usdmxn_Tp0_logret_+0140bp_and_above'].ix[i];\n",
    "  usdmxn_Tp0_logret_p0130bp_and_below = dvislr['usdmxn_Tp0_logret_+0130bp_and_below'].ix[i]; usdmxn_Tp0_logret_p0130bp_and_above = dvislr['usdmxn_Tp0_logret_+0130bp_and_above'].ix[i];\n",
    "  usdmxn_Tp0_logret_p0120bp_and_below = dvislr['usdmxn_Tp0_logret_+0120bp_and_below'].ix[i]; usdmxn_Tp0_logret_p0120bp_and_above = dvislr['usdmxn_Tp0_logret_+0120bp_and_above'].ix[i];\n",
    "  usdmxn_Tp0_logret_p0110bp_and_below = dvislr['usdmxn_Tp0_logret_+0110bp_and_below'].ix[i]; usdmxn_Tp0_logret_p0110bp_and_above = dvislr['usdmxn_Tp0_logret_+0110bp_and_above'].ix[i];\n",
    "  \n",
    "  usdmxn_Tp0_logret_p0100bp_and_below = dvislr['usdmxn_Tp0_logret_+0100bp_and_below'].ix[i]; usdmxn_Tp0_logret_p0100bp_and_above = dvislr['usdmxn_Tp0_logret_+0100bp_and_above'].ix[i];\n",
    "  usdmxn_Tp0_logret_p0090bp_and_below = dvislr['usdmxn_Tp0_logret_+0090bp_and_below'].ix[i]; usdmxn_Tp0_logret_p0090bp_and_above = dvislr['usdmxn_Tp0_logret_+0090bp_and_above'].ix[i];\n",
    "  usdmxn_Tp0_logret_p0080bp_and_below = dvislr['usdmxn_Tp0_logret_+0080bp_and_below'].ix[i]; usdmxn_Tp0_logret_p0080bp_and_above = dvislr['usdmxn_Tp0_logret_+0080bp_and_above'].ix[i];\n",
    "  usdmxn_Tp0_logret_p0070bp_and_below = dvislr['usdmxn_Tp0_logret_+0070bp_and_below'].ix[i]; usdmxn_Tp0_logret_p0070bp_and_above = dvislr['usdmxn_Tp0_logret_+0070bp_and_above'].ix[i];\n",
    "  usdmxn_Tp0_logret_p0060bp_and_below = dvislr['usdmxn_Tp0_logret_+0060bp_and_below'].ix[i]; usdmxn_Tp0_logret_p0060bp_and_above = dvislr['usdmxn_Tp0_logret_+0060bp_and_above'].ix[i];\n",
    "\n",
    "  usdmxn_Tp0_logret_p0050bp_and_below = dvislr['usdmxn_Tp0_logret_+0050bp_and_below'].ix[i]; usdmxn_Tp0_logret_p0050bp_and_above = dvislr['usdmxn_Tp0_logret_+0050bp_and_above'].ix[i];\n",
    "  usdmxn_Tp0_logret_p0040bp_and_below = dvislr['usdmxn_Tp0_logret_+0040bp_and_below'].ix[i]; usdmxn_Tp0_logret_p0040bp_and_above = dvislr['usdmxn_Tp0_logret_+0040bp_and_above'].ix[i];\n",
    "  usdmxn_Tp0_logret_p0030bp_and_below = dvislr['usdmxn_Tp0_logret_+0030bp_and_below'].ix[i]; usdmxn_Tp0_logret_p0030bp_and_above = dvislr['usdmxn_Tp0_logret_+0030bp_and_above'].ix[i];\n",
    "  usdmxn_Tp0_logret_p0020bp_and_below = dvislr['usdmxn_Tp0_logret_+0020bp_and_below'].ix[i]; usdmxn_Tp0_logret_p0020bp_and_above = dvislr['usdmxn_Tp0_logret_+0020bp_and_above'].ix[i];\n",
    "  usdmxn_Tp0_logret_p0010bp_and_below = dvislr['usdmxn_Tp0_logret_+0010bp_and_below'].ix[i]; usdmxn_Tp0_logret_p0010bp_and_above = dvislr['usdmxn_Tp0_logret_+0010bp_and_above'].ix[i];\n",
    "\n",
    "  usdmxn_Tp0_logret_p0000bp_and_below = dvislr['usdmxn_Tp0_logret_+0000bp_and_below'].ix[i]; usdmxn_Tp0_logret_p0000bp_and_above = dvislr['usdmxn_Tp0_logret_+0000bp_and_above'].ix[i];\n",
    "\n",
    "  usdmxn_Tp0_logret_m0010bp_and_below = dvislr['usdmxn_Tp0_logret_-0010bp_and_below'].ix[i]; usdmxn_Tp0_logret_m0010bp_and_above = dvislr['usdmxn_Tp0_logret_-0010bp_and_above'].ix[i];\n",
    "  usdmxn_Tp0_logret_m0020bp_and_below = dvislr['usdmxn_Tp0_logret_-0020bp_and_below'].ix[i]; usdmxn_Tp0_logret_m0020bp_and_above = dvislr['usdmxn_Tp0_logret_-0020bp_and_above'].ix[i];\n",
    "  usdmxn_Tp0_logret_m0030bp_and_below = dvislr['usdmxn_Tp0_logret_-0030bp_and_below'].ix[i]; usdmxn_Tp0_logret_m0030bp_and_above = dvislr['usdmxn_Tp0_logret_-0030bp_and_above'].ix[i];\n",
    "  usdmxn_Tp0_logret_m0040bp_and_below = dvislr['usdmxn_Tp0_logret_-0040bp_and_below'].ix[i]; usdmxn_Tp0_logret_m0040bp_and_above = dvislr['usdmxn_Tp0_logret_-0040bp_and_above'].ix[i];\n",
    "  usdmxn_Tp0_logret_m0050bp_and_below = dvislr['usdmxn_Tp0_logret_-0050bp_and_below'].ix[i]; usdmxn_Tp0_logret_m0050bp_and_above = dvislr['usdmxn_Tp0_logret_-0050bp_and_above'].ix[i];\n",
    "\n",
    "  usdmxn_Tp0_logret_m0060bp_and_below = dvislr['usdmxn_Tp0_logret_-0060bp_and_below'].ix[i]; usdmxn_Tp0_logret_m0060bp_and_above = dvislr['usdmxn_Tp0_logret_-0060bp_and_above'].ix[i];\n",
    "  usdmxn_Tp0_logret_m0070bp_and_below = dvislr['usdmxn_Tp0_logret_-0070bp_and_below'].ix[i]; usdmxn_Tp0_logret_m0070bp_and_above = dvislr['usdmxn_Tp0_logret_-0070bp_and_above'].ix[i];\n",
    "  usdmxn_Tp0_logret_m0080bp_and_below = dvislr['usdmxn_Tp0_logret_-0080bp_and_below'].ix[i]; usdmxn_Tp0_logret_m0080bp_and_above = dvislr['usdmxn_Tp0_logret_-0080bp_and_above'].ix[i];\n",
    "  usdmxn_Tp0_logret_m0090bp_and_below = dvislr['usdmxn_Tp0_logret_-0090bp_and_below'].ix[i]; usdmxn_Tp0_logret_m0090bp_and_above = dvislr['usdmxn_Tp0_logret_-0090bp_and_above'].ix[i];\n",
    "  usdmxn_Tp0_logret_m0100bp_and_below = dvislr['usdmxn_Tp0_logret_-0100bp_and_below'].ix[i]; usdmxn_Tp0_logret_m0100bp_and_above = dvislr['usdmxn_Tp0_logret_-0100bp_and_above'].ix[i];\n",
    "\n",
    "  usdmxn_Tp0_logret_m0110bp_and_below = dvislr['usdmxn_Tp0_logret_-0110bp_and_below'].ix[i]; usdmxn_Tp0_logret_m0110bp_and_above = dvislr['usdmxn_Tp0_logret_-0110bp_and_above'].ix[i];\n",
    "  usdmxn_Tp0_logret_m0120bp_and_below = dvislr['usdmxn_Tp0_logret_-0120bp_and_below'].ix[i]; usdmxn_Tp0_logret_m0120bp_and_above = dvislr['usdmxn_Tp0_logret_-0120bp_and_above'].ix[i];\n",
    "  usdmxn_Tp0_logret_m0130bp_and_below = dvislr['usdmxn_Tp0_logret_-0130bp_and_below'].ix[i]; usdmxn_Tp0_logret_m0130bp_and_above = dvislr['usdmxn_Tp0_logret_-0130bp_and_above'].ix[i];\n",
    "  usdmxn_Tp0_logret_m0140bp_and_below = dvislr['usdmxn_Tp0_logret_-0140bp_and_below'].ix[i]; usdmxn_Tp0_logret_m0140bp_and_above = dvislr['usdmxn_Tp0_logret_-0140bp_and_above'].ix[i];\n",
    "  usdmxn_Tp0_logret_m0150bp_and_below = dvislr['usdmxn_Tp0_logret_-0150bp_and_below'].ix[i]; usdmxn_Tp0_logret_m0150bp_and_above = dvislr['usdmxn_Tp0_logret_-0150bp_and_above'].ix[i];\n",
    "\n",
    "  #==========================================================================================  \n",
    "#   bc_Tp0_dn = usdmxn_Tp0_logret_m0010bp_and_below; bc_Tp0_up = usdmxn_Tp0_logret_p0010bp_and_above;\n",
    "#   bc_Tp0_dn = usdmxn_Tp0_logret_m0020bp_and_below; bc_Tp0_up = usdmxn_Tp0_logret_p0020bp_and_above;\n",
    "#   bc_Tp0_dn = usdmxn_Tp0_logret_m0030bp_and_below; bc_Tp0_up = usdmxn_Tp0_logret_p0030bp_and_above;\n",
    "#   bc_Tp0_dn = usdmxn_Tp0_logret_m0040bp_and_below; bc_Tp0_up = usdmxn_Tp0_logret_p0040bp_and_above;\n",
    "#   bc_Tp0_dn = usdmxn_Tp0_logret_m0050bp_and_below; bc_Tp0_up = usdmxn_Tp0_logret_p0050bp_and_above;\n",
    "#   bc_Tp0_dn = usdmxn_Tp0_logret_m0060bp_and_below; bc_Tp0_up = usdmxn_Tp0_logret_p0060bp_and_above;\n",
    "#   bc_Tp0_dn = usdmxn_Tp0_logret_m0070bp_and_below; bc_Tp0_up = usdmxn_Tp0_logret_p0070bp_and_above;\n",
    "#   bc_Tp0_dn = usdmxn_Tp0_logret_m0080bp_and_below; bc_Tp0_up = usdmxn_Tp0_logret_p0080bp_and_above;\n",
    "#   bc_Tp0_dn = usdmxn_Tp0_logret_m0090bp_and_below; bc_Tp0_up = usdmxn_Tp0_logret_p0090bp_and_above;\n",
    "#   bc_Tp0_dn = usdmxn_Tp0_logret_m0100bp_and_below; bc_Tp0_up = usdmxn_Tp0_logret_p0100bp_and_above;\n",
    "  \n",
    "  #   v_001_Tm005 = dvislr['1'].ix[i-5]  # lookback 1d\n",
    "  # col 067 = dvislr[1]\n",
    "  # col 068 = dvislr[2]\n",
    "  # ...\n",
    "  # col 153 = dvislr[87]\n",
    "  \n",
    "  # XXX 2nd - changethis\n",
    "  v_001_Tm000 = dvislr[1].ix[i];  v_002_Tm000 = dvislr[2].ix[i];  v_003_Tm000 = dvislr[3].ix[i];  v_004_Tm000 = dvislr[4].ix[i];  v_005_Tm000 = dvislr[5].ix[i];\n",
    "  v_006_Tm000 = dvislr[6].ix[i];  v_007_Tm000 = dvislr[7].ix[i];  v_008_Tm000 = dvislr[8].ix[i];  v_009_Tm000 = dvislr[9].ix[i];  v_010_Tm000 = dvislr[10].ix[i];\n",
    "  v_011_Tm000 = dvislr[11].ix[i]; v_012_Tm000 = dvislr[12].ix[i]; v_013_Tm000 = dvislr[13].ix[i]; v_014_Tm000 = dvislr[14].ix[i]; v_015_Tm000 = dvislr[15].ix[i];\n",
    "  v_016_Tm000 = dvislr[16].ix[i]; v_017_Tm000 = dvislr[17].ix[i]; v_018_Tm000 = dvislr[18].ix[i]; v_019_Tm000 = dvislr[19].ix[i]; v_020_Tm000 = dvislr[20].ix[i];  \n",
    "  v_021_Tm000 = dvislr[21].ix[i]; v_022_Tm000 = dvislr[22].ix[i]; v_023_Tm000 = dvislr[23].ix[i]; v_024_Tm000 = dvislr[24].ix[i]; v_025_Tm000 = dvislr[25].ix[i];\n",
    "  v_026_Tm000 = dvislr[26].ix[i]; v_027_Tm000 = dvislr[27].ix[i]; v_028_Tm000 = dvislr[28].ix[i]; v_029_Tm000 = dvislr[29].ix[i]; v_030_Tm000 = dvislr[30].ix[i];\n",
    "  v_031_Tm000 = dvislr[31].ix[i]; v_032_Tm000 = dvislr[32].ix[i]; v_033_Tm000 = dvislr[33].ix[i]; v_034_Tm000 = dvislr[34].ix[i]; v_035_Tm000 = dvislr[35].ix[i];\n",
    "  v_036_Tm000 = dvislr[36].ix[i]; v_037_Tm000 = dvislr[37].ix[i]; v_038_Tm000 = dvislr[38].ix[i]; v_039_Tm000 = dvislr[39].ix[i]; v_040_Tm000 = dvislr[40].ix[i];\n",
    "  v_041_Tm000 = dvislr[41].ix[i]; v_042_Tm000 = dvislr[42].ix[i]; v_043_Tm000 = dvislr[43].ix[i]; v_044_Tm000 = dvislr[44].ix[i]; v_045_Tm000 = dvislr[45].ix[i];\n",
    "  v_046_Tm000 = dvislr[46].ix[i]; v_047_Tm000 = dvislr[47].ix[i]; v_048_Tm000 = dvislr[48].ix[i]; v_049_Tm000 = dvislr[49].ix[i]; v_050_Tm000 = dvislr[50].ix[i];\n",
    "  v_051_Tm000 = dvislr[51].ix[i]; v_052_Tm000 = dvislr[52].ix[i]; v_053_Tm000 = dvislr[53].ix[i]; v_054_Tm000 = dvislr[54].ix[i]; v_055_Tm000 = dvislr[55].ix[i];\n",
    "  v_056_Tm000 = dvislr[56].ix[i]; v_057_Tm000 = dvislr[57].ix[i]; v_058_Tm000 = dvislr[58].ix[i]; v_059_Tm000 = dvislr[59].ix[i]; v_060_Tm000 = dvislr[60].ix[i];\n",
    "  v_061_Tm000 = dvislr[61].ix[i]; v_062_Tm000 = dvislr[62].ix[i]; v_063_Tm000 = dvislr[63].ix[i]; v_064_Tm000 = dvislr[64].ix[i]; v_065_Tm000 = dvislr[65].ix[i];\n",
    "  v_066_Tm000 = dvislr[66].ix[i]; v_067_Tm000 = dvislr[67].ix[i]; v_068_Tm000 = dvislr[68].ix[i]; v_069_Tm000 = dvislr[69].ix[i]; v_070_Tm000 = dvislr[70].ix[i];\n",
    "  v_071_Tm000 = dvislr[71].ix[i]; v_072_Tm000 = dvislr[72].ix[i]; v_073_Tm000 = dvislr[73].ix[i]; v_074_Tm000 = dvislr[74].ix[i]; v_075_Tm000 = dvislr[75].ix[i];\n",
    "  v_076_Tm000 = dvislr[76].ix[i]; v_077_Tm000 = dvislr[77].ix[i]; v_078_Tm000 = dvislr[78].ix[i]; v_079_Tm000 = dvislr[79].ix[i]; v_080_Tm000 = dvislr[80].ix[i];\n",
    "  v_081_Tm000 = dvislr[81].ix[i]; v_082_Tm000 = dvislr[82].ix[i]; v_083_Tm000 = dvislr[83].ix[i]; v_084_Tm000 = dvislr[84].ix[i]; v_085_Tm000 = dvislr[85].ix[i];\n",
    "  v_086_Tm000 = dvislr[86].ix[i]; v_087_Tm000 = dvislr[87].ix[i];\n",
    "\n",
    "  # training_test_data: append data\n",
    "  training_test_data = training_test_data.append(\n",
    "    {\n",
    "#       'usdmxn_Tp0_logret_positive':usdmxn_Tp0_logret_positive,\n",
    "#       'usdmxn_Tp0_logret_negative':usdmxn_Tp0_logret_negative,\n",
    "#       'usdmxn_Tp0_logret_positive_0050bp':usdmxn_Tp0_logret_positive_0050bp,\n",
    "#       'usdmxn_Tp0_logret_negative_0050bp':usdmxn_Tp0_logret_negative_0050bp,\n",
    "      \n",
    "      'bc_Tp1_up':bc_Tp1_up,\n",
    "      'bc_Tp1_dn':bc_Tp1_dn,\n",
    "      \n",
    "      'bc_Tp0_up':bc_Tp0_up,\n",
    "      'bc_Tp0_dn':bc_Tp0_dn,\n",
    "      \n",
    "      # '1':v_001_Tm000 = USDMXN Curncy_Tp1\n",
    "      # '2':v_002_Tm000 = USDMXN Curncy_Tp0\n",
    "      \n",
    "      # XXX 3rd\n",
    "      '1':v_001_Tm000,  '2':v_002_Tm000, \n",
    "      \n",
    "      #  v3 to v87 = 85 econ variables(inputs)\n",
    "      '3':v_003_Tm000, '4':v_004_Tm000, '5':v_005_Tm000,\n",
    "      '6':v_006_Tm000,  '7':v_007_Tm000, '8':v_008_Tm000, '9':v_009_Tm000, '10':v_010_Tm000,\n",
    "\n",
    "      '11':v_011_Tm000, '12':v_012_Tm000, '13':v_013_Tm000, '14':v_014_Tm000, '15':v_015_Tm000,\n",
    "      '16':v_016_Tm000, '17':v_017_Tm000, '18':v_018_Tm000, '19':v_019_Tm000, '20':v_020_Tm000,\n",
    "\n",
    "      '21':v_021_Tm000, '22':v_022_Tm000, '23':v_023_Tm000, '24':v_024_Tm000, '25':v_025_Tm000,\n",
    "      '26':v_026_Tm000, '27':v_027_Tm000, '28':v_028_Tm000, '29':v_029_Tm000, '30':v_030_Tm000,\n",
    "\n",
    "      '31':v_031_Tm000, '32':v_032_Tm000, '33':v_033_Tm000, '34':v_034_Tm000, '35':v_035_Tm000,\n",
    "      '36':v_036_Tm000, '37':v_037_Tm000, '38':v_038_Tm000, '39':v_039_Tm000, '40':v_040_Tm000,\n",
    "\n",
    "      '41':v_041_Tm000, '42':v_042_Tm000, '43':v_043_Tm000, '44':v_044_Tm000, '45':v_045_Tm000,\n",
    "      '46':v_046_Tm000, '47':v_047_Tm000, '48':v_048_Tm000, '49':v_049_Tm000, '50':v_050_Tm000,\n",
    "\n",
    "      '51':v_051_Tm000, '52':v_052_Tm000, '53':v_053_Tm000, '54':v_054_Tm000, '55':v_055_Tm000,\n",
    "      '56':v_056_Tm000, '57':v_057_Tm000, '58':v_058_Tm000, '59':v_059_Tm000, '60':v_060_Tm000,\n",
    "\n",
    "      '61':v_061_Tm000, '62':v_062_Tm000, '63':v_063_Tm000, '64':v_064_Tm000, '65':v_065_Tm000,\n",
    "      '66':v_066_Tm000, '67':v_067_Tm000, '68':v_068_Tm000, '69':v_069_Tm000, '70':v_070_Tm000,\n",
    "\n",
    "      '71':v_071_Tm000, '72':v_072_Tm000, '73':v_073_Tm000, '74':v_074_Tm000, '75':v_075_Tm000,\n",
    "      '76':v_076_Tm000, '77':v_077_Tm000, '78':v_078_Tm000, '79':v_079_Tm000, '80':v_080_Tm000,\n",
    "      \n",
    "      '81':v_081_Tm000, '82':v_082_Tm000, '83':v_083_Tm000, '84':v_084_Tm000, '85':v_085_Tm000,\n",
    "      '86':v_086_Tm000, '87':v_087_Tm000, \n",
    "      \n",
    "    },\n",
    "    ignore_index=True)\n",
    "  \n",
    "# data_values_indexed_scaled_logret: row [7,4279] \n",
    "# training_test_data               : row [0,4272] = 4273 rows\n",
    "# training_test_data: col 01-02 = 02 cols = binary outputs\n",
    "# training_test_data: col 03-89 = 87 cols = inputs  \n",
    "\n",
    "training_test_data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "training_test_data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# training_test_data.describe()\n",
    "training_test_data[['bc_Tp1_up','bc_Tp1_dn']].describe()\n",
    "# training_test_data[['bc_Tp0_up','bc_Tp0_dn']].describe()\n",
    "\n",
    "# check rows results\n",
    "# dvislr_nr_m0010_z0000_p0010 rows =3554 - 7 = 3547\n",
    "# dvislr_nr_m0020_z0000_p0020 rows =2904 - 7 = 2897\n",
    "# dvislr_nr_m0030_z0000_p0030 rows =2316 - 7 = 2309\n",
    "# dvislr_nr_m0040_z0000_p0040 rows =1837 - 7 = 1830\n",
    "# dvislr_nr_m0050_z0000_p0050 rows =1437 - 7 = 1430\n",
    "# dvislr_nr_m0060_z0000_p0060 rows =1135 - 7 = 1128\n",
    "# dvislr_nr_m0070_z0000_p0070 rows = 893 - 7 =  886\n",
    "# dvislr_nr_m0080_z0000_p0080 rows = 697 - 7 =  690\n",
    "# dvislr_nr_m0090_z0000_p0090 rows = 559 - 7 =  552\n",
    "# dvislr_nr_m0100_z0000_p0100 rows = 439 - 7 =  432"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cols(Input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# predictors_tf = training_test_data[training_test_data.columns[2+2+(31*2):2+2+(31*2)+87]]\n",
    "# predictors_tf.describe()\n",
    "# predictors_tf.head()\n",
    "\n",
    "# Tp1=2, Tp0=2, strat=62, inputs=87 = X(t+1), X(t), v1-v85\n",
    "# tf_innput = training_test_data[ training_test_data.columns[2+2+(31*2) : 2+2+(31*2)+87] ]\n",
    "\n",
    "# XXX 4\n",
    "\n",
    "# Tp1=2, Tp0=2, strat=62, inputs=86 =         X(t), v1-v85\n",
    "tf_innput = training_test_data[ training_test_data.columns[2+2+(31*2)+1 : 2+2+(31*2)+1+86] ]\n",
    "\n",
    "# Example for 80 econ variables\n",
    "# Tp1=2, Tp0=2, strat=62, inputs=81 =         X(t), v1-v80\n",
    "#tf_innput = training_test_data[ training_test_data.columns[2+2+(31*2)+1 : 2+2+(31*2)+1+81] ]\n",
    "\n",
    "\n",
    "tf_innput.describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cols(Output): Tp1, Tp0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 2 output columns\n",
    "# classes_tf = training_test_data[training_test_data.columns[0:2]] # col 0, 1\n",
    "# classes_tf = training_test_data[training_test_data.columns[2:4]] # col 2, 3\n",
    "# classes_tf.describe()\n",
    "# count = 4273"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf_output_Tp1 = training_test_data[training_test_data.columns[0:2]] # col 0, 1\n",
    "tf_output_Tp1.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf_output_Tp0 = training_test_data[training_test_data.columns[2:4]] # col 2, 3\n",
    "tf_output_Tp0.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data: cols(Input, Output_Tp1, Output_Tp0): rows(Tst, Trn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Split: test data = 20%\n",
    "size_tst = int(len(training_test_data) * 0.2)\n",
    "print 'size_tst = size of testing set = ' + str(size_tst)\n",
    "# Split: train data = 80%\n",
    "size_trn = len(training_test_data) - size_tst\n",
    "print 'size_trn = size of training set = ' + str(size_trn)\n",
    "\n",
    "# noise remove stratification: [-0010,+0010]: training_set_size=2837, test_set_size=710\n",
    "# noise remove stratification: [-0020,+0020]: training_set_size=2317, test_set_size=580\n",
    "# noise remove stratification: [-0030,+0030]: training_set_size=1847, test_set_size=462\n",
    "# noise remove stratification: [-0040,+0040]: training_set_size=1464, test_set_size=366\n",
    "# noise remove stratification: [-0050,+0050]: training_set_size=1144, test_set_size=286\n",
    "# noise remove stratification: [-0060,+0060]: training_set_size=902,  test_set_size=226\n",
    "# noise remove stratification: [-0070,+0070]: training_set_size=708,  test_set_size=178\n",
    "# noise remove stratification: [-0080,+0080]: training_set_size=552,  test_set_size=138\n",
    "# noise remove stratification: [-0090,+0090]: training_set_size=441,  test_set_size=111\n",
    "# noise remove stratification: [-0100,+0100]: training_set_size=345,  test_set_size=87\n",
    "\n",
    "# size_tst = size of testing set = 854 # size_trn = size of training set = 3419\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split into training and test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Since data is reverse chronological (ie. lastest on top)\n",
    "# top (size_tst=855) rows\n",
    "tf_innput_tst     = tf_innput[:size_tst]\n",
    "tf_output_tst_Tp1 = tf_output_Tp1[:size_tst]\n",
    "tf_output_tst_Tp0 = tf_output_Tp0[:size_tst]\n",
    "\n",
    "tf_output_tst_Tp1.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Since data is reverse chronological (ie. lastest on top)\n",
    "# bottom (total-(size_tst=855)=3418) rows\n",
    "# ie. start from row 855 onwards\n",
    "tf_innput_trn     = tf_innput[size_tst:]      \n",
    "tf_output_trn_Tp1 = tf_output_Tp1[size_tst:]\n",
    "tf_output_trn_Tp0 = tf_output_Tp0[size_tst:]\n",
    "\n",
    "tf_output_trn_Tp1.describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define some metrics here to evaluate the models.\n",
    "\n",
    "* [Precision](https://en.wikipedia.org/wiki/Precision_and_recall#Precision) -  The ability of the classifier not to label as positive a sample that is negative.\n",
    "* [Recall](https://en.wikipedia.org/wiki/Precision_and_recall#Recall) - The ability of the classifier to find all the positive samples.\n",
    "* [F1 Score](https://en.wikipedia.org/wiki/F1_score) - A weighted average of the precision and recall, where an F1 score reaches its best value at 1 and worst score at 0.\n",
    "* Accuracy - The percentage correctly predicted in the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def tf_confusion_metrics(model, actual_classes, session, feed_dict):\n",
    "  predictions = tf.argmax(model, 1)\n",
    "  actuals = tf.argmax(actual_classes, 1)\n",
    "\n",
    "  ones_like_actuals = tf.ones_like(actuals)\n",
    "  zeros_like_actuals = tf.zeros_like(actuals)\n",
    "  ones_like_predictions = tf.ones_like(predictions)\n",
    "  zeros_like_predictions = tf.zeros_like(predictions)\n",
    "\n",
    "  tp_op = tf.reduce_sum(\n",
    "    tf.cast(\n",
    "      tf.logical_and(\n",
    "        tf.equal(actuals, ones_like_actuals), \n",
    "        tf.equal(predictions, ones_like_predictions)\n",
    "      ), \n",
    "      \"float\"\n",
    "    )\n",
    "  )\n",
    "\n",
    "  tn_op = tf.reduce_sum(\n",
    "    tf.cast(\n",
    "      tf.logical_and(\n",
    "        tf.equal(actuals, zeros_like_actuals), \n",
    "        tf.equal(predictions, zeros_like_predictions)\n",
    "      ), \n",
    "      \"float\"\n",
    "    )\n",
    "  )\n",
    "\n",
    "  fp_op = tf.reduce_sum(\n",
    "    tf.cast(\n",
    "      tf.logical_and(\n",
    "        tf.equal(actuals, zeros_like_actuals), \n",
    "        tf.equal(predictions, ones_like_predictions)\n",
    "      ), \n",
    "      \"float\"\n",
    "    )\n",
    "  )\n",
    "\n",
    "  fn_op = tf.reduce_sum(\n",
    "    tf.cast(\n",
    "      tf.logical_and(\n",
    "        tf.equal(actuals, ones_like_actuals), \n",
    "        tf.equal(predictions, zeros_like_predictions)\n",
    "      ), \n",
    "      \"float\"\n",
    "    )\n",
    "  )\n",
    "\n",
    "  tp, tn, fp, fn = \\\n",
    "    session.run(\n",
    "      [tp_op, tn_op, fp_op, fn_op], \n",
    "      feed_dict\n",
    "    )\n",
    "\n",
    "  tpr = float(tp)/(float(tp) + float(fn))\n",
    "  fpr = float(fp)/(float(tp) + float(fn))\n",
    "\n",
    "  accuracy = (float(tp) + float(tn))/(float(tp) + float(fp) + float(fn) + float(tn))\n",
    "\n",
    "  recall = tpr\n",
    "  precision = float(tp)/(float(tp) + float(fp))\n",
    "  \n",
    "  f1_score = (2 * (precision * recall)) / (precision + recall)\n",
    "  \n",
    "  print 'Precision = ', precision\n",
    "  print 'Recall = ', recall\n",
    "  print 'F1 Score = ', f1_score\n",
    "  print 'Accuracy = ', accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary classification with TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, get some tensors flowing. The model is binary classification expressed in TensorFlow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# tensorflow session\n",
    "sess_m01_BC = tf.Session()\n",
    "\n",
    "# Define variables for the number of predictors and number of classes to remove magic numbers from our code.\n",
    "num_innput     = len(tf_innput_trn.columns)     # 86\n",
    "num_output_Tp1 = len(tf_output_trn_Tp1.columns) # 02 = Tp1\n",
    "num_output_Tp0 = len(tf_output_trn_Tp0.columns) # 02 = Tp0\n",
    "\n",
    "print 'num_innput     = ' + str(num_innput)\n",
    "print 'num_output_Tp1 = ' + str(num_output_Tp1)\n",
    "print 'num_output_Tp0 = ' + str(num_output_Tp0)\n",
    "\n",
    "# Define placeholders for the data we feed into the process\n",
    "# data_innput     = feature_data\n",
    "# data_output_Tp1 = actual_classes\n",
    "# data_output_Tp0 = actual_classes\n",
    "data_innput     = tf.placeholder(\"float\", [None, num_innput])\n",
    "data_output_Tp1 = tf.placeholder(\"float\", [None, num_output_Tp1])\n",
    "data_output_Tp0 = tf.placeholder(\"float\", [None, num_output_Tp0])\n",
    "\n",
    "# Define a matrix of weights and initialize it with some small random values.\n",
    "weights_Tp1 = tf.Variable(tf.truncated_normal([num_innput, num_output_Tp1], stddev=0.0001))\n",
    "weights_Tp0 = tf.Variable(tf.truncated_normal([num_innput, num_output_Tp0], stddev=0.0001))\n",
    "\n",
    "biases_Tp1  = tf.Variable(tf.ones([num_output_Tp1]))\n",
    "biases_Tp0  = tf.Variable(tf.ones([num_output_Tp0]))\n",
    "\n",
    "# Define our model...\n",
    "# Here we take a softmax regression of the product of our data_innput and weights.\n",
    "model_Tp1 = tf.nn.softmax(tf.matmul(data_innput, weights_Tp1) + biases_Tp1)\n",
    "model_Tp0 = tf.nn.softmax(tf.matmul(data_innput, weights_Tp0) + biases_Tp0)\n",
    "# Define a cost function (we're using the cross entropy).\n",
    "cost_Tp1 = -tf.reduce_sum(data_output_Tp1 * tf.log(model_Tp1))\n",
    "cost_Tp0 = -tf.reduce_sum(data_output_Tp0 * tf.log(model_Tp0))\n",
    "# Define a training step...\n",
    "# Here we use gradient descent with a learning rate of 0.01 using the cost function we just defined.\n",
    "training_step_Tp1 = tf.train.AdamOptimizer(learning_rate=0.0001).minimize(cost_Tp1)\n",
    "training_step_Tp0 = tf.train.AdamOptimizer(learning_rate=0.0001).minimize(cost_Tp0)\n",
    "# Init\n",
    "init = tf.initialize_all_variables()\n",
    "# Run\n",
    "sess_m01_BC.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# BEFORE the model has been run, ie. not yet trained\n",
    "# display weights\n",
    "m01_BC_Tp1_w = sess_m01_BC.run(weights_Tp1)    \n",
    "m01_BC_Tp0_w = sess_m01_BC.run(weights_Tp0)    \n",
    "\n",
    "# print m01_BC_w_Tp1 \n",
    "# print m01_BC_w_Tp0 \n",
    "\n",
    "# Expect something like this (small numbers e-05): \n",
    "# [[ -6.66152628e-05  -6.49469657e-05]\n",
    "#  [  2.86067752e-05   3.27789494e-05]\n",
    "#  [  4.73170294e-05   1.85125769e-04]\n",
    "#  [ -6.42955492e-05   4.16024632e-05]\n",
    "#  [ -1.54488771e-05  -2.10903818e-05]\n",
    "#  [  4.62506796e-05  -2.98624745e-05]\n",
    "#  [ -5.35508079e-05  -1.30856875e-04]\n",
    "#  [ -1.70812025e-04   1.33106529e-04]\n",
    "#  [ -1.45097118e-04  -1.80459567e-04]\n",
    "#  [  1.02448161e-04   8.27739277e-05]\n",
    "#  [ -1.33277281e-04  -4.04360726e-05]\n",
    "#  [ -1.36186543e-04   8.62382149e-05]\n",
    "#  [  2.80324894e-05  -2.20580205e-05]\n",
    "#  [ -4.52000713e-05   2.54859442e-05]\n",
    "#  [ -6.20259525e-05  -6.95227573e-05]\n",
    "#  [  1.68935469e-04  -4.03221093e-05]\n",
    "#  [ -9.51008842e-05   4.10227585e-05]\n",
    "#  [ -5.90909331e-05  -9.20566745e-05]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll train our model in the following snippet. The approach of TensorFlow to executing graph operations allows fine-grained control over the process. Any operation you provide to the session as part of the run operation will be executed and the results returned. You can provide a list of multiple operations.\n",
    "\n",
    "You'll train the model over 30,000 iterations using the full dataset each time. Every thousandth iteration we'll assess the accuracy of the model on the training data to assess progress."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## m01_BC: On Data_Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "correct_prediction_Tp1 = tf.equal(tf.argmax(model_Tp1, 1), tf.argmax(data_output_Tp1, 1))\n",
    "correct_prediction_Tp0 = tf.equal(tf.argmax(model_Tp0, 1), tf.argmax(data_output_Tp0, 1))\n",
    "\n",
    "accuracy_Tp1 = tf.reduce_mean(tf.cast(correct_prediction_Tp1, \"float\"))\n",
    "accuracy_Tp0 = tf.reduce_mean(tf.cast(correct_prediction_Tp0, \"float\"))\n",
    "\n",
    "for i in range(1, 30001):\n",
    "  s = ''\n",
    "\n",
    "  # Tp1 ==================================================\n",
    "  sess_m01_BC.run(\n",
    "    training_step_Tp1, \n",
    "    feed_dict={\n",
    "      data_innput:     tf_innput_trn.values, \n",
    "      data_output_Tp1: tf_output_trn_Tp1.values.reshape(len(tf_output_trn_Tp1.values), 2)\n",
    "    }\n",
    "  )\n",
    "  if i%5000 == 0:\n",
    "    s = s + ' ' + 'Tp1'\n",
    "    s = s + '_' + str(i) \n",
    "    s = s + '_' + str(sess_m01_BC.run(\n",
    "      accuracy_Tp1,\n",
    "      feed_dict={\n",
    "        data_innput:     tf_innput_trn.values, \n",
    "        data_output_Tp1: tf_output_trn_Tp1.values.reshape(len(tf_output_trn_Tp1.values), 2)\n",
    "      }\n",
    "    ))\n",
    "    s = s + ';'\n",
    "\n",
    "  # Tp0 ==================================================\n",
    "  sess_m01_BC.run(\n",
    "    training_step_Tp0, \n",
    "    feed_dict={\n",
    "      data_innput:     tf_innput_trn.values, \n",
    "      data_output_Tp0: tf_output_trn_Tp0.values.reshape(len(tf_output_trn_Tp0.values), 2)\n",
    "    }\n",
    "  )\n",
    "  if i%5000 == 0:\n",
    "    s = s + ' ' + 'Tp0'\n",
    "    s = s + '_' + str(i) \n",
    "    s = s + '_' + str(sess_m01_BC.run(\n",
    "      accuracy_Tp0,\n",
    "      feed_dict={\n",
    "        data_innput:     tf_innput_trn.values, \n",
    "        data_output_Tp0: tf_output_trn_Tp0.values.reshape(len(tf_output_trn_Tp0.values), 2)\n",
    "      }\n",
    "    ))\n",
    "    s = s + ';'\n",
    "\n",
    "    print s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ## m01_BC: On Data_Training: Archive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Expect: \n",
    "# 5000 0.603862 # 10000 0.630778 # 15000 0.639555 # 20000 0.654184 # 25000 0.660035 # 30000 0.666764\n",
    "# After using < -0.5% and > +0.5%\n",
    "# 5000 0.928906 # 10000 0.928906 # 15000 0.928906 # 20000 0.928906 # 25000 0.928906 # 30000 0.928906\n",
    "# Noise Removal: [-0010, +0010]\n",
    "# 5000 0.617201 # 10000 0.64399 # 15000 0.665139 # 20000 0.683468 # 25000 0.688403 # 30000 0.691928\n",
    "# Noise Removal: [-0020, +0020]\n",
    "# 5000 0.625809 # 10000 0.6612 # 15000 0.680622 # 20000 0.695727 # 25000 0.708675 # 30000 0.712128\n",
    "# Noise Removal: [-0030, +0030]\n",
    "# 5000 0.635084 # 10000 0.677315 # 15000 0.701678 # 20000 0.714672 # 25000 0.729291 # 30000 0.738495\n",
    "# Noise Removal: [-0040, +0040]\n",
    "# 5000 0.645492 # 10000 0.689891 # 15000 0.717213 # 20000 0.734973 # 25000 0.748634 # 30000 0.758197\n",
    "# Noise Removal: [-0050, +0050]\n",
    "# 5000 0.653846 # 10000 0.702797 # 15000 0.730769 # 20000 0.740385 # 25000 0.75 # 30000 0.768357\n",
    "# Noise Removal: [-0060, +0060]\n",
    "# 5000 0.672949 # 10000 0.72173 # 15000 0.750554 # 20000 0.761641 # 25000 0.783814 # 30000 0.792683\n",
    "# Noise Removal: [-0070, +0070]\n",
    "# 5000 0.673729 # 10000 0.735876 # 15000 0.771186 # 20000 0.789548 # 25000 0.809322 # 30000 0.817797\n",
    "# Noise Removal: [-0080, +0080]\n",
    "# 5000 0.692029 # 10000 0.748188 # 15000 0.780797 # 20000 0.815217 # 25000 0.826087 # 30000 0.833333\n",
    "# Noise Removal: [-0090, +0090]\n",
    "# 5000 0.696145 # 10000 0.750567 # 15000 0.773243 # 20000 0.795918 # 25000 0.811791 # 30000 0.816327\n",
    "# Noise Removal: [-0100, +0100]\n",
    "# 5000 0.718841 # 10000 0.75942 # 15000 0.788406 # 20000 0.808696 # 25000 0.823188 # 30000 0.84058\n",
    "# inputs=87, output=2, no filters, Tp1\n",
    "# 5000 0.574605 # 10000 0.587771 # 15000 0.595377 # 20000 0.603277 # 25000 0.607958 # 30000 0.610591\n",
    "# ======================================================\n",
    "# 87 inputs = X(t+1), X(t), v1, .., v85\n",
    "# ======================================================\n",
    "# Tp1_05000_0.560983; Tp0_05000_0.593448;\n",
    "# Tp1_10000_0.584966; Tp0_10000_0.622112;\n",
    "# Tp1_15000_0.598421; Tp0_15000_0.627961;\n",
    "# Tp1_20000_0.603978; Tp0_20000_0.639076;\n",
    "# Tp1_25000_0.613922; Tp0_25000_0.650775;\n",
    "# Tp1_30000_0.621527; Tp0_30000_0.65487;    <= m01_BC_87: predicting (T+1) has 62% accu, predicting (T+0) has 65% accu\n",
    "# ======================================================\n",
    "# 86 inputs =         X(t), v1, .., v85\n",
    "# ======================================================\n",
    "# Tp1_05000_0.547236; Tp0_05000_0.592863;\n",
    "# Tp1_10000_0.561568; Tp0_10000_0.622112;\n",
    "# Tp1_15000_0.569465; Tp0_15000_0.626791;\n",
    "# Tp1_20000_0.577362; Tp0_20000_0.639661;\n",
    "# Tp1_25000_0.584966; Tp0_25000_0.65019;\n",
    "# Tp1_30000_0.583504; Tp0_30000_0.650483;   <= m01_BC_86: predicting (T+1) has 58% accu, predicting (T+0) has 65% accu\n",
    "# ======================================================\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using all data:\n",
    "Accuracy  66.6% on training data\n",
    "That is OK, better than random."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## m01_BC: On Data_Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print '===== Tp1: Performance on Test Data ====='\n",
    "feed_dict= {\n",
    "  data_innput:     tf_innput_tst.values,\n",
    "  data_output_Tp1: tf_output_tst_Tp1.values.reshape(len(tf_output_tst_Tp1.values), 2)\n",
    "}\n",
    "tf_confusion_metrics(model_Tp1, data_output_Tp1, sess_m01_BC, feed_dict)\n",
    "\n",
    "print '===== Tp0: Performance on Test Data ====='\n",
    "feed_dict= {\n",
    "  data_innput:     tf_innput_tst.values,\n",
    "  data_output_Tp0: tf_output_tst_Tp0.values.reshape(len(tf_output_tst_Tp0.values), 2)\n",
    "}\n",
    "tf_confusion_metrics(model_Tp0, data_output_Tp0, sess_m01_BC, feed_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## m01_BC: On Data_Testing: Archive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Precision =  0.541176470588 # Recall =  0.555555555556 # F1 Score =  0.548271752086 # Accuracy =  0.556725146199\n",
    "# After removing noise\n",
    "# Precision =  0.684210526316 # Recall =  0.534246575342 # F1 Score =  0.6 # Accuracy =  0.636363636364\n",
    "# Noise Removal: [-0010, +0010]\n",
    "# Precision =  0.577235772358 # Recall =  0.605113636364 # F1 Score =  0.590846047157 # Accuracy =  0.584507042254\n",
    "# Noise Removal: [-0020, +0020]\n",
    "# Precision =  0.588850174216 # Recall =  0.603571428571 # F1 Score =  0.596119929453 # Accuracy =  0.605172413793\n",
    "# Noise Removal: [-0030, +0030]\n",
    "# Precision =  0.616740088106 # Recall =  0.619469026549 # F1 Score =  0.618101545254 # Accuracy =  0.625541125541\n",
    "# Noise Removal: [-0040, +0040]\n",
    "# Precision =  0.622093023256 # Recall =  0.58152173913 # F1 Score =  0.601123595506 # Accuracy =  0.612021857923\n",
    "# Noise Removal: [-0050, +0050]\n",
    "# Precision =  0.684210526316 # Recall =  0.534246575342 # F1 Score =  0.6 # Accuracy =  0.636363636364\n",
    "# Noise Removal: [-0060, +0060]\n",
    "# Precision =  0.644736842105 # Recall =  0.453703703704 # F1 Score =  0.532608695652 # Accuracy =  0.619469026549\n",
    "# Noise Removal: [-0070, +0070]\n",
    "# Precision =  0.666666666667 # Recall =  0.47619047619 # F1 Score =  0.555555555556 # Accuracy =  0.640449438202\n",
    "# Noise Removal: [-0080, +0080]\n",
    "# Precision =  0.6 # Recall =  0.406779661017 # F1 Score =  0.484848484848 # Accuracy =  0.630434782609\n",
    "# Noise Removal: [-0090, +0090]\n",
    "# Precision =  0.478260869565 # Recall =  0.261904761905 # F1 Score =  0.338461538462 # Accuracy =  0.612612612613\n",
    "# Noise Removal: [-0100, +0100]\n",
    "# Precision =  0.588235294118 # Recall =  0.37037037037 # F1 Score =  0.454545454545 # Accuracy =  0.724137931034\n",
    "\n",
    "# check rows results                         numrows   AccuOnTrainData    AccuOnTestData\n",
    "# dvislr_nr_m0010_z0000_p0010 rows =3554 - 7 = 3547    # 30000 0.691928   # Accuracy =  0.584507042254\n",
    "# dvislr_nr_m0020_z0000_p0020 rows =2904 - 7 = 2897    # 30000 0.712128   # Accuracy =  0.605172413793\n",
    "# dvislr_nr_m0030_z0000_p0030 rows =2316 - 7 = 2309    # 30000 0.738495   # Accuracy =  0.625541125541\n",
    "# dvislr_nr_m0040_z0000_p0040 rows =1837 - 7 = 1830    # 30000 0.758197   # Accuracy =  0.612021857923\n",
    "# dvislr_nr_m0050_z0000_p0050 rows =1437 - 7 = 1430    # 30000 0.768357   # Accuracy =  0.636363636364\n",
    "# dvislr_nr_m0060_z0000_p0060 rows =1135 - 7 = 1128    # 30000 0.792683   # Accuracy =  0.619469026549\n",
    "# dvislr_nr_m0070_z0000_p0070 rows = 893 - 7 =  886    # 30000 0.817797   # Accuracy =  0.640449438202\n",
    "# dvislr_nr_m0080_z0000_p0080 rows = 697 - 7 =  690    # 30000 0.833333   # Accuracy =  0.630434782609\n",
    "# dvislr_nr_m0090_z0000_p0090 rows = 559 - 7 =  552    # 30000 0.816327   # Accuracy =  0.612612612613\n",
    "# dvislr_nr_m0100_z0000_p0100 rows = 439 - 7 =  432    # 30000 0.84058    # Accuracy =  0.724137931034\n",
    "\n",
    "# ======================================================\n",
    "# 87 inputs = X(t+1), X(t), v1, .., v85\n",
    "# ======================================================\n",
    "# ===== Tp1: Performance on Test Data =====\n",
    "# Precision =  0.507795100223\n",
    "# Recall =  0.560196560197\n",
    "# F1 Score =  0.532710280374\n",
    "# Accuracy =  0.531615925059\n",
    "# ===== Tp0: Performance on Test Data =====\n",
    "# Precision =  0.537209302326\n",
    "# Recall =  0.567567567568\n",
    "# F1 Score =  0.551971326165\n",
    "# Accuracy =  0.560889929742\n",
    "\n",
    "# ======================================================\n",
    "# 86 inputs =       X(t), v1, .., v85\n",
    "# ======================================================\n",
    "# ===== Tp1: Performance on Test Data =====\n",
    "# Precision =  0.486725663717\n",
    "# Recall =  0.540540540541\n",
    "# F1 Score =  0.512223515716\n",
    "# Accuracy =  0.509367681499\n",
    "# ===== Tp0: Performance on Test Data =====\n",
    "# Precision =  0.54060324826\n",
    "# Recall =  0.572481572482\n",
    "# F1 Score =  0.556085918854\n",
    "# Accuracy =  0.564402810304\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ======================================================\n",
    "# 87 inputs = X(t+1), X(t), v1, .., v85\n",
    "# ======================================================\n",
    "# m01_BC: on data_trn: Tp1.accu = 62% | Tp0.accu = 65%\n",
    "# m01_BC: on data_tst: Tp1.accu = 53% | Tp0.accu = 56%\n",
    "# m02_NN: on data_trn: Tp1.accu = __% | Tp0.accu = __%\n",
    "# m02_NN: on data_tst: Tp1.accu = __% | Tp0.accu = __%\n",
    "\n",
    "# ======================================================\n",
    "# 86 inputs =       X(t), v1, .., v85\n",
    "# ======================================================\n",
    "# m01_BC: on data_trn: Tp1.accu = 58% | Tp0.accu = 65%\n",
    "# m01_BC: on data_tst: Tp1.accu = 51% | Tp0.accu = 56%\n",
    "# m02_NN: on data_trn: Tp1.accu = __% | Tp0.accu = __%\n",
    "# m02_NN: on data_tst: Tp1.accu = __% | Tp0.accu = __%\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# AFTER the model has been run, ie. trained\n",
    "# display weights\n",
    "m01_BC_Tp1_w = sess_m01_BC.run(weights_Tp1)    \n",
    "m01_BC_Tp0_w = sess_m01_BC.run(weights_Tp0)    \n",
    "# print m01_BC_w_Tp1 \n",
    "# print m01_BC_w_Tp0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# response = client.put_file('/output.csv', str(pd.DataFrame(npa).to_csv()), overwrite=True)\n",
    "# response = client.put_file('/output.csv', str(               df.to_csv()), overwrite=True)\n",
    "response = client.put_file('/m01_BC_Tp1_w.csv', str(pd.DataFrame(m01_BC_Tp1_w).to_csv()), overwrite=True)\n",
    "response = client.put_file('/m01_BC_Tp0_w.csv', str(pd.DataFrame(m01_BC_Tp0_w).to_csv()), overwrite=True)\n",
    "print \"uploaded:\", response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# grab a sample input row from the test set\n",
    "sample_input_1_86 = tf_innput_tst[:10]\n",
    "# HTML(pd.DataFrame(sample_input_1_86).to_html())\n",
    "sample_input_1_86"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sample_input_1_86.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## m01_BC_Tp1: Make a prediction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# use the weights from our model to make a prediction\n",
    "m01_BC_Tp1_output_1_2 = np.dot(sample_input_1_86, m01_BC_Tp1_w) > 0\n",
    "print m01_BC_Tp1_output_1_2\n",
    "m01_BC_Tp1_usdmxn_up = m01_BC_Tp1_output_1_2[0,0]\n",
    "m01_BC_Tp1_usdmxn_dn = m01_BC_Tp1_output_1_2[0,1]\n",
    "\n",
    "if m01_BC_Tp1_usdmxn_up >= m01_BC_Tp1_usdmxn_dn:\n",
    "  m01_BC_Tp1_usdmxn_pred = 1 # up\n",
    "else:\n",
    "  m01_BC_Tp1_usdmxn_pred = -1 # down\n",
    "print 'm01_BC_Tp1_usdmxn_pred = '+ str(m01_BC_Tp1_usdmxn_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 2: Neural Network (NN): Feed-Forward with 2 Hidden Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll now build a proper feed-forward neural net with two hidden layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# tensorflow session (new)\n",
    "sess_m02_NN = tf.Session()\n",
    "\n",
    "# Define variables for the number of predictors and number of classes to remove magic numbers from our code.\n",
    "num_innput     = len(tf_innput_trn.columns)     # 86             # num_predictors = len(tf_innput_trn.columns)\n",
    "num_output_Tp1 = len(tf_output_trn_Tp1.columns) # 02 = Tp1       # num_classes = len(training_classes_tf.columns)\n",
    "num_output_Tp0 = len(tf_output_trn_Tp0.columns) # 02 = Tp0       # num_classes = len(training_classes_tf.columns)\n",
    "\n",
    "print 'num_innput     = ' + str(num_innput)\n",
    "print 'num_output_Tp1 = ' + str(num_output_Tp1)\n",
    "print 'num_output_Tp0 = ' + str(num_output_Tp0)\n",
    "\n",
    "# Define placeholders for the data we feed into the process\n",
    "# data_innput     = feature_data\n",
    "# data_output_Tp1 = actual_classes\n",
    "# data_output_Tp0 = actual_classes\n",
    "data_innput     = tf.placeholder(\"float\", [None, num_innput])\n",
    "data_output_Tp1 = tf.placeholder(\"float\", [None, num_output_Tp1])\n",
    "data_output_Tp0 = tf.placeholder(\"float\", [None, num_output_Tp0])\n",
    "\n",
    "# layer 1\n",
    "i_01_in = num_innput # 86\n",
    "i_01_out = 100\n",
    "NN_Tp1_w1 = tf.Variable(tf.truncated_normal([i_01_in, i_01_out], stddev=0.0001)) # w = weight\n",
    "NN_Tp1_b1 = tf.Variable(tf.ones([i_01_out]))                                     # b = bias\n",
    "NN_Tp0_w1 = tf.Variable(tf.truncated_normal([i_01_in, i_01_out], stddev=0.0001)) # w = weight\n",
    "NN_Tp0_b1 = tf.Variable(tf.ones([i_01_out]))                                     # b = bias\n",
    "\n",
    "# layer 2\n",
    "i_02_in = i_01_out\n",
    "i_02_out = 25\n",
    "NN_Tp1_w2 = tf.Variable(tf.truncated_normal([i_02_in, i_02_out], stddev=0.0001))\n",
    "NN_Tp1_b2 = tf.Variable(tf.ones([i_02_out]))\n",
    "NN_Tp0_w2 = tf.Variable(tf.truncated_normal([i_02_in, i_02_out], stddev=0.0001))\n",
    "NN_Tp0_b2 = tf.Variable(tf.ones([i_02_out]))\n",
    "\n",
    "# layer 3\n",
    "i_03_in = i_02_out\n",
    "if num_output_Tp1 == num_output_Tp0:\n",
    "  i_03_out = num_output_Tp1\n",
    "else:\n",
    "  i_03_out = 2\n",
    "NN_Tp1_w3 = tf.Variable(tf.truncated_normal([i_03_in, i_03_out], stddev=0.0001))\n",
    "NN_Tp1_b3 = tf.Variable(tf.ones([i_03_out]))\n",
    "NN_Tp0_w3 = tf.Variable(tf.truncated_normal([i_03_in, i_03_out], stddev=0.0001))\n",
    "NN_Tp0_b3 = tf.Variable(tf.ones([i_03_out]))\n",
    "\n",
    "# m02_NN_Tp1\n",
    "NN_Tp1_hidden_layer_0 = data_innput\n",
    "NN_Tp1_hidden_layer_1 = tf.nn.relu(tf.matmul(NN_Tp1_hidden_layer_0, NN_Tp1_w1) + NN_Tp1_b1)\n",
    "NN_Tp1_hidden_layer_2 = tf.nn.relu(tf.matmul(NN_Tp1_hidden_layer_1, NN_Tp1_w2) + NN_Tp1_b2)\n",
    "NN_Tp1_hidden_layer_3 = tf.matmul(           NN_Tp1_hidden_layer_2, NN_Tp1_w3) + NN_Tp1_b3\n",
    "\n",
    "# m02_NN_Tp0\n",
    "NN_Tp0_hidden_layer_0 = data_innput\n",
    "NN_Tp0_hidden_layer_1 = tf.nn.relu(tf.matmul(NN_Tp0_hidden_layer_0, NN_Tp0_w1) + NN_Tp0_b1)\n",
    "NN_Tp0_hidden_layer_2 = tf.nn.relu(tf.matmul(NN_Tp0_hidden_layer_1, NN_Tp0_w2) + NN_Tp0_b2)\n",
    "NN_Tp0_hidden_layer_3 = tf.matmul(           NN_Tp0_hidden_layer_2, NN_Tp0_w3) + NN_Tp0_b3\n",
    "\n",
    "# Define our model...\n",
    "# Here we take a softmax regression of the product of our data_innput and weights.\n",
    "NN_Tp1_model = tf.nn.softmax(NN_Tp1_hidden_layer_3)\n",
    "NN_Tp0_model = tf.nn.softmax(NN_Tp0_hidden_layer_3)\n",
    "\n",
    "# Define a cost function (we're using the cross entropy).\n",
    "NN_Tp1_cost = -tf.reduce_sum(data_output_Tp1 * tf.log(NN_Tp1_model))\n",
    "NN_Tp0_cost = -tf.reduce_sum(data_output_Tp0 * tf.log(NN_Tp0_model))\n",
    "\n",
    "# Define a training operation, or step...\n",
    "# Here we use gradient descent with a learning rate of 0.01 using the cost function we just defined.\n",
    "NN_Tp1_train_op = tf.train.AdamOptimizer(learning_rate=0.0001).minimize(NN_Tp1_cost)\n",
    "NN_Tp0_train_op = tf.train.AdamOptimizer(learning_rate=0.0001).minimize(NN_Tp0_cost)\n",
    "\n",
    "# Init\n",
    "init = tf.initialize_all_variables()\n",
    "# Run\n",
    "sess_m02_NN.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Tp1: BEFORE the model has been run, ie. trained\n",
    "m02_NN_Tp1_b1 = sess_m02_NN.run(NN_Tp1_b1)    \n",
    "m02_NN_Tp1_w1 = sess_m02_NN.run(NN_Tp1_w1)    \n",
    "m02_NN_Tp1_b2 = sess_m02_NN.run(NN_Tp1_b2)    \n",
    "m02_NN_Tp1_w2 = sess_m02_NN.run(NN_Tp1_w2)    \n",
    "m02_NN_Tp1_b3 = sess_m02_NN.run(NN_Tp1_b3) \n",
    "m02_NN_Tp1_w3 = sess_m02_NN.run(NN_Tp1_w3)    \n",
    "# Tp0: BEFORE the model has been run, ie. trained\n",
    "m02_NN_Tp0_b1 = sess_m02_NN.run(NN_Tp0_b1)    \n",
    "m02_NN_Tp0_w1 = sess_m02_NN.run(NN_Tp0_w1)    \n",
    "m02_NN_Tp0_b2 = sess_m02_NN.run(NN_Tp0_b2)    \n",
    "m02_NN_Tp0_w2 = sess_m02_NN.run(NN_Tp0_w2)    \n",
    "m02_NN_Tp0_b3 = sess_m02_NN.run(NN_Tp0_b3) \n",
    "m02_NN_Tp0_w3 = sess_m02_NN.run(NN_Tp0_w3)    \n",
    "\n",
    "# Tp1\n",
    "# HTML(pd.DataFrame(m02_NN_Tp1_b1).transpose().to_html())\n",
    "# HTML(pd.DataFrame(m02_NN_Tp1_w1).to_html())\n",
    "# HTML(pd.DataFrame(m02_NN_Tp1_b2).transpose().to_html())\n",
    "# HTML(pd.DataFrame(m02_NN_Tp1_w2).to_html())\n",
    "# HTML(pd.DataFrame(m02_NN_Tp1_b3).transpose().to_html())\n",
    "# HTML(pd.DataFrame(m02_NN_Tp1_w3).to_html())\n",
    "# Tp0\n",
    "# HTML(pd.DataFrame(m02_NN_Tp0_b1).transpose().to_html())\n",
    "# HTML(pd.DataFrame(m02_NN_Tp0_w1).to_html())\n",
    "# HTML(pd.DataFrame(m02_NN_Tp0_b2).transpose().to_html())\n",
    "# HTML(pd.DataFrame(m02_NN_Tp0_w2).to_html())\n",
    "# HTML(pd.DataFrame(m02_NN_Tp0_b3).transpose().to_html())\n",
    "# HTML(pd.DataFrame(m02_NN_Tp0_w3).to_html())\n",
    "\n",
    "# Tp1\n",
    "# pd.DataFrame(m02_NN_Tp1_b1).transpose()\n",
    "# pd.DataFrame(m02_NN_Tp1_w1)\n",
    "# pd.DataFrame(m02_NN_Tp1_b2).transpose()\n",
    "# pd.DataFrame(m02_NN_Tp1_w2)\n",
    "# pd.DataFrame(m02_NN_Tp1_b3).transpose()\n",
    "# pd.DataFrame(m02_NN_Tp1_w3)\n",
    "# Tp0\n",
    "# pd.DataFrame(m02_NN_Tp0_b1).transpose()\n",
    "# pd.DataFrame(m02_NN_Tp0_w1)\n",
    "# pd.DataFrame(m02_NN_Tp0_b2).transpose()\n",
    "# pd.DataFrame(m02_NN_Tp0_w2)\n",
    "# pd.DataFrame(m02_NN_Tp0_b3).transpose()\n",
    "# pd.DataFrame(m02_NN_Tp0_w3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, you'll train the model over 10,000 iterations using the full dataset each time. Every thousandth iteration, you'll assess the accuracy of the model on the training data to assess progress."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## m02_NN: On Data_Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "NN_Tp1_correct_prediction = tf.equal(tf.argmax(NN_Tp1_model, 1), tf.argmax(data_output_Tp1, 1))\n",
    "NN_Tp0_correct_prediction = tf.equal(tf.argmax(NN_Tp0_model, 1), tf.argmax(data_output_Tp0, 1))\n",
    "\n",
    "NN_Tp1_accuracy = tf.reduce_mean(tf.cast(NN_Tp1_correct_prediction, \"float\"))\n",
    "NN_Tp0_accuracy = tf.reduce_mean(tf.cast(NN_Tp0_correct_prediction, \"float\"))\n",
    "\n",
    "num_iterations = 5001\n",
    "num_iterations_until_display = 1000\n",
    "\n",
    "for i in range(1, num_iterations):\n",
    "  s = ''\n",
    "  \n",
    "  # Tp1 ==================================================\n",
    "  sess_m02_NN.run(\n",
    "    NN_Tp1_train_op, \n",
    "    feed_dict={\n",
    "      data_innput:     tf_innput_trn.values, \n",
    "      data_output_Tp1: tf_output_trn_Tp1.values.reshape(len(tf_output_trn_Tp1.values), 2)\n",
    "    }\n",
    "  )\n",
    "  if i%num_iterations_until_display == 0:\n",
    "    s = s + ' ' + 'm02_NN'\n",
    "    s = s + '_' + 'Tp1'\n",
    "    s = s + '_' + str(i) \n",
    "    s = s + '_' + str(sess_m02_NN.run(\n",
    "      NN_Tp1_accuracy,\n",
    "      feed_dict={\n",
    "        data_innput:     tf_innput_trn.values, \n",
    "        data_output_Tp1: tf_output_trn_Tp1.values.reshape(len(tf_output_trn_Tp1.values), 2)\n",
    "      }\n",
    "    ))\n",
    "    s = s + ';'\n",
    "\n",
    "  # Tp0 ==================================================\n",
    "  sess_m02_NN.run(\n",
    "    NN_Tp0_train_op, \n",
    "    feed_dict={\n",
    "      data_innput:     tf_innput_trn.values, \n",
    "      data_output_Tp0: tf_output_trn_Tp0.values.reshape(len(tf_output_trn_Tp0.values), 2)\n",
    "    }\n",
    "  )\n",
    "  if i%num_iterations_until_display == 0:\n",
    "    s = s + ' ' + 'm02_NN'\n",
    "    s = s + '_' + 'Tp0'\n",
    "    s = s + '_' + str(i) \n",
    "    s = s + '_' + str(sess_m02_NN.run(\n",
    "      NN_Tp0_accuracy,\n",
    "      feed_dict={\n",
    "        data_innput:     tf_innput_trn.values, \n",
    "        data_output_Tp0: tf_output_trn_Tp0.values.reshape(len(tf_output_trn_Tp0.values), 2)\n",
    "      }\n",
    "    ))\n",
    "    s = s + ';'\n",
    "    \n",
    "    print s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## m02_NN: On Data_Training: Archive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Original\n",
    "# 1000 0.676126 # 2000 0.768578 # 3000 0.83031 # 4000 0.891164 # 5000 0.93739# 6000 0.956115 # 7000 0.972206 # 8000 0.978935 # 9000 0.985079 # 10000 0.987127\n",
    "# Original\n",
    "# 1000 0.685489 # 2000 0.777063 # 3000 0.84055 # 4000 0.902867 # 5000 0.941486 # 6000 0.957578 # 7000 0.974254 # 8000 0.980105 # 9000 0.985079 # 10000 0.987712\n",
    "# 87 to 2\n",
    "# 1000 0.595963 # 2000 0.731714 # 3000 0.870392 # 4000 0.928613 # 5000 0.9567\n",
    "# ======================================================\n",
    "# 87 inputs = X(t+1), X(t), v1, .., v85\n",
    "# ======================================================\n",
    "#  m02_NN_Tp1_1000_0.583796; m02_NN_Tp0_1000_0.631764;\n",
    "#  m02_NN_Tp1_2000_0.717754; m02_NN_Tp0_2000_0.740567;\n",
    "#  m02_NN_Tp1_3000_0.905235; m02_NN_Tp0_3000_0.869845;\n",
    "#  m02_NN_Tp1_4000_0.954958; m02_NN_Tp0_4000_0.937409;\n",
    "#  m02_NN_Tp1_5000_0.969582; m02_NN_Tp0_5000_0.964317;\n",
    "# ======================================================\n",
    "\n",
    "# ======================================================\n",
    "# 86 inputs =       X(t), v1, .., v85\n",
    "# ======================================================\n",
    "#  m02_NN_Tp1_1000_0.558935; m02_NN_Tp0_1000_0.631471;\n",
    "#  m02_NN_Tp1_2000_0.587306; m02_NN_Tp0_2000_0.737643;\n",
    "#  m02_NN_Tp1_3000_0.589061; m02_NN_Tp0_3000_0.868675;\n",
    "#  m02_NN_Tp1_4000_0.588184; m02_NN_Tp0_4000_0.938286;\n",
    "#  m02_NN_Tp1_5000_0.590231; m02_NN_Tp0_5000_0.964902;\n",
    "# ======================================================\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A significant improvement in accuracy with the training data shows that the hidden layers are adding additional capacity for learning to the model.\n",
    "\n",
    "Looking at precision, recall, and accuracy, you can see a measurable improvement in performance, but certainly not a [step function](https://wikipedia.org/wiki/Step_function). This indicates that we're likely reaching the limits of this relatively simple feature set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## m02_NN: On Data_Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print '===== m02_NN_Tp1: Performance on Test Data ====='\n",
    "feed_dict= {\n",
    "  data_innput:     tf_innput_tst.values,\n",
    "  data_output_Tp1: tf_output_tst_Tp1.values.reshape(len(tf_output_tst_Tp1.values), 2)\n",
    "}\n",
    "tf_confusion_metrics(NN_Tp1_model, data_output_Tp1, sess_m02_NN, feed_dict)\n",
    "\n",
    "print '===== m02_NN_Tp0: Performance on Test Data ====='\n",
    "feed_dict= {\n",
    "  data_innput:     tf_innput_tst.values,\n",
    "  data_output_Tp0: tf_output_tst_Tp0.values.reshape(len(tf_output_tst_Tp0.values), 2)\n",
    "}\n",
    "tf_confusion_metrics(NN_Tp0_model, data_output_Tp0, sess_m02_NN, feed_dict)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## m02_NN: On Data_Testing: Archive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Original\n",
    "# Precision =  0.934731934732\n",
    "# Recall =  0.968599033816\n",
    "# F1 Score =  0.951364175563\n",
    "# Accuracy =  0.952046783626\n",
    "\n",
    "# 87:2 Tp1\n",
    "# Precision =  0.857446808511\n",
    "# Recall =  0.973429951691\n",
    "# F1 Score =  0.911764705882\n",
    "# Accuracy =  0.908771929825\n",
    "\n",
    "# ======================================================\n",
    "# 87 inputs = X(t+1), X(t), v1, .., v85\n",
    "# ======================================================\n",
    "# ===== m02_NN_Tp1: Performance on Test Data =====\n",
    "# Precision =  0.893719806763\n",
    "# Recall =  0.909090909091\n",
    "# F1 Score =  0.901339829476\n",
    "# Accuracy =  0.905152224824\n",
    "# ===== m02_NN_Tp0: Performance on Test Data =====\n",
    "# Precision =  0.844660194175\n",
    "# Recall =  0.855036855037\n",
    "# F1 Score =  0.849816849817\n",
    "# Accuracy =  0.855971896956\n",
    "\n",
    "# ======================================================\n",
    "# 86 inputs =       X(t), v1, .., v85\n",
    "# ======================================================\n",
    "# ===== m02_NN_Tp1: Performance on Test Data =====\n",
    "# Precision =  0.493303571429\n",
    "# Recall =  0.542997542998\n",
    "# F1 Score =  0.516959064327\n",
    "# Accuracy =  0.516393442623\n",
    "# ===== m02_NN_Tp0: Performance on Test Data =====\n",
    "# Precision =  0.844282238443\n",
    "# Recall =  0.85257985258\n",
    "# F1 Score =  0.848410757946\n",
    "# Accuracy =  0.854800936768"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ======================================================\n",
    "# 87 inputs = X(t+1), X(t), v1, .., v85\n",
    "# ======================================================\n",
    "# m01_BC: on data_trn: Tp1.accu = 62% | Tp0.accu = 65%\n",
    "# m01_BC: on data_tst: Tp1.accu = 53% | Tp0.accu = 56%\n",
    "# m02_NN: on data_trn: Tp1.accu = 97% | Tp0.accu = 96%\n",
    "# m02_NN: on data_tst: Tp1.accu = 90% | Tp0.accu = 85%\n",
    "\n",
    "# ======================================================\n",
    "# 86 inputs =       X(t), v1, .., v85\n",
    "# ======================================================\n",
    "# m01_BC: on data_trn: Tp1.accu = 58% | Tp0.accu = 65%\n",
    "# m01_BC: on data_tst: Tp1.accu = 51% | Tp0.accu = 56%\n",
    "# m02_NN: on data_trn: Tp1.accu = 59% | Tp0.accu = 96%\n",
    "# m02_NN: on data_tst: Tp1.accu = 52% | Tp0.accu = 85%\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Tp1: AFTER the model has been run, ie. trained\n",
    "m02_NN_Tp1_b1 = sess_m02_NN.run(NN_Tp1_b1)    \n",
    "m02_NN_Tp1_w1 = sess_m02_NN.run(NN_Tp1_w1)    \n",
    "m02_NN_Tp1_b2 = sess_m02_NN.run(NN_Tp1_b2)    \n",
    "m02_NN_Tp1_w2 = sess_m02_NN.run(NN_Tp1_w2)    \n",
    "m02_NN_Tp1_b3 = sess_m02_NN.run(NN_Tp1_b3) \n",
    "m02_NN_Tp1_w3 = sess_m02_NN.run(NN_Tp1_w3)    \n",
    "# Tp0: AFTER the model has been run, ie. trained\n",
    "m02_NN_Tp0_b1 = sess_m02_NN.run(NN_Tp0_b1)    \n",
    "m02_NN_Tp0_w1 = sess_m02_NN.run(NN_Tp0_w1)    \n",
    "m02_NN_Tp0_b2 = sess_m02_NN.run(NN_Tp0_b2)    \n",
    "m02_NN_Tp0_w2 = sess_m02_NN.run(NN_Tp0_w2)    \n",
    "m02_NN_Tp0_b3 = sess_m02_NN.run(NN_Tp0_b3) \n",
    "m02_NN_Tp0_w3 = sess_m02_NN.run(NN_Tp0_w3)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# response = client.put_file('/output.csv', str(pd.DataFrame(npa).to_csv()), overwrite=True)\n",
    "# response = client.put_file('/output.csv', str(               df.to_csv()), overwrite=True)\n",
    "\n",
    "# Tp1\n",
    "response = client.put_file('/m02_NN_Tp1_b1.csv', str(pd.DataFrame(m02_NN_Tp1_b1).to_csv()), overwrite=True); print \"uploaded:\", response\n",
    "response = client.put_file('/m02_NN_Tp1_w1.csv', str(pd.DataFrame(m02_NN_Tp1_w1).to_csv()), overwrite=True); print \"uploaded:\", response\n",
    "response = client.put_file('/m02_NN_Tp1_b2.csv', str(pd.DataFrame(m02_NN_Tp1_b2).to_csv()), overwrite=True); print \"uploaded:\", response\n",
    "response = client.put_file('/m02_NN_Tp1_w2.csv', str(pd.DataFrame(m02_NN_Tp1_w2).to_csv()), overwrite=True); print \"uploaded:\", response\n",
    "response = client.put_file('/m02_NN_Tp1_b3.csv', str(pd.DataFrame(m02_NN_Tp1_b3).to_csv()), overwrite=True); print \"uploaded:\", response\n",
    "response = client.put_file('/m02_NN_Tp1_w3.csv', str(pd.DataFrame(m02_NN_Tp1_w3).to_csv()), overwrite=True); print \"uploaded:\", response\n",
    "# Tp0\n",
    "response = client.put_file('/m02_NN_Tp0_b1.csv', str(pd.DataFrame(m02_NN_Tp0_b1).to_csv()), overwrite=True); print \"uploaded:\", response\n",
    "response = client.put_file('/m02_NN_Tp0_w1.csv', str(pd.DataFrame(m02_NN_Tp0_w1).to_csv()), overwrite=True); print \"uploaded:\", response\n",
    "response = client.put_file('/m02_NN_Tp0_b2.csv', str(pd.DataFrame(m02_NN_Tp0_b2).to_csv()), overwrite=True); print \"uploaded:\", response\n",
    "response = client.put_file('/m02_NN_Tp0_w2.csv', str(pd.DataFrame(m02_NN_Tp0_w2).to_csv()), overwrite=True); print \"uploaded:\", response\n",
    "response = client.put_file('/m02_NN_Tp0_b3.csv', str(pd.DataFrame(m02_NN_Tp0_b3).to_csv()), overwrite=True); print \"uploaded:\", response\n",
    "response = client.put_file('/m02_NN_Tp0_w3.csv', str(pd.DataFrame(m02_NN_Tp0_w3).to_csv()), overwrite=True); print \"uploaded:\", response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You've covered a lot of ground. You moved from sourcing five years of financial time-series data, to munging that data into a more suitable form. You explored and visualized that data with exploratory data analysis and then decided on a machine learning model and the features for that model. You engineered those features, built a binary classifier in TensorFlow, and analyzed its performance. You built a feed forward neural net with two hidden layers in TensorFlow and analyzed its performance.\n",
    "\n",
    "How did the technology fare? It should take most people 1.5 to 3 hours to extract the juice from this solution, and none of that time is spent waiting for infrastructure or software; it's spent reading and thinking. In many organizations, it can take anywhere from days to months to do this sort of data analysis, depending on whether you need to procure any hardware. And you didn't need to do anything with infrastructure or additional software. Rather, you used a web-based console to direct GCP to set up systems on your behalf, which it did—fully managed, maintained, and supported—freeing you up to spend your time analyzing. \n",
    "\n",
    "It was also cost effective. If you took your time with this solution and spent three hours to go through it, the cost would be a few pennies. \n",
    "\n",
    "Cloud Datalab worked admirably, too. iPython/Jupyter has always been a great platform for interactive, iterative work and a fully-managed version of that platform on GCP, with connectors to other GCP technologies such as BigQuery and Google Cloud Storage, is a force multiplier for your analysis needs.  If you haven't used iPython before, this solution might have been eye opening, for you. If you're already familiar with iPython, then you'll love the connectors to other GCP technologies.\n",
    "\n",
    "Of course, R and Matlab are popular tools in machine learning, and we've made no mention either in this solution. Neither R nor Matlab are available as managed services on GCP. Both can be hosted in GCP and accessed through a cloud-friendly, web frontend.\n",
    "\n",
    "TensorFlow is a special piece of technology. It is expressive, performs well, and comes with the weight of Google's machine learning history and expertise to back it up and support it. We've only scratched the surface, but you can already see that within a handful of lines of code we've been able to write two models. Neither of them is cutting edge, by design, but neither of them is trivial either. With some additional tuning they would suit a whole spectrum of machine learning tasks. \n",
    "\n",
    "Finally, how did we do with the data analysis? We did well: over 70% accuracy in predicting the close of the S&P 500 is the highest we've seen achieved on this dataset, so with few steps and a few lines of code we've produced a full-on machine learning model. The reason for the relatively modest accuracy achieved is the dataset itself; there isn't enough signal there to do significantly better. But 7 times out of 10, we were able to correctly determine if the S&P 500 index would close up or down on the day, and that's objectively good."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**When you're finished, shut down the managed VM you used for Cloud Datalab to avoid incurring costs.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
