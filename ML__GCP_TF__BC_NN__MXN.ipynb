{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning (ML): Google Cloud Platform (GCP): TensorFlow (TF): Financial Time-Series\n",
    "# Model 01 = Binary Classifier\n",
    "# Model 02 = Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import StringIO\n",
    "\n",
    "import pandas as pd\n",
    "from pandas.tools.plotting import autocorrelation_plot\n",
    "from pandas.tools.plotting import scatter_matrix\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import gcp\n",
    "import gcp.bigquery as bq\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Install stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied (use --upgrade to upgrade): xlrd in /usr/local/lib/python2.7/dist-packages\r\n",
      "Cleaning up...\r\n"
     ]
    }
   ],
   "source": [
    "# Install xlrd\n",
    "!pip install xlrd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Get the data from dropbox excel file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save file on dropbox.com\n",
    "# Right click to get the link\n",
    "force = 1\n",
    "\n",
    "# url_data_raw = 'https://www.dropbox.com/s/t8egr3lqxlpzcj8/data_raw.xlsm' # (dropbox for wylie.chan@gmail.com)\n",
    "# url_data_raw = 'https://www.dropbox.com/s/9hlf1lrmqfy093v/data_raw.xlsm' #  (dropbox for eyup.saltik.2016@gmail.com)  \n",
    "url_data_raw = 'https://www.dropbox.com/s/hwkuunzjvj9vph5/data_raw_03.xlsm' # updated 2016/07/04"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_raw_03.xlsm\n",
      "converted 'https://www.dropbox.com/s/hwkuunzjvj9vph5/data_raw_03.xlsm' (ANSI_X3.4-1968) -> 'https://www.dropbox.com/s/hwkuunzjvj9vph5/data_raw_03.xlsm' (UTF-8)\n",
      "--2016-07-28 13:12:08--  https://www.dropbox.com/s/hwkuunzjvj9vph5/data_raw_03.xlsm\n",
      "Resolving www.dropbox.com (www.dropbox.com)... 108.160.172.238\n",
      "Connecting to www.dropbox.com (www.dropbox.com)|108.160.172.238|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://dl.dropboxusercontent.com/content_link/lv7q5Bf6h0hMJ79iS3Z1PPq2dnqhYlo0CFJTh3cRgIRf3NOeq841XRitKIi5dXJO/file [following]\n",
      "converted 'https://dl.dropboxusercontent.com/content_link/lv7q5Bf6h0hMJ79iS3Z1PPq2dnqhYlo0CFJTh3cRgIRf3NOeq841XRitKIi5dXJO/file' (ANSI_X3.4-1968) -> 'https://dl.dropboxusercontent.com/content_link/lv7q5Bf6h0hMJ79iS3Z1PPq2dnqhYlo0CFJTh3cRgIRf3NOeq841XRitKIi5dXJO/file' (UTF-8)\n",
      "--2016-07-28 13:12:09--  https://dl.dropboxusercontent.com/content_link/lv7q5Bf6h0hMJ79iS3Z1PPq2dnqhYlo0CFJTh3cRgIRf3NOeq841XRitKIi5dXJO/file\n",
      "Resolving dl.dropboxusercontent.com (dl.dropboxusercontent.com)... 45.58.75.165\n",
      "Connecting to dl.dropboxusercontent.com (dl.dropboxusercontent.com)|45.58.75.165|:443... connected.\n",
      "HTTP request sent, awaiting response... "
     ]
    }
   ],
   "source": [
    "# Import file using url to file on dropbox\n",
    "from urlparse import urlparse\n",
    "from os.path import basename\n",
    "\n",
    "bn_data_raw = basename(urlparse(url_data_raw).path)\n",
    "print bn_data_raw\n",
    "\n",
    "try:\n",
    "    already_downloaded\n",
    "except:\n",
    "    already_downloaded = False\n",
    "    \n",
    "if force or not already_downloaded:\n",
    "    already_downloaded = True\n",
    "    !rm $bn_data_raw\n",
    "    !wget $url_data_raw\n",
    "#!ls -l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Read the Excel file raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import xlrd\n",
    "\n",
    "# Read the excel data\n",
    "data_raw = pd.read_excel(bn_data_raw, 'data_raw')\n",
    "# data = pd.read_excel(bn_data_raw, 'data_raw', header=0, index_col=0, parse_cols=None)\n",
    "  # parse_cols=None: parse all columns\n",
    "  # header=0:    sets the row 0 as col labels (ie. headers)\n",
    "  # index_col=0: sets the col 0 as row labels (ie. index)\n",
    "data_raw.head() # display the first few lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Skip: Select only the columns that we have chosen from the PCA50 exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This list pertains to first set of data.\n",
    "# list_cols_pca = [ 0,1,\n",
    "#                   1070,788,926,112,69,574,654,1160,527,323,\n",
    "#                   397,118,774,1028,1034,655,907,736,251,388,\n",
    "#                   327,243,705,303,1146,467,136,1006,600,15,\n",
    "#                   231,290,131,782,20,1048,630,1173,431,856,\n",
    "#                   67,299,838,639,53,932,870,938,1061\n",
    "#                 ]\n",
    "# data_raw_pca = data_raw[list_cols_pca]\n",
    "\n",
    "# For our new data set, we take all 86 columns\n",
    "data_raw_pca = data_raw\n",
    "data_raw_pca.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6) data_ticker_desc = Just a table to map column id to the ticker and name of the variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_ticker_desc = data_raw_pca.ix[:1] # 1st 2 rows\n",
    "data_ticker_desc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7) data_values = the actual values of the 86 variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_values = data_raw_pca.ix[2:] # From row 2 onwards\n",
    "data_values.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8) Set the dates as index of the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_values_indexed = data_values.set_index([0])\n",
    "data_values_indexed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_values_indexed.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9) Fill any gaps in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Pandas includes a very convenient function for filling gaps in the data.\n",
    "data_values_indexed = data_values_indexed.fillna(method='ffill')\n",
    "data_values_indexed.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory data analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploratory Data Analysis (EDA) is foundational to working with machine learning, and any other sort of analysis. EDA means getting to know your data, getting your fingers dirty with your data, feeling it and seeing it. The end result is you know your data very well, so when you build models you build them based on an actual, practical, physical understanding of the data, not assumptions or vaguely held notions. You can still make assumptions of course, but EDA means you will understand your assumptions and why you're making those assumptions. \n",
    "\n",
    "First, take a look at the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10) Describe the data briefly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_values_indexed.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that the various indices operate on scales differing by orders of magnitude. It's best to scale the data so that, for example, operations involving multiple indices aren't unduly influenced by a single, massive index.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11) Plot the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# N.B. A super-useful trick-ette is to assign the return value of plot to _ \n",
    "# so that you don't get text printed before the plot itself.\n",
    "\n",
    "# _ = pd.concat([data_values_indexed[1],\n",
    "#   data_values_indexed[1070],\n",
    "#   data_values_indexed[788],\n",
    "#   data_values_indexed[926]], axis=1).plot(figsize=(20, 15))\n",
    "\n",
    "_ = data_values_indexed.plot(figsize=(20, 15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the structure isn't uniformly visible for the indices. Divide each value in an individual index by the maximum value for that index., and then replot. The maximum value of all indices will be 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12) Calculate the max value for each column, prepare to scale data for each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_values_indexed_max = data_values_indexed.max(axis=0) # max across axis 0 = rows\n",
    "data_values_indexed_max.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13) Scale the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_values_indexed_scaled = data_values_indexed / data_values_indexed_max\n",
    "data_values_indexed_scaled.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14) Plot the scaled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "_ = data_values_indexed_scaled.plot(figsize=(20, 15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15) Auto-correlations\n",
    "\n",
    "Next, plot autocorrelations for each of the indices. The autocorrelations determine correlations between current values of the index and lagged values of the same index. The goal is to determine whether the lagged values are reliable indicators of the current values. If they are, then we've identified a correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "fig.set_figwidth(20)\n",
    "fig.set_figheight(10)\n",
    "\n",
    "_ = autocorrelation_plot(data_values_indexed_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For 2300 lagged days, we observe positive auto-correlations.\n",
    "This suggests that as the variables increase, they tend to keep on increasing. Momentum.\n",
    "\n",
    "After 2300 lagged days, we observe negative auto-correlations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16) Skip: Just a reminder of the PCA columns we selected earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# list_cols_pca = [ 0,1,\n",
    "#                   1070,788,926,112,69,574,654,1160,527,323,\n",
    "#                   397,118,774,1028,1034,655,907,736,251,388,\n",
    "#                   327,243,705,303,1146,467,136,1006,600,15,\n",
    "#                   231,290,131,782,20,1048,630,1173,431,856,\n",
    "#                   67,299,838,639,53,932,870,938,1061\n",
    "#                 ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 17) Scatter plots of the first 20 of our 86 variables vs USDMXN(varid=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# _ = scatter_matrix(data_values_indexed_scaled)\n",
    "dvis = data_values_indexed_scaled\n",
    "\n",
    "_ = scatter_matrix(\n",
    "      pd.concat(\n",
    "      [ \n",
    "        dvis[ 1],dvis[ 2],dvis[ 3],dvis[ 4],dvis[ 5],dvis[ 6],dvis[ 7],dvis[ 8],dvis[ 9],dvis[10],\n",
    "        dvis[11],dvis[12],dvis[13],dvis[14],dvis[15],dvis[16],dvis[17],dvis[18],dvis[19],dvis[20],\n",
    "      ], axis=1), figsize=(15, 15), diagonal='kde')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 18) Remind ourselves what our scaled data (of price or index LEVELS) looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_values_indexed_scaled.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 19) Calculate Log Returns on our scaled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_values_indexed_scaled_logret = pd.DataFrame()\n",
    "data_values_indexed_scaled_logret = np.log(data_values_indexed_scaled/data_values_indexed_scaled.shift(-1)) # note dates are reverse-chrono\n",
    "\n",
    "data_values_indexed_scaled_logret.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_values_indexed_scaled_logret.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_values_indexed_scaled_logret.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 2000/01/07 to 2016/05/27 = 1 header row + 4279 data rows\n",
    "# print data_values_indexed_scaled_logret.head()\n",
    "# print data_values_indexed_scaled_logret.tail()\n",
    "print 'NumRowsIncludeHeader = len(data_values_indexed_scaled_logret) = ' + str(len(data_values_indexed_scaled_logret))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 20) Replace inf, NaN in data with Zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_values_indexed_scaled_logret = data_values_indexed_scaled_logret.replace([np.inf, -np.inf, np.nan], 0)\n",
    "data_values_indexed_scaled_logret.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 21) Skip: Fill the Gaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Pandas includes a very convenient function for filling gaps in the data.\n",
    "# data_values_indexed_scaled_logret = data_values_indexed_scaled_logret.fillna(method='ffill')\n",
    "data_values_indexed_scaled_logret.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 22) Plot log returns of scaled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "_ = data_values_indexed_scaled_logret.plot(figsize=(20, 15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 23) Auto-Correlations of log returns of scaled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "fig.set_figwidth(20)\n",
    "fig.set_figheight(10)\n",
    "\n",
    "_ = autocorrelation_plot(data_values_indexed_scaled_logret)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no auto-correlations, so we are good."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 24) Skip: Just a reminder of the PCA50 Columns we selected earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# list_cols_pca = [ 0,1,\n",
    "#                   1070,788,926,112,69,574,654,1160,527,323,\n",
    "#                   397,118,774,1028,1034,655,907,736,251,388,\n",
    "#                   327,243,705,303,1146,467,136,1006,600,15,\n",
    "#                   231,290,131,782,20,1048,630,1173,431,856,\n",
    "#                   67,299,838,639,53,932,870,938,1061\n",
    "#                 ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 25) Scatter plots of log returns of first 20 of the 86 variables where log-returns of USDMXN(varid=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# _ = scatter_matrix(data_values_indexed_scaled_logret)   # takes long time, becareful, save work first\n",
    "dvislr = data_values_indexed_scaled_logret\n",
    "\n",
    "_ = scatter_matrix(\n",
    "      pd.concat(\n",
    "      [ \n",
    "        dvislr[ 1],dvislr[ 2],dvislr[ 3],dvislr[ 4],dvislr[ 5],dvislr[ 6],dvislr[ 7],dvislr[ 8],dvislr[ 9],dvislr[10],\n",
    "        dvislr[11],dvislr[12],dvislr[13],dvislr[14],dvislr[15],dvislr[16],dvislr[17],dvislr[18],dvislr[19],dvislr[20],\n",
    "      ], axis=1), figsize=(15, 15), diagonal='kde')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_values_indexed_scaled_logret.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summing up the EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, you've done a good enough job of exploratory data analysis. You've visualized our data and come to know it better. \n",
    "You've transformed it into a form that is useful for modelling, log returns, and looked at how indices relate to each other. \n",
    "\n",
    "What should we think so far?\n",
    "\n",
    "Cloud Datalab is working great. With just a few lines of code, you were able to munge the data, visualize the changes, and make decisions. You could easily analyze and iterate. This is a common feature of iPython, but the advantage here is that Cloud Datalab is a managed service that you can simply click and use, so you can focus on your analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we can see a model:\n",
    "\n",
    "* We'll predict whether the USDMXN close today will be higher or lower than yesterday.\n",
    "\n",
    "Predicting whether the log return of the USDMXN is positive or negative is a classification problem. \n",
    "That is, we want to choose one option from a finite set of options, in this case positive or negative. \n",
    "This is the base case of classification where we have only two values to choose from, known as binary classification, or logistic regression.\n",
    "\n",
    "Machine learning models are very good at finding weak signals from data.\n",
    "In machine learning, as in most things, there are subtle tradeoffs happening, but in general good data is better than good algorithms, which are better than good frameworks. \n",
    "You need all three pillars but in that order of importance: data, algorithms, frameworks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[TensorFlow](https://tensorflow.org) is an open source software library, initiated by Google, for numerical computation using data flow graphs. TensorFlow is based on Google's machine learning expertise and is the next generation framework used internally at Google for tasks such as translation and image recognition. It's a wonderful framework for machine learning because it's expressive, efficient, and easy to use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature engineering for TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From a training and testing perspective, time series data is easy. Training data should come from events that happened before test data events, and be contiguous in time.  Otherwise,  your model would be trained on events from \"the future\", at least as compared to the test data. It would then likely perform badly in practice, because you can’t really have access to data from the future. That means random sampling or cross validation don't apply to time series data. Decide on a training-versus-testing split, and divide your data into training and test datasets.\n",
    "\n",
    "In this case, you'll create the features together with two additional columns:\n",
    "\n",
    "* usdmxn_logret_positive, which is 1 if the log return of the USDMXN close is positive, and 0 otherwise. \n",
    "* usdmxn_logret_negative, which is 1 if the log return of the USDMXN close is negative, and 1 otherwise. \n",
    "\n",
    "We'll use 80% of our data for training and 20% for testing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 1: Binary Classification (BC) = Logistic Regression (LR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1: Step 01: Indicator Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 00: Original: Initialize indicator columns to 0\n",
    "data_values_indexed_scaled_logret['usdmxn_logret_positive'] = 0\n",
    "data_values_indexed_scaled_logret['usdmxn_logret_negative'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 00: Populate results columns according to actual usdmxn returns (positive or negative)\n",
    "data_values_indexed_scaled_logret.ix[data_values_indexed_scaled_logret[1] >= 0, 'usdmxn_logret_positive'] = 1\n",
    "data_values_indexed_scaled_logret.ix[data_values_indexed_scaled_logret[1] <  0, 'usdmxn_logret_negative'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 01: Noise Cancellation\n",
    "data_values_indexed_scaled_logret['usdmxn_logret_positive_0050bp'] = 0\n",
    "data_values_indexed_scaled_logret['usdmxn_logret_negative_0050bp'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 01: Populate results columns according to actual usdmxn returns (outside noise zone above or below)\n",
    "data_values_indexed_scaled_logret.ix[data_values_indexed_scaled_logret[1] >= 0.0050, 'usdmxn_logret_positive_0050bp'] = 1\n",
    "data_values_indexed_scaled_logret.ix[data_values_indexed_scaled_logret[1] <  -0.0050, 'usdmxn_logret_negative_0050bp'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # 02: Stratification\n",
    "# data_values_indexed_scaled_logret['usdmxn_logret_+0150bp_and_above'] = 0; data_values_indexed_scaled_logret['usdmxn_logret_+0150bp_and_below'] = 0;\n",
    "# data_values_indexed_scaled_logret['usdmxn_logret_+0140bp_and_above'] = 0; data_values_indexed_scaled_logret['usdmxn_logret_+0140bp_and_below'] = 0;\n",
    "# data_values_indexed_scaled_logret['usdmxn_logret_+0130bp_and_above'] = 0; data_values_indexed_scaled_logret['usdmxn_logret_+0130bp_and_below'] = 0;\n",
    "# data_values_indexed_scaled_logret['usdmxn_logret_+0120bp_and_above'] = 0; data_values_indexed_scaled_logret['usdmxn_logret_+0120bp_and_below'] = 0;\n",
    "# data_values_indexed_scaled_logret['usdmxn_logret_+0110bp_and_above'] = 0; data_values_indexed_scaled_logret['usdmxn_logret_+0110bp_and_below'] = 0;\n",
    "# data_values_indexed_scaled_logret['usdmxn_logret_+0100bp_and_above'] = 0; data_values_indexed_scaled_logret['usdmxn_logret_+0100bp_and_below'] = 0;\n",
    "# data_values_indexed_scaled_logret['usdmxn_logret_+0090bp_and_above'] = 0; data_values_indexed_scaled_logret['usdmxn_logret_+0090bp_and_below'] = 0;\n",
    "# data_values_indexed_scaled_logret['usdmxn_logret_+0080bp_and_above'] = 0; data_values_indexed_scaled_logret['usdmxn_logret_+0080bp_and_below'] = 0;\n",
    "# data_values_indexed_scaled_logret['usdmxn_logret_+0070bp_and_above'] = 0; data_values_indexed_scaled_logret['usdmxn_logret_+0070bp_and_below'] = 0;\n",
    "# data_values_indexed_scaled_logret['usdmxn_logret_+0060bp_and_above'] = 0; data_values_indexed_scaled_logret['usdmxn_logret_+0060bp_and_below'] = 0;\n",
    "# data_values_indexed_scaled_logret['usdmxn_logret_+0050bp_and_above'] = 0; data_values_indexed_scaled_logret['usdmxn_logret_+0050bp_and_below'] = 0;\n",
    "# data_values_indexed_scaled_logret['usdmxn_logret_+0040bp_and_above'] = 0; data_values_indexed_scaled_logret['usdmxn_logret_+0040bp_and_below'] = 0;\n",
    "# data_values_indexed_scaled_logret['usdmxn_logret_+0030bp_and_above'] = 0; data_values_indexed_scaled_logret['usdmxn_logret_+0030bp_and_below'] = 0;\n",
    "# data_values_indexed_scaled_logret['usdmxn_logret_+0020bp_and_above'] = 0; data_values_indexed_scaled_logret['usdmxn_logret_+0020bp_and_below'] = 0;\n",
    "# data_values_indexed_scaled_logret['usdmxn_logret_+0010bp_and_above'] = 0; data_values_indexed_scaled_logret['usdmxn_logret_+0010bp_and_below'] = 0;\n",
    "# data_values_indexed_scaled_logret['usdmxn_logret_+0000bp_and_above'] = 0; data_values_indexed_scaled_logret['usdmxn_logret_+0000bp_and_below'] = 0;\n",
    "# data_values_indexed_scaled_logret['usdmxn_logret_-0010bp_and_above'] = 0; data_values_indexed_scaled_logret['usdmxn_logret_-0010bp_and_below'] = 0;\n",
    "# data_values_indexed_scaled_logret['usdmxn_logret_-0020bp_and_above'] = 0; data_values_indexed_scaled_logret['usdmxn_logret_-0020bp_and_below'] = 0;\n",
    "# data_values_indexed_scaled_logret['usdmxn_logret_-0030bp_and_above'] = 0; data_values_indexed_scaled_logret['usdmxn_logret_-0030bp_and_below'] = 0;\n",
    "# data_values_indexed_scaled_logret['usdmxn_logret_-0040bp_and_above'] = 0; data_values_indexed_scaled_logret['usdmxn_logret_-0040bp_and_below'] = 0;\n",
    "# data_values_indexed_scaled_logret['usdmxn_logret_-0050bp_and_above'] = 0; data_values_indexed_scaled_logret['usdmxn_logret_-0050bp_and_below'] = 0;\n",
    "# data_values_indexed_scaled_logret['usdmxn_logret_-0060bp_and_above'] = 0; data_values_indexed_scaled_logret['usdmxn_logret_-0060bp_and_below'] = 0;\n",
    "# data_values_indexed_scaled_logret['usdmxn_logret_-0070bp_and_above'] = 0; data_values_indexed_scaled_logret['usdmxn_logret_-0070bp_and_below'] = 0;\n",
    "# data_values_indexed_scaled_logret['usdmxn_logret_-0080bp_and_above'] = 0; data_values_indexed_scaled_logret['usdmxn_logret_-0080bp_and_below'] = 0;\n",
    "# data_values_indexed_scaled_logret['usdmxn_logret_-0090bp_and_above'] = 0; data_values_indexed_scaled_logret['usdmxn_logret_-0090bp_and_below'] = 0;\n",
    "# data_values_indexed_scaled_logret['usdmxn_logret_-0100bp_and_above'] = 0; data_values_indexed_scaled_logret['usdmxn_logret_-0100bp_and_below'] = 0;\n",
    "# data_values_indexed_scaled_logret['usdmxn_logret_-0110bp_and_above'] = 0; data_values_indexed_scaled_logret['usdmxn_logret_-0110bp_and_below'] = 0;\n",
    "# data_values_indexed_scaled_logret['usdmxn_logret_-0120bp_and_above'] = 0; data_values_indexed_scaled_logret['usdmxn_logret_-0120bp_and_below'] = 0;\n",
    "# data_values_indexed_scaled_logret['usdmxn_logret_-0130bp_and_above'] = 0; data_values_indexed_scaled_logret['usdmxn_logret_-0130bp_and_below'] = 0;\n",
    "# data_values_indexed_scaled_logret['usdmxn_logret_-0140bp_and_above'] = 0; data_values_indexed_scaled_logret['usdmxn_logret_-0140bp_and_below'] = 0;\n",
    "# data_values_indexed_scaled_logret['usdmxn_logret_-0150bp_and_above'] = 0; data_values_indexed_scaled_logret['usdmxn_logret_-0150bp_and_below'] = 0;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 02: Stratification: Initialize\n",
    "for d_bp in xrange(-150,160,10): # -150:10:+150\n",
    "  d = d_bp / 10000.00\n",
    "  s_above = 'usdmxn_logret_' + '{0:+05d}'.format(d_bp) + 'bp_and_above'\n",
    "  s_below = 'usdmxn_logret_' + '{0:+05d}'.format(d_bp) + 'bp_and_below'\n",
    "  # print d\n",
    "  # print s_above\n",
    "  # print s_below\n",
    "  data_values_indexed_scaled_logret[s_above] = 0;\n",
    "  data_values_indexed_scaled_logret[s_below] = 0;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 02: Stratification: Populate results columns according to actual usdmxn returns (above or below each level)\n",
    "for d_bp in xrange(-150,160,10): # -150:10:+150\n",
    "  d = d_bp / 10000.00\n",
    "  s_above = 'usdmxn_logret_' + '{0:+05d}'.format(d_bp) + 'bp_and_above'\n",
    "  s_below = 'usdmxn_logret_' + '{0:+05d}'.format(d_bp) + 'bp_and_below'\n",
    "  # print d\n",
    "  # print s_above\n",
    "  # print s_below\n",
    "  data_values_indexed_scaled_logret.ix[data_values_indexed_scaled_logret[1] >= d, s_above] = 1\n",
    "  data_values_indexed_scaled_logret.ix[data_values_indexed_scaled_logret[1] <  d, s_below] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_values_indexed_scaled_logret.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_values_indexed_scaled_logret.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1: Step 01.01: Remove data noise in a stratified way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dvislr = data_values_indexed_scaled_logret\n",
    "# dvislrnr = data_values_indexed_scaled_logret (noise removed)\n",
    "\n",
    "# Example Filter: df[(df.A == 1) & (df.D == 6)]\n",
    "dvislr_nr_m0010_z0000_p0010 = dvislr[ (dvislr['usdmxn_logret_-0010bp_and_below'] == 1) | (dvislr['usdmxn_logret_+0010bp_and_above'] == 1) ] # remove data between [-0.10%, +0.10%], center at +0.00%\n",
    "dvislr_nr_m0020_z0000_p0020 = dvislr[ (dvislr['usdmxn_logret_-0020bp_and_below'] == 1) | (dvislr['usdmxn_logret_+0020bp_and_above'] == 1) ] # remove data between [-0.10%, +0.10%], center at +0.00%\n",
    "dvislr_nr_m0030_z0000_p0030 = dvislr[ (dvislr['usdmxn_logret_-0030bp_and_below'] == 1) | (dvislr['usdmxn_logret_+0030bp_and_above'] == 1) ] # remove data between [-0.10%, +0.10%], center at +0.00%\n",
    "dvislr_nr_m0040_z0000_p0040 = dvislr[ (dvislr['usdmxn_logret_-0040bp_and_below'] == 1) | (dvislr['usdmxn_logret_+0040bp_and_above'] == 1) ] # remove data between [-0.10%, +0.10%], center at +0.00%\n",
    "dvislr_nr_m0050_z0000_p0050 = dvislr[ (dvislr['usdmxn_logret_-0050bp_and_below'] == 1) | (dvislr['usdmxn_logret_+0050bp_and_above'] == 1) ] # remove data between [-0.50%, +0.50%], center at +0.00%\n",
    "dvislr_nr_m0060_z0000_p0060 = dvislr[ (dvislr['usdmxn_logret_-0060bp_and_below'] == 1) | (dvislr['usdmxn_logret_+0060bp_and_above'] == 1) ] # remove data between [-0.50%, +0.50%], center at +0.00%\n",
    "dvislr_nr_m0070_z0000_p0070 = dvislr[ (dvislr['usdmxn_logret_-0070bp_and_below'] == 1) | (dvislr['usdmxn_logret_+0070bp_and_above'] == 1) ] # remove data between [-0.50%, +0.50%], center at +0.00%\n",
    "dvislr_nr_m0080_z0000_p0080 = dvislr[ (dvislr['usdmxn_logret_-0080bp_and_below'] == 1) | (dvislr['usdmxn_logret_+0080bp_and_above'] == 1) ] # remove data between [-0.50%, +0.50%], center at +0.00%\n",
    "dvislr_nr_m0090_z0000_p0090 = dvislr[ (dvislr['usdmxn_logret_-0090bp_and_below'] == 1) | (dvislr['usdmxn_logret_+0090bp_and_above'] == 1) ] # remove data between [-0.50%, +0.50%], center at +0.00%\n",
    "dvislr_nr_m0100_z0000_p0100 = dvislr[ (dvislr['usdmxn_logret_-0100bp_and_below'] == 1) | (dvislr['usdmxn_logret_+0100bp_and_above'] == 1) ] # remove data between [-0.50%, +0.50%], center at +0.00%\n",
    "\n",
    "# check rows\n",
    "print 'dvislr_nr_m0010_z0000_p0010 rows =' + str(len(dvislr_nr_m0010_z0000_p0010))\n",
    "print 'dvislr_nr_m0020_z0000_p0020 rows =' + str(len(dvislr_nr_m0020_z0000_p0020))\n",
    "print 'dvislr_nr_m0030_z0000_p0030 rows =' + str(len(dvislr_nr_m0030_z0000_p0030))\n",
    "print 'dvislr_nr_m0040_z0000_p0040 rows =' + str(len(dvislr_nr_m0040_z0000_p0040))\n",
    "print 'dvislr_nr_m0050_z0000_p0050 rows =' + str(len(dvislr_nr_m0050_z0000_p0050))\n",
    "print 'dvislr_nr_m0060_z0000_p0060 rows =' + str(len(dvislr_nr_m0060_z0000_p0060))\n",
    "print 'dvislr_nr_m0070_z0000_p0070 rows =' + str(len(dvislr_nr_m0070_z0000_p0070))\n",
    "print 'dvislr_nr_m0080_z0000_p0080 rows =' + str(len(dvislr_nr_m0080_z0000_p0080))\n",
    "print 'dvislr_nr_m0090_z0000_p0090 rows =' + str(len(dvislr_nr_m0090_z0000_p0090))\n",
    "print 'dvislr_nr_m0100_z0000_p0100 rows =' + str(len(dvislr_nr_m0100_z0000_p0100))\n",
    "\n",
    "# check rows results\n",
    "# dvislr_nr_m0010_z0000_p0010 rows =3554\n",
    "# dvislr_nr_m0020_z0000_p0020 rows =2904\n",
    "# dvislr_nr_m0030_z0000_p0030 rows =2316\n",
    "# dvislr_nr_m0040_z0000_p0040 rows =1837\n",
    "# dvislr_nr_m0050_z0000_p0050 rows =1437\n",
    "# dvislr_nr_m0060_z0000_p0060 rows =1135\n",
    "# dvislr_nr_m0070_z0000_p0070 rows =893\n",
    "# dvislr_nr_m0080_z0000_p0080 rows =697\n",
    "# dvislr_nr_m0090_z0000_p0090 rows =559\n",
    "# dvislr_nr_m0100_z0000_p0100 rows =439"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1: Step 02: Split data into training, test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### training test data empty shell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# training_test_data\n",
    "# col 01-02 = 02 cols = indicators of results\n",
    "# col 03-88 = 86 cols = inputs\n",
    "training_test_data = pd.DataFrame(\n",
    "  columns=[\n",
    "    # 'usdmxn_logret_positive', 'usdmxn_logret_negative',\n",
    "    # 'usdmxn_logret_positive_0050bp', 'usdmxn_logret_negative_0050bp',\n",
    "\n",
    "    'bc_up', 'bc_dn',\n",
    "    \n",
    "    'usdmxn_logret_-0150bp_and_above', 'usdmxn_logret_-0150bp_and_below',\n",
    "    'usdmxn_logret_-0140bp_and_above', 'usdmxn_logret_-0140bp_and_below',\n",
    "    'usdmxn_logret_-0130bp_and_above', 'usdmxn_logret_-0130bp_and_below',\n",
    "    'usdmxn_logret_-0120bp_and_above', 'usdmxn_logret_-0120bp_and_below',\n",
    "    'usdmxn_logret_-0110bp_and_above', 'usdmxn_logret_-0110bp_and_below',\n",
    "    'usdmxn_logret_-0100bp_and_above', 'usdmxn_logret_-0100bp_and_below',\n",
    "    'usdmxn_logret_-0090bp_and_above', 'usdmxn_logret_-0090bp_and_below',\n",
    "    'usdmxn_logret_-0080bp_and_above', 'usdmxn_logret_-0080bp_and_below',\n",
    "    'usdmxn_logret_-0070bp_and_above', 'usdmxn_logret_-0070bp_and_below',\n",
    "    'usdmxn_logret_-0060bp_and_above', 'usdmxn_logret_-0060bp_and_below',\n",
    "    'usdmxn_logret_-0050bp_and_above', 'usdmxn_logret_-0050bp_and_below',\n",
    "    'usdmxn_logret_-0040bp_and_above', 'usdmxn_logret_-0040bp_and_below',\n",
    "    'usdmxn_logret_-0030bp_and_above', 'usdmxn_logret_-0030bp_and_below',\n",
    "    'usdmxn_logret_-0020bp_and_above', 'usdmxn_logret_-0020bp_and_below',\n",
    "    'usdmxn_logret_-0010bp_and_above', 'usdmxn_logret_-0010bp_and_below',\n",
    "    'usdmxn_logret_+0000bp_and_above', 'usdmxn_logret_+0000bp_and_below',\n",
    "    'usdmxn_logret_+0010bp_and_above', 'usdmxn_logret_+0010bp_and_below',\n",
    "    'usdmxn_logret_+0020bp_and_above', 'usdmxn_logret_+0020bp_and_below',\n",
    "    'usdmxn_logret_+0030bp_and_above', 'usdmxn_logret_+0030bp_and_below',\n",
    "    'usdmxn_logret_+0040bp_and_above', 'usdmxn_logret_+0040bp_and_below',\n",
    "    'usdmxn_logret_+0050bp_and_above', 'usdmxn_logret_+0050bp_and_below',\n",
    "    'usdmxn_logret_+0060bp_and_above', 'usdmxn_logret_+0060bp_and_below',\n",
    "    'usdmxn_logret_+0070bp_and_above', 'usdmxn_logret_+0070bp_and_below',\n",
    "    'usdmxn_logret_+0080bp_and_above', 'usdmxn_logret_+0080bp_and_below',\n",
    "    'usdmxn_logret_+0090bp_and_above', 'usdmxn_logret_+0090bp_and_below',\n",
    "    'usdmxn_logret_+0100bp_and_above', 'usdmxn_logret_+0100bp_and_below',\n",
    "    'usdmxn_logret_+0110bp_and_above', 'usdmxn_logret_+0110bp_and_below',\n",
    "    'usdmxn_logret_+0120bp_and_above', 'usdmxn_logret_+0120bp_and_below',\n",
    "    'usdmxn_logret_+0130bp_and_above', 'usdmxn_logret_+0130bp_and_below',\n",
    "    'usdmxn_logret_+0140bp_and_above', 'usdmxn_logret_+0140bp_and_below',\n",
    "    'usdmxn_logret_+0150bp_and_above', 'usdmxn_logret_+0150bp_and_below',\n",
    "\n",
    "    '1','2','3','4','5','6','7','8','9','10',\n",
    "    '11','12','13','14','15','16','17','18','19','20',\n",
    "    '21','22','23','24','25','26','27','28','29','30',\n",
    "    '31','32','33','34','35','36','37','38','39','40',\n",
    "    '41','42','43','44','45','46','47','48','49','50',\n",
    "    '51','52','53','54','55','56','57','58','59','60',\n",
    "    '61','62','63','64','65','66','67','68','69','70',\n",
    "    '71','72','73','74','75','76','77','78','79','80',\n",
    "    '81','82','83','84','85','86',\n",
    "  ])\n",
    "\n",
    "training_test_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data into variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# row 0      = header\n",
    "# row 1-4279 = data\n",
    "# NumRowsIncludeHeader = len(data_values_indexed_scaled_logret) = 4280 = 0-4279\n",
    "# Start from row 7, so we can have up to (7-1) lookback days\n",
    "\n",
    "# check rows results\n",
    "# dvislr_nr_m0010_z0000_p0010 rows =3554\n",
    "# dvislr_nr_m0020_z0000_p0020 rows =2904\n",
    "# dvislr_nr_m0030_z0000_p0030 rows =2316\n",
    "# dvislr_nr_m0040_z0000_p0040 rows =1837\n",
    "# dvislr_nr_m0050_z0000_p0050 rows =1437\n",
    "# dvislr_nr_m0060_z0000_p0060 rows =1135\n",
    "# dvislr_nr_m0070_z0000_p0070 rows =893\n",
    "# dvislr_nr_m0080_z0000_p0080 rows =697\n",
    "# dvislr_nr_m0090_z0000_p0090 rows =559\n",
    "# dvislr_nr_m0100_z0000_p0100 rows =439\n",
    "\n",
    "#==========================================================================================\n",
    "# Select the data\n",
    "#==========================================================================================\n",
    "# dvislr =  dvislr_nr_m0010_z0000_p0010\n",
    "# dvislr =  dvislr_nr_m0020_z0000_p0020\n",
    "# dvislr =  dvislr_nr_m0030_z0000_p0030\n",
    "# dvislr =  dvislr_nr_m0040_z0000_p0040\n",
    "# dvislr =  dvislr_nr_m0050_z0000_p0050\n",
    "# dvislr =  dvislr_nr_m0060_z0000_p0060\n",
    "dvislr =  dvislr_nr_m0070_z0000_p0070\n",
    "# dvislr =  dvislr_nr_m0080_z0000_p0080\n",
    "# dvislr =  dvislr_nr_m0090_z0000_p0090\n",
    "# dvislr =  dvislr_nr_m0100_z0000_p0100\n",
    "\n",
    "for i in range(7, len(dvislr)): # [7, 4280) = [7,4279] \n",
    "\n",
    "  # 00: original\n",
    "  #   usdmxn_logret_positive = data_values_indexed_scaled_logret['usdmxn_logret_positive'].ix[i]\n",
    "  #   usdmxn_logret_negative = data_values_indexed_scaled_logret['usdmxn_logret_negative'].ix[i]\n",
    "\n",
    "  # 01: noise cancellation\n",
    "  #   usdmxn_logret_positive_0050bp = data_values_indexed_scaled_logret['usdmxn_logret_positive_0050bp'].ix[i]\n",
    "  #   usdmxn_logret_negative_0050bp = data_values_indexed_scaled_logret['usdmxn_logret_negative_0050bp'].ix[i]\n",
    "\n",
    "  # 02: stratification  \n",
    "  usdmxn_logret_p0150bp_and_below = dvislr['usdmxn_logret_+0150bp_and_below'].ix[i]; usdmxn_logret_p0150bp_and_above = dvislr['usdmxn_logret_+0150bp_and_above'].ix[i];\n",
    "  usdmxn_logret_p0140bp_and_below = dvislr['usdmxn_logret_+0140bp_and_below'].ix[i]; usdmxn_logret_p0140bp_and_above = dvislr['usdmxn_logret_+0140bp_and_above'].ix[i];\n",
    "  usdmxn_logret_p0130bp_and_below = dvislr['usdmxn_logret_+0130bp_and_below'].ix[i]; usdmxn_logret_p0130bp_and_above = dvislr['usdmxn_logret_+0130bp_and_above'].ix[i];\n",
    "  usdmxn_logret_p0120bp_and_below = dvislr['usdmxn_logret_+0120bp_and_below'].ix[i]; usdmxn_logret_p0120bp_and_above = dvislr['usdmxn_logret_+0120bp_and_above'].ix[i];\n",
    "  usdmxn_logret_p0110bp_and_below = dvislr['usdmxn_logret_+0110bp_and_below'].ix[i]; usdmxn_logret_p0110bp_and_above = dvislr['usdmxn_logret_+0110bp_and_above'].ix[i];\n",
    "  \n",
    "  usdmxn_logret_p0100bp_and_below = dvislr['usdmxn_logret_+0100bp_and_below'].ix[i]; usdmxn_logret_p0100bp_and_above = dvislr['usdmxn_logret_+0100bp_and_above'].ix[i];\n",
    "  usdmxn_logret_p0090bp_and_below = dvislr['usdmxn_logret_+0090bp_and_below'].ix[i]; usdmxn_logret_p0090bp_and_above = dvislr['usdmxn_logret_+0090bp_and_above'].ix[i];\n",
    "  usdmxn_logret_p0080bp_and_below = dvislr['usdmxn_logret_+0080bp_and_below'].ix[i]; usdmxn_logret_p0080bp_and_above = dvislr['usdmxn_logret_+0080bp_and_above'].ix[i];\n",
    "  usdmxn_logret_p0070bp_and_below = dvislr['usdmxn_logret_+0070bp_and_below'].ix[i]; usdmxn_logret_p0070bp_and_above = dvislr['usdmxn_logret_+0070bp_and_above'].ix[i];\n",
    "  usdmxn_logret_p0060bp_and_below = dvislr['usdmxn_logret_+0060bp_and_below'].ix[i]; usdmxn_logret_p0060bp_and_above = dvislr['usdmxn_logret_+0060bp_and_above'].ix[i];\n",
    "\n",
    "  usdmxn_logret_p0050bp_and_below = dvislr['usdmxn_logret_+0050bp_and_below'].ix[i]; usdmxn_logret_p0050bp_and_above = dvislr['usdmxn_logret_+0050bp_and_above'].ix[i];\n",
    "  usdmxn_logret_p0040bp_and_below = dvislr['usdmxn_logret_+0040bp_and_below'].ix[i]; usdmxn_logret_p0040bp_and_above = dvislr['usdmxn_logret_+0040bp_and_above'].ix[i];\n",
    "  usdmxn_logret_p0030bp_and_below = dvislr['usdmxn_logret_+0030bp_and_below'].ix[i]; usdmxn_logret_p0030bp_and_above = dvislr['usdmxn_logret_+0030bp_and_above'].ix[i];\n",
    "  usdmxn_logret_p0020bp_and_below = dvislr['usdmxn_logret_+0020bp_and_below'].ix[i]; usdmxn_logret_p0020bp_and_above = dvislr['usdmxn_logret_+0020bp_and_above'].ix[i];\n",
    "  usdmxn_logret_p0010bp_and_below = dvislr['usdmxn_logret_+0010bp_and_below'].ix[i]; usdmxn_logret_p0010bp_and_above = dvislr['usdmxn_logret_+0010bp_and_above'].ix[i];\n",
    "\n",
    "  usdmxn_logret_p0000bp_and_below = dvislr['usdmxn_logret_+0000bp_and_below'].ix[i]; usdmxn_logret_p0000bp_and_above = dvislr['usdmxn_logret_+0000bp_and_above'].ix[i];\n",
    "\n",
    "  usdmxn_logret_m0010bp_and_below = dvislr['usdmxn_logret_-0010bp_and_below'].ix[i]; usdmxn_logret_m0010bp_and_above = dvislr['usdmxn_logret_-0010bp_and_above'].ix[i];\n",
    "  usdmxn_logret_m0020bp_and_below = dvislr['usdmxn_logret_-0020bp_and_below'].ix[i]; usdmxn_logret_m0020bp_and_above = dvislr['usdmxn_logret_-0020bp_and_above'].ix[i];\n",
    "  usdmxn_logret_m0030bp_and_below = dvislr['usdmxn_logret_-0030bp_and_below'].ix[i]; usdmxn_logret_m0030bp_and_above = dvislr['usdmxn_logret_-0030bp_and_above'].ix[i];\n",
    "  usdmxn_logret_m0040bp_and_below = dvislr['usdmxn_logret_-0040bp_and_below'].ix[i]; usdmxn_logret_m0040bp_and_above = dvislr['usdmxn_logret_-0040bp_and_above'].ix[i];\n",
    "  usdmxn_logret_m0050bp_and_below = dvislr['usdmxn_logret_-0050bp_and_below'].ix[i]; usdmxn_logret_m0050bp_and_above = dvislr['usdmxn_logret_-0050bp_and_above'].ix[i];\n",
    "\n",
    "  usdmxn_logret_m0060bp_and_below = dvislr['usdmxn_logret_-0060bp_and_below'].ix[i]; usdmxn_logret_m0060bp_and_above = dvislr['usdmxn_logret_-0060bp_and_above'].ix[i];\n",
    "  usdmxn_logret_m0070bp_and_below = dvislr['usdmxn_logret_-0070bp_and_below'].ix[i]; usdmxn_logret_m0070bp_and_above = dvislr['usdmxn_logret_-0070bp_and_above'].ix[i];\n",
    "  usdmxn_logret_m0080bp_and_below = dvislr['usdmxn_logret_-0080bp_and_below'].ix[i]; usdmxn_logret_m0080bp_and_above = dvislr['usdmxn_logret_-0080bp_and_above'].ix[i];\n",
    "  usdmxn_logret_m0090bp_and_below = dvislr['usdmxn_logret_-0090bp_and_below'].ix[i]; usdmxn_logret_m0090bp_and_above = dvislr['usdmxn_logret_-0090bp_and_above'].ix[i];\n",
    "  usdmxn_logret_m0100bp_and_below = dvislr['usdmxn_logret_-0100bp_and_below'].ix[i]; usdmxn_logret_m0100bp_and_above = dvislr['usdmxn_logret_-0100bp_and_above'].ix[i];\n",
    "\n",
    "  usdmxn_logret_m0110bp_and_below = dvislr['usdmxn_logret_-0110bp_and_below'].ix[i]; usdmxn_logret_m0110bp_and_above = dvislr['usdmxn_logret_-0110bp_and_above'].ix[i];\n",
    "  usdmxn_logret_m0120bp_and_below = dvislr['usdmxn_logret_-0120bp_and_below'].ix[i]; usdmxn_logret_m0120bp_and_above = dvislr['usdmxn_logret_-0120bp_and_above'].ix[i];\n",
    "  usdmxn_logret_m0130bp_and_below = dvislr['usdmxn_logret_-0130bp_and_below'].ix[i]; usdmxn_logret_m0130bp_and_above = dvislr['usdmxn_logret_-0130bp_and_above'].ix[i];\n",
    "  usdmxn_logret_m0140bp_and_below = dvislr['usdmxn_logret_-0140bp_and_below'].ix[i]; usdmxn_logret_m0140bp_and_above = dvislr['usdmxn_logret_-0140bp_and_above'].ix[i];\n",
    "  usdmxn_logret_m0150bp_and_below = dvislr['usdmxn_logret_-0150bp_and_below'].ix[i]; usdmxn_logret_m0150bp_and_above = dvislr['usdmxn_logret_-0150bp_and_above'].ix[i];\n",
    "\n",
    "  #==========================================================================================\n",
    "  # Select 2 levels: up and dn\n",
    "  #==========================================================================================\n",
    "#   bc_dn = usdmxn_logret_m0010bp_and_below; bc_up = usdmxn_logret_p0010bp_and_above;\n",
    "#   bc_dn = usdmxn_logret_m0020bp_and_below; bc_up = usdmxn_logret_p0020bp_and_above;\n",
    "#   bc_dn = usdmxn_logret_m0030bp_and_below; bc_up = usdmxn_logret_p0030bp_and_above;\n",
    "#   bc_dn = usdmxn_logret_m0040bp_and_below; bc_up = usdmxn_logret_p0040bp_and_above;\n",
    "#   bc_dn = usdmxn_logret_m0050bp_and_below; bc_up = usdmxn_logret_p0050bp_and_above;\n",
    "#   bc_dn = usdmxn_logret_m0060bp_and_below; bc_up = usdmxn_logret_p0060bp_and_above;\n",
    "  bc_dn = usdmxn_logret_m0070bp_and_below; bc_up = usdmxn_logret_p0070bp_and_above;\n",
    "#   bc_dn = usdmxn_logret_m0080bp_and_below; bc_up = usdmxn_logret_p0080bp_and_above;\n",
    "#   bc_dn = usdmxn_logret_m0090bp_and_below; bc_up = usdmxn_logret_p0090bp_and_above;\n",
    "#   bc_dn = usdmxn_logret_m0100bp_and_below; bc_up = usdmxn_logret_p0100bp_and_above;\n",
    "  \n",
    "  #   v_001_Tm005 = data_values_indexed_scaled_logret['1'].ix[i-5]  # lookback 1d\n",
    "\n",
    "  v_001_Tm000 = dvislr[1].ix[i];  v_002_Tm000 = dvislr[2].ix[i];  v_003_Tm000 = dvislr[3].ix[i];  v_004_Tm000 = dvislr[4].ix[i];  v_005_Tm000 = dvislr[5].ix[i];\n",
    "  v_006_Tm000 = dvislr[6].ix[i];  v_007_Tm000 = dvislr[7].ix[i];  v_008_Tm000 = dvislr[8].ix[i];  v_009_Tm000 = dvislr[9].ix[i];  v_010_Tm000 = dvislr[10].ix[i]\n",
    "\n",
    "  v_011_Tm000 = dvislr[11].ix[i]; v_012_Tm000 = dvislr[12].ix[i]; v_013_Tm000 = dvislr[13].ix[i]; v_014_Tm000 = dvislr[14].ix[i]; v_015_Tm000 = dvislr[15].ix[i];\n",
    "  v_016_Tm000 = dvislr[16].ix[i]; v_017_Tm000 = dvislr[17].ix[i]; v_018_Tm000 = dvislr[18].ix[i]; v_019_Tm000 = dvislr[19].ix[i]; v_020_Tm000 = dvislr[20].ix[i];\n",
    "  \n",
    "  v_021_Tm000 = dvislr[21].ix[i]; v_022_Tm000 = dvislr[22].ix[i]; v_023_Tm000 = dvislr[23].ix[i]; v_024_Tm000 = dvislr[24].ix[i]; v_025_Tm000 = dvislr[25].ix[i];\n",
    "  v_026_Tm000 = dvislr[26].ix[i]; v_027_Tm000 = dvislr[27].ix[i]; v_028_Tm000 = dvislr[28].ix[i]; v_029_Tm000 = dvislr[29].ix[i]; v_030_Tm000 = dvislr[30].ix[i];\n",
    "\n",
    "  v_031_Tm000 = dvislr[31].ix[i]; v_032_Tm000 = dvislr[32].ix[i]; v_033_Tm000 = dvislr[33].ix[i]; v_034_Tm000 = dvislr[34].ix[i]; v_035_Tm000 = dvislr[35].ix[i];\n",
    "  v_036_Tm000 = dvislr[36].ix[i]; v_037_Tm000 = dvislr[37].ix[i]; v_038_Tm000 = dvislr[38].ix[i]; v_039_Tm000 = dvislr[39].ix[i]; v_040_Tm000 = dvislr[40].ix[i];\n",
    "\n",
    "  v_041_Tm000 = dvislr[41].ix[i]; v_042_Tm000 = dvislr[42].ix[i]; v_043_Tm000 = dvislr[43].ix[i]; v_044_Tm000 = dvislr[44].ix[i]; v_045_Tm000 = dvislr[45].ix[i];\n",
    "  v_046_Tm000 = dvislr[46].ix[i]; v_047_Tm000 = dvislr[47].ix[i]; v_048_Tm000 = dvislr[48].ix[i]; v_049_Tm000 = dvislr[49].ix[i]; v_050_Tm000 = dvislr[50].ix[i];\n",
    "\n",
    "  v_051_Tm000 = dvislr[51].ix[i]; v_052_Tm000 = dvislr[52].ix[i]; v_053_Tm000 = dvislr[53].ix[i]; v_054_Tm000 = dvislr[54].ix[i]; v_055_Tm000 = dvislr[55].ix[i];\n",
    "  v_056_Tm000 = dvislr[56].ix[i]; v_057_Tm000 = dvislr[57].ix[i]; v_058_Tm000 = dvislr[58].ix[i]; v_059_Tm000 = dvislr[59].ix[i]; v_060_Tm000 = dvislr[60].ix[i];\n",
    "\n",
    "  v_061_Tm000 = dvislr[61].ix[i]; v_062_Tm000 = dvislr[62].ix[i]; v_063_Tm000 = dvislr[63].ix[i]; v_064_Tm000 = dvislr[64].ix[i]; v_065_Tm000 = dvislr[65].ix[i];\n",
    "  v_066_Tm000 = dvislr[66].ix[i]; v_067_Tm000 = dvislr[67].ix[i]; v_068_Tm000 = dvislr[68].ix[i]; v_069_Tm000 = dvislr[69].ix[i]; v_070_Tm000 = dvislr[70].ix[i];\n",
    "\n",
    "  v_071_Tm000 = dvislr[71].ix[i]; v_072_Tm000 = dvislr[72].ix[i]; v_073_Tm000 = dvislr[73].ix[i]; v_074_Tm000 = dvislr[74].ix[i]; v_075_Tm000 = dvislr[75].ix[i];\n",
    "  v_076_Tm000 = dvislr[76].ix[i]; v_077_Tm000 = dvislr[77].ix[i]; v_078_Tm000 = dvislr[78].ix[i]; v_079_Tm000 = dvislr[79].ix[i]; v_080_Tm000 = dvislr[80].ix[i];\n",
    "\n",
    "  v_081_Tm000 = dvislr[81].ix[i]; v_082_Tm000 = dvislr[82].ix[i]; v_083_Tm000 = dvislr[83].ix[i]; v_084_Tm000 = dvislr[84].ix[i]; v_085_Tm000 = dvislr[85].ix[i];\n",
    "  v_086_Tm000 = dvislr[86].ix[i];\n",
    "\n",
    "  # training_test_data: append data\n",
    "  training_test_data = training_test_data.append(\n",
    "    {\n",
    "#       'usdmxn_logret_positive':usdmxn_logret_positive,\n",
    "#       'usdmxn_logret_negative':usdmxn_logret_negative,\n",
    "#       'usdmxn_logret_positive_0050bp':usdmxn_logret_positive_0050bp,\n",
    "#       'usdmxn_logret_negative_0050bp':usdmxn_logret_negative_0050bp,\n",
    "      \n",
    "      'bc_up':bc_up,\n",
    "      'bc_dn':bc_dn,\n",
    "      \n",
    "      '1':v_001_Tm000,  '2':v_002_Tm000, '3':v_003_Tm000, '4':v_004_Tm000, '5':v_005_Tm000,\n",
    "      '6':v_006_Tm000,  '7':v_007_Tm000, '8':v_008_Tm000, '9':v_009_Tm000, '10':v_010_Tm000,\n",
    "\n",
    "      '11':v_011_Tm000, '12':v_012_Tm000, '13':v_013_Tm000, '14':v_014_Tm000, '15':v_015_Tm000,\n",
    "      '16':v_016_Tm000, '17':v_017_Tm000, '18':v_018_Tm000, '19':v_019_Tm000, '20':v_020_Tm000,\n",
    "\n",
    "      '21':v_021_Tm000, '22':v_022_Tm000, '23':v_023_Tm000, '24':v_024_Tm000, '25':v_025_Tm000,\n",
    "      '26':v_026_Tm000, '27':v_027_Tm000, '28':v_028_Tm000, '29':v_029_Tm000, '30':v_030_Tm000,\n",
    "\n",
    "      '31':v_031_Tm000, '32':v_032_Tm000, '33':v_033_Tm000, '34':v_034_Tm000, '35':v_035_Tm000,\n",
    "      '36':v_036_Tm000, '37':v_037_Tm000, '38':v_038_Tm000, '39':v_039_Tm000, '40':v_040_Tm000,\n",
    "\n",
    "      '41':v_041_Tm000, '42':v_042_Tm000, '43':v_043_Tm000, '44':v_044_Tm000, '45':v_045_Tm000,\n",
    "      '46':v_046_Tm000, '47':v_047_Tm000, '48':v_048_Tm000, '49':v_049_Tm000, '50':v_050_Tm000,\n",
    "\n",
    "      '51':v_051_Tm000, '52':v_052_Tm000, '53':v_053_Tm000, '54':v_054_Tm000, '55':v_055_Tm000,\n",
    "      '56':v_056_Tm000, '57':v_057_Tm000, '58':v_058_Tm000, '59':v_059_Tm000, '60':v_060_Tm000,\n",
    "\n",
    "      '61':v_061_Tm000, '62':v_062_Tm000, '63':v_063_Tm000, '64':v_064_Tm000, '65':v_065_Tm000,\n",
    "      '66':v_066_Tm000, '67':v_067_Tm000, '68':v_068_Tm000, '69':v_069_Tm000, '70':v_070_Tm000,\n",
    "\n",
    "      '71':v_071_Tm000, '72':v_072_Tm000, '73':v_073_Tm000, '74':v_074_Tm000, '75':v_075_Tm000,\n",
    "      '76':v_076_Tm000, '77':v_077_Tm000, '78':v_078_Tm000, '79':v_079_Tm000, '80':v_080_Tm000,\n",
    "      \n",
    "      '81':v_081_Tm000, '82':v_082_Tm000, '83':v_083_Tm000, '84':v_084_Tm000, '85':v_085_Tm000,\n",
    "      '86':v_086_Tm000,\n",
    "      \n",
    "    },\n",
    "    ignore_index=True)\n",
    "  \n",
    "# data_values_indexed_scaled_logret: row [7,4279] \n",
    "# training_test_data               : row [0,4272] = 4273 rows\n",
    "# training_test_data: col 01-02 = 02 cols = binary outputs\n",
    "# training_test_data: col 03-88 = 86 cols = inputs  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bc_up</th>\n",
       "      <th>bc_dn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1128.000000</td>\n",
       "      <td>1128.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.525709</td>\n",
       "      <td>0.474291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.499560</td>\n",
       "      <td>0.499560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             bc_up        bc_dn\n",
       "count  1128.000000  1128.000000\n",
       "mean      0.525709     0.474291\n",
       "std       0.499560     0.499560\n",
       "min       0.000000     0.000000\n",
       "25%       0.000000     0.000000\n",
       "50%       1.000000     0.000000\n",
       "75%       1.000000     1.000000\n",
       "max       1.000000     1.000000"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training_test_data.describe()\n",
    "training_test_data[['bc_up','bc_dn']].describe()\n",
    "\n",
    "# check rows results\n",
    "# dvislr_nr_m0010_z0000_p0010 rows =3554 - 7 = 3547\n",
    "# dvislr_nr_m0020_z0000_p0020 rows =2904 - 7 = 2897\n",
    "# dvislr_nr_m0030_z0000_p0030 rows =2316 - 7 = 2309\n",
    "# dvislr_nr_m0040_z0000_p0040 rows =1837 - 7 = 1830\n",
    "# dvislr_nr_m0050_z0000_p0050 rows =1437 - 7 = 1430\n",
    "# dvislr_nr_m0060_z0000_p0060 rows =1135 - 7 = 1128\n",
    "# dvislr_nr_m0070_z0000_p0070 rows =893\n",
    "# dvislr_nr_m0080_z0000_p0080 rows =697\n",
    "# dvislr_nr_m0090_z0000_p0090 rows =559\n",
    "# dvislr_nr_m0100_z0000_p0100 rows =439"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create the training and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>77</th>\n",
       "      <th>78</th>\n",
       "      <th>79</th>\n",
       "      <th>80</th>\n",
       "      <th>81</th>\n",
       "      <th>82</th>\n",
       "      <th>83</th>\n",
       "      <th>84</th>\n",
       "      <th>85</th>\n",
       "      <th>86</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1128.000000</td>\n",
       "      <td>1128.000000</td>\n",
       "      <td>1128.000000</td>\n",
       "      <td>1128.000000</td>\n",
       "      <td>1128.000000</td>\n",
       "      <td>1128.000000</td>\n",
       "      <td>1128.000000</td>\n",
       "      <td>1128.000000</td>\n",
       "      <td>1128.000000</td>\n",
       "      <td>1128.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1128.000000</td>\n",
       "      <td>1128.000000</td>\n",
       "      <td>1128.000000</td>\n",
       "      <td>1128.000000</td>\n",
       "      <td>1128.000000</td>\n",
       "      <td>1128.000000</td>\n",
       "      <td>1128.000000</td>\n",
       "      <td>1128.000000</td>\n",
       "      <td>1128.000000</td>\n",
       "      <td>1128.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.000921</td>\n",
       "      <td>-0.000612</td>\n",
       "      <td>-0.000107</td>\n",
       "      <td>0.000174</td>\n",
       "      <td>0.002763</td>\n",
       "      <td>0.000081</td>\n",
       "      <td>-0.000135</td>\n",
       "      <td>0.000507</td>\n",
       "      <td>0.000589</td>\n",
       "      <td>0.000264</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000825</td>\n",
       "      <td>-0.000780</td>\n",
       "      <td>-0.003498</td>\n",
       "      <td>-0.004030</td>\n",
       "      <td>-0.009649</td>\n",
       "      <td>-0.000828</td>\n",
       "      <td>-0.000753</td>\n",
       "      <td>-0.000848</td>\n",
       "      <td>-0.000678</td>\n",
       "      <td>0.000185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.012126</td>\n",
       "      <td>0.027248</td>\n",
       "      <td>0.020950</td>\n",
       "      <td>0.004432</td>\n",
       "      <td>0.049899</td>\n",
       "      <td>0.001005</td>\n",
       "      <td>0.014616</td>\n",
       "      <td>0.009634</td>\n",
       "      <td>0.037564</td>\n",
       "      <td>0.003739</td>\n",
       "      <td>...</td>\n",
       "      <td>0.020015</td>\n",
       "      <td>0.019885</td>\n",
       "      <td>0.110957</td>\n",
       "      <td>0.240191</td>\n",
       "      <td>0.378439</td>\n",
       "      <td>0.022573</td>\n",
       "      <td>0.021444</td>\n",
       "      <td>0.021239</td>\n",
       "      <td>0.021208</td>\n",
       "      <td>0.005680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-0.066527</td>\n",
       "      <td>-0.614143</td>\n",
       "      <td>-0.101624</td>\n",
       "      <td>-0.020109</td>\n",
       "      <td>-0.837715</td>\n",
       "      <td>-0.006117</td>\n",
       "      <td>-0.146171</td>\n",
       "      <td>-0.053708</td>\n",
       "      <td>-0.385704</td>\n",
       "      <td>-0.024960</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.136928</td>\n",
       "      <td>-0.131822</td>\n",
       "      <td>-1.930917</td>\n",
       "      <td>-3.277145</td>\n",
       "      <td>-3.624341</td>\n",
       "      <td>-0.112705</td>\n",
       "      <td>-0.105936</td>\n",
       "      <td>-0.114583</td>\n",
       "      <td>-0.109933</td>\n",
       "      <td>-0.013134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-0.008351</td>\n",
       "      <td>-0.008547</td>\n",
       "      <td>-0.011057</td>\n",
       "      <td>-0.002419</td>\n",
       "      <td>-0.005569</td>\n",
       "      <td>-0.000315</td>\n",
       "      <td>-0.000615</td>\n",
       "      <td>-0.002524</td>\n",
       "      <td>-0.007303</td>\n",
       "      <td>-0.001368</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.010882</td>\n",
       "      <td>-0.010807</td>\n",
       "      <td>-0.020199</td>\n",
       "      <td>-0.024052</td>\n",
       "      <td>-0.032944</td>\n",
       "      <td>-0.013403</td>\n",
       "      <td>-0.012643</td>\n",
       "      <td>-0.012278</td>\n",
       "      <td>-0.011888</td>\n",
       "      <td>-0.000433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.006223</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000182</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000197</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000875</td>\n",
       "      <td>-0.000371</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.001132</td>\n",
       "      <td>-0.000885</td>\n",
       "      <td>-0.000130</td>\n",
       "      <td>-0.001282</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.009541</td>\n",
       "      <td>0.008484</td>\n",
       "      <td>0.009965</td>\n",
       "      <td>0.002867</td>\n",
       "      <td>0.010439</td>\n",
       "      <td>0.000504</td>\n",
       "      <td>0.000731</td>\n",
       "      <td>0.002631</td>\n",
       "      <td>0.009637</td>\n",
       "      <td>0.001789</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009629</td>\n",
       "      <td>0.009593</td>\n",
       "      <td>0.018318</td>\n",
       "      <td>0.021716</td>\n",
       "      <td>0.037170</td>\n",
       "      <td>0.012441</td>\n",
       "      <td>0.011955</td>\n",
       "      <td>0.011727</td>\n",
       "      <td>0.011705</td>\n",
       "      <td>0.000586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.070259</td>\n",
       "      <td>0.094618</td>\n",
       "      <td>0.130605</td>\n",
       "      <td>0.020071</td>\n",
       "      <td>0.700986</td>\n",
       "      <td>0.005774</td>\n",
       "      <td>0.229973</td>\n",
       "      <td>0.082470</td>\n",
       "      <td>0.697777</td>\n",
       "      <td>0.035906</td>\n",
       "      <td>...</td>\n",
       "      <td>0.072736</td>\n",
       "      <td>0.082584</td>\n",
       "      <td>0.798726</td>\n",
       "      <td>4.235250</td>\n",
       "      <td>3.007205</td>\n",
       "      <td>0.081709</td>\n",
       "      <td>0.082212</td>\n",
       "      <td>0.078476</td>\n",
       "      <td>0.085061</td>\n",
       "      <td>0.176596</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 86 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 1            2            3            4            5  \\\n",
       "count  1128.000000  1128.000000  1128.000000  1128.000000  1128.000000   \n",
       "mean      0.000921    -0.000612    -0.000107     0.000174     0.002763   \n",
       "std       0.012126     0.027248     0.020950     0.004432     0.049899   \n",
       "min      -0.066527    -0.614143    -0.101624    -0.020109    -0.837715   \n",
       "25%      -0.008351    -0.008547    -0.011057    -0.002419    -0.005569   \n",
       "50%       0.006223     0.000000     0.000000     0.000182     0.000000   \n",
       "75%       0.009541     0.008484     0.009965     0.002867     0.010439   \n",
       "max       0.070259     0.094618     0.130605     0.020071     0.700986   \n",
       "\n",
       "                 6            7            8            9           10  \\\n",
       "count  1128.000000  1128.000000  1128.000000  1128.000000  1128.000000   \n",
       "mean      0.000081    -0.000135     0.000507     0.000589     0.000264   \n",
       "std       0.001005     0.014616     0.009634     0.037564     0.003739   \n",
       "min      -0.006117    -0.146171    -0.053708    -0.385704    -0.024960   \n",
       "25%      -0.000315    -0.000615    -0.002524    -0.007303    -0.001368   \n",
       "50%       0.000046     0.000000     0.000000     0.000000     0.000197   \n",
       "75%       0.000504     0.000731     0.002631     0.009637     0.001789   \n",
       "max       0.005774     0.229973     0.082470     0.697777     0.035906   \n",
       "\n",
       "          ...                77           78           79           80  \\\n",
       "count     ...       1128.000000  1128.000000  1128.000000  1128.000000   \n",
       "mean      ...         -0.000825    -0.000780    -0.003498    -0.004030   \n",
       "std       ...          0.020015     0.019885     0.110957     0.240191   \n",
       "min       ...         -0.136928    -0.131822    -1.930917    -3.277145   \n",
       "25%       ...         -0.010882    -0.010807    -0.020199    -0.024052   \n",
       "50%       ...         -0.000875    -0.000371     0.000000     0.000000   \n",
       "75%       ...          0.009629     0.009593     0.018318     0.021716   \n",
       "max       ...          0.072736     0.082584     0.798726     4.235250   \n",
       "\n",
       "                81           82           83           84           85  \\\n",
       "count  1128.000000  1128.000000  1128.000000  1128.000000  1128.000000   \n",
       "mean     -0.009649    -0.000828    -0.000753    -0.000848    -0.000678   \n",
       "std       0.378439     0.022573     0.021444     0.021239     0.021208   \n",
       "min      -3.624341    -0.112705    -0.105936    -0.114583    -0.109933   \n",
       "25%      -0.032944    -0.013403    -0.012643    -0.012278    -0.011888   \n",
       "50%       0.000000    -0.001132    -0.000885    -0.000130    -0.001282   \n",
       "75%       0.037170     0.012441     0.011955     0.011727     0.011705   \n",
       "max       3.007205     0.081709     0.082212     0.078476     0.085061   \n",
       "\n",
       "                86  \n",
       "count  1128.000000  \n",
       "mean      0.000185  \n",
       "std       0.005680  \n",
       "min      -0.013134  \n",
       "25%      -0.000433  \n",
       "50%       0.000000  \n",
       "75%       0.000586  \n",
       "max       0.176596  \n",
       "\n",
       "[8 rows x 86 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 86 input columns\n",
    "predictors_tf = training_test_data[training_test_data.columns[0+2+(31*2):0+2+(31*2)+86]]\n",
    "predictors_tf.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bc_up</th>\n",
       "      <th>bc_dn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1128.000000</td>\n",
       "      <td>1128.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.525709</td>\n",
       "      <td>0.474291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.499560</td>\n",
       "      <td>0.499560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             bc_up        bc_dn\n",
       "count  1128.000000  1128.000000\n",
       "mean      0.525709     0.474291\n",
       "std       0.499560     0.499560\n",
       "min       0.000000     0.000000\n",
       "25%       0.000000     0.000000\n",
       "50%       1.000000     0.000000\n",
       "75%       1.000000     1.000000\n",
       "max       1.000000     1.000000"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2 output columns\n",
    "classes_tf = training_test_data[training_test_data.columns[0:2]] # col 0, 1\n",
    "# classes_tf = training_test_data[training_test_data.columns[2:4]] # col 2, 3\n",
    "classes_tf.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_set_size=902\n",
      "test_set_size=226\n"
     ]
    }
   ],
   "source": [
    "# Split: train data = 80%\n",
    "training_set_size = int(len(training_test_data) * 0.8)\n",
    "print 'training_set_size=' + str(training_set_size )\n",
    "# Split: test data = 20%\n",
    "test_set_size = len(training_test_data) - training_set_size\n",
    "print 'test_set_size=' + str(test_set_size )\n",
    "\n",
    "# After noise removed\n",
    "# training_set_size=1144\n",
    "# test_set_size=286\n",
    "\n",
    "# noise remove stratification: [-0010,+0010]: training_set_size=2837, test_set_size=710\n",
    "# noise remove stratification: [-0020,+0020]: training_set_size=2317, test_set_size=580\n",
    "# noise remove stratification: [-0030,+0030]: training_set_size=1847, test_set_size=462\n",
    "# noise remove stratification: [-0040,+0040]: training_set_size=1464, test_set_size=366\n",
    "# noise remove stratification: [-0050,+0050]: training_set_size=1144, test_set_size=286\n",
    "# noise remove stratification: [-0060,+0060]: training_set_size=902,  test_set_size=226\n",
    "# noise remove stratification: [-0070,+0070]: \n",
    "# noise remove stratification: [-0080,+0080]: \n",
    "# noise remove stratification: [-0090,+0090]: \n",
    "# noise remove stratification: [-0100,+0100]: \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split into training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "training_predictors_tf = predictors_tf[:training_set_size]\n",
    "training_classes_tf = classes_tf[:training_set_size]\n",
    "test_predictors_tf = predictors_tf[training_set_size:]\n",
    "test_classes_tf = classes_tf[training_set_size:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define some metrics here to evaluate the models.\n",
    "\n",
    "* [Precision](https://en.wikipedia.org/wiki/Precision_and_recall#Precision) -  The ability of the classifier not to label as positive a sample that is negative.\n",
    "* [Recall](https://en.wikipedia.org/wiki/Precision_and_recall#Recall) - The ability of the classifier to find all the positive samples.\n",
    "* [F1 Score](https://en.wikipedia.org/wiki/F1_score) - A weighted average of the precision and recall, where an F1 score reaches its best value at 1 and worst score at 0.\n",
    "* Accuracy - The percentage correctly predicted in the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def tf_confusion_metrics(model, actual_classes, session, feed_dict):\n",
    "  predictions = tf.argmax(model, 1)\n",
    "  actuals = tf.argmax(actual_classes, 1)\n",
    "\n",
    "  ones_like_actuals = tf.ones_like(actuals)\n",
    "  zeros_like_actuals = tf.zeros_like(actuals)\n",
    "  ones_like_predictions = tf.ones_like(predictions)\n",
    "  zeros_like_predictions = tf.zeros_like(predictions)\n",
    "\n",
    "  tp_op = tf.reduce_sum(\n",
    "    tf.cast(\n",
    "      tf.logical_and(\n",
    "        tf.equal(actuals, ones_like_actuals), \n",
    "        tf.equal(predictions, ones_like_predictions)\n",
    "      ), \n",
    "      \"float\"\n",
    "    )\n",
    "  )\n",
    "\n",
    "  tn_op = tf.reduce_sum(\n",
    "    tf.cast(\n",
    "      tf.logical_and(\n",
    "        tf.equal(actuals, zeros_like_actuals), \n",
    "        tf.equal(predictions, zeros_like_predictions)\n",
    "      ), \n",
    "      \"float\"\n",
    "    )\n",
    "  )\n",
    "\n",
    "  fp_op = tf.reduce_sum(\n",
    "    tf.cast(\n",
    "      tf.logical_and(\n",
    "        tf.equal(actuals, zeros_like_actuals), \n",
    "        tf.equal(predictions, ones_like_predictions)\n",
    "      ), \n",
    "      \"float\"\n",
    "    )\n",
    "  )\n",
    "\n",
    "  fn_op = tf.reduce_sum(\n",
    "    tf.cast(\n",
    "      tf.logical_and(\n",
    "        tf.equal(actuals, ones_like_actuals), \n",
    "        tf.equal(predictions, zeros_like_predictions)\n",
    "      ), \n",
    "      \"float\"\n",
    "    )\n",
    "  )\n",
    "\n",
    "  tp, tn, fp, fn = \\\n",
    "    session.run(\n",
    "      [tp_op, tn_op, fp_op, fn_op], \n",
    "      feed_dict\n",
    "    )\n",
    "\n",
    "  tpr = float(tp)/(float(tp) + float(fn))\n",
    "  fpr = float(fp)/(float(tp) + float(fn))\n",
    "\n",
    "  accuracy = (float(tp) + float(tn))/(float(tp) + float(fp) + float(fn) + float(tn))\n",
    "\n",
    "  recall = tpr\n",
    "  precision = float(tp)/(float(tp) + float(fp))\n",
    "  \n",
    "  f1_score = (2 * (precision * recall)) / (precision + recall)\n",
    "  \n",
    "  print 'Precision = ', precision\n",
    "  print 'Recall = ', recall\n",
    "  print 'F1 Score = ', f1_score\n",
    "  print 'Accuracy = ', accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary classification with TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, get some tensors flowing. The model is binary classification expressed in TensorFlow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tensorflow session\n",
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_predictors=86\n",
      "num_classes=2\n"
     ]
    }
   ],
   "source": [
    "# Define variables for the number of predictors and number of classes to remove magic numbers from our code.\n",
    "num_predictors = len(training_predictors_tf.columns) # 24-6=18 in the default case\n",
    "num_classes = len(training_classes_tf.columns) # 2 in the default case\n",
    "print 'num_predictors=' + str(num_predictors)\n",
    "print 'num_classes=' + str(num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define placeholders for the data we feed into the process - feature data and actual classes.\n",
    "# input = 18\n",
    "feature_data = tf.placeholder(\"float\", [None, num_predictors])\n",
    "# output = 2\n",
    "actual_classes = tf.placeholder(\"float\", [None, num_classes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define a matrix of weights and initialize it with some small random values.\n",
    "weights = tf.Variable(tf.truncated_normal([num_predictors, num_classes], stddev=0.0001))\n",
    "biases = tf.Variable(tf.ones([num_classes]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define our model...\n",
    "# Here we take a softmax regression of the product of our feature data and weights.\n",
    "model = tf.nn.softmax(tf.matmul(feature_data, weights) + biases)\n",
    "\n",
    "# Define a cost function (we're using the cross entropy).\n",
    "cost = -tf.reduce_sum(actual_classes*tf.log(model))\n",
    "\n",
    "# Define a training step...\n",
    "# Here we use gradient descent with a learning rate of 0.01 using the cost function we just defined.\n",
    "training_step = tf.train.AdamOptimizer(learning_rate=0.0001).minimize(cost)\n",
    "\n",
    "init = tf.initialize_all_variables()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ -1.19075463e-04   5.21266084e-05]\n",
      " [ -7.93897107e-06  -3.01031469e-06]\n",
      " [  4.90011917e-05  -5.81187051e-05]\n",
      " [  1.21333414e-05   1.45631580e-04]\n",
      " [ -1.32475485e-04  -9.35834178e-05]\n",
      " [  2.40627087e-05   4.88963524e-05]\n",
      " [  5.46589181e-05  -9.34279742e-05]\n",
      " [  8.56469342e-06   1.75672685e-04]\n",
      " [ -9.23640400e-05  -7.35338617e-05]\n",
      " [ -8.66797054e-05   9.84547223e-05]\n",
      " [ -5.46369556e-05  -1.41040684e-04]\n",
      " [  4.45059231e-05   3.50342307e-05]\n",
      " [ -2.30454339e-06   1.12168687e-04]\n",
      " [  2.35478656e-05  -9.92986679e-05]\n",
      " [ -9.31521645e-05  -4.61520722e-05]\n",
      " [  2.63135062e-05   9.50964313e-05]\n",
      " [ -1.53614237e-04   7.12975161e-05]\n",
      " [ -3.20473919e-05   3.06708425e-05]\n",
      " [  6.16221441e-05  -2.55244577e-05]\n",
      " [ -2.03459294e-05  -3.29161376e-05]\n",
      " [ -1.84350618e-04  -6.06808771e-06]\n",
      " [ -6.34520911e-05  -8.38328288e-06]\n",
      " [ -7.84539880e-05  -3.64078551e-05]\n",
      " [ -2.09466943e-05  -1.86818317e-04]\n",
      " [  1.44720660e-04  -1.38785632e-04]\n",
      " [  1.41966433e-04  -4.01814432e-05]\n",
      " [ -7.50212275e-05   1.12914917e-04]\n",
      " [  2.93412504e-05   1.90761260e-04]\n",
      " [  1.45301645e-04  -2.67815230e-05]\n",
      " [ -1.05898525e-05  -4.04732600e-05]\n",
      " [  2.17457928e-05  -8.65768234e-05]\n",
      " [  1.61676944e-05   8.04884039e-05]\n",
      " [  2.69664615e-05  -5.88150651e-05]\n",
      " [  9.29058879e-05   4.94502565e-05]\n",
      " [ -1.16530879e-04   1.01271093e-04]\n",
      " [  6.15731478e-05  -5.09287420e-05]\n",
      " [  1.19364318e-04  -5.28428609e-05]\n",
      " [ -1.13697017e-04   9.31829636e-05]\n",
      " [  1.94871682e-05   2.84528437e-06]\n",
      " [  3.49428083e-05   5.87138238e-05]\n",
      " [ -1.43803045e-04   1.31057895e-04]\n",
      " [  1.27953055e-04  -2.10641683e-05]\n",
      " [ -6.09432791e-05   2.74354152e-05]\n",
      " [  1.19756005e-04   7.47144222e-05]\n",
      " [ -1.19235003e-04   3.91550311e-05]\n",
      " [  5.25958094e-05   1.01649930e-05]\n",
      " [ -2.21439350e-05   1.94763285e-04]\n",
      " [  1.26987390e-04   1.12161011e-04]\n",
      " [ -1.41333203e-05  -1.37662437e-05]\n",
      " [ -3.16529586e-05   6.64198087e-05]\n",
      " [ -1.38743810e-04   1.05913268e-05]\n",
      " [ -7.70620245e-05  -7.63860953e-05]\n",
      " [ -6.85290797e-05   3.92184811e-05]\n",
      " [ -5.05157732e-06   1.56321272e-04]\n",
      " [ -1.97128422e-04  -3.58365651e-05]\n",
      " [  1.73579523e-04   1.28025553e-04]\n",
      " [ -1.44781967e-04   8.05082818e-05]\n",
      " [  7.70378028e-05   7.41910253e-06]\n",
      " [  1.70088882e-04  -1.58698473e-04]\n",
      " [ -9.43885098e-05  -1.85071201e-06]\n",
      " [  7.10361928e-05  -3.22671003e-05]\n",
      " [  8.12183571e-05  -4.48488681e-05]\n",
      " [  1.30753047e-04   6.50717848e-05]\n",
      " [ -1.42850395e-05   4.71073654e-05]\n",
      " [ -3.09295065e-05  -5.61314919e-05]\n",
      " [  8.75456972e-05   5.58799475e-05]\n",
      " [  7.27178485e-05   7.57004309e-05]\n",
      " [ -3.68990732e-05   4.59067305e-05]\n",
      " [  1.32702698e-04  -6.54161122e-05]\n",
      " [ -1.49604230e-05   3.24954982e-07]\n",
      " [  7.20756798e-05  -3.96591131e-05]\n",
      " [ -1.05219529e-04   1.00344514e-04]\n",
      " [ -3.08393101e-05   5.87078066e-05]\n",
      " [ -2.84794987e-05  -2.30795595e-05]\n",
      " [  3.27709240e-05  -5.53267528e-05]\n",
      " [ -6.21699437e-05  -3.88072294e-05]\n",
      " [ -3.65834931e-05  -1.63163524e-04]\n",
      " [  9.93233116e-05  -3.68749752e-05]\n",
      " [  1.22991842e-04   1.16821284e-04]\n",
      " [  1.71439984e-04  -9.14727498e-05]\n",
      " [  1.10983092e-04  -5.38471868e-05]\n",
      " [ -4.17717529e-05  -8.99485167e-06]\n",
      " [ -9.79017059e-05  -1.54151110e-06]\n",
      " [ -6.99149241e-05  -6.89019580e-05]\n",
      " [  4.30784530e-05   8.96828424e-05]\n",
      " [  3.60757639e-07   5.06557881e-05]]\n"
     ]
    }
   ],
   "source": [
    "# BEFORE the model has been run, ie. not yet trained\n",
    "# display weights\n",
    "w_86_2 = sess.run(weights)    \n",
    "print w_86_2 \n",
    "\n",
    "# Expect something like this (small numbers e-05): \n",
    "# [[ -6.66152628e-05  -6.49469657e-05]\n",
    "#  [  2.86067752e-05   3.27789494e-05]\n",
    "#  [  4.73170294e-05   1.85125769e-04]\n",
    "#  [ -6.42955492e-05   4.16024632e-05]\n",
    "#  [ -1.54488771e-05  -2.10903818e-05]\n",
    "#  [  4.62506796e-05  -2.98624745e-05]\n",
    "#  [ -5.35508079e-05  -1.30856875e-04]\n",
    "#  [ -1.70812025e-04   1.33106529e-04]\n",
    "#  [ -1.45097118e-04  -1.80459567e-04]\n",
    "#  [  1.02448161e-04   8.27739277e-05]\n",
    "#  [ -1.33277281e-04  -4.04360726e-05]\n",
    "#  [ -1.36186543e-04   8.62382149e-05]\n",
    "#  [  2.80324894e-05  -2.20580205e-05]\n",
    "#  [ -4.52000713e-05   2.54859442e-05]\n",
    "#  [ -6.20259525e-05  -6.95227573e-05]\n",
    "#  [  1.68935469e-04  -4.03221093e-05]\n",
    "#  [ -9.51008842e-05   4.10227585e-05]\n",
    "#  [ -5.90909331e-05  -9.20566745e-05]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll train our model in the following snippet. The approach of TensorFlow to executing graph operations allows fine-grained control over the process. Any operation you provide to the session as part of the run operation will be executed and the results returned. You can provide a list of multiple operations.\n",
    "\n",
    "You'll train the model over 30,000 iterations using the full dataset each time. Every thousandth iteration we'll assess the accuracy of the model on the training data to assess progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000 0.672949\n",
      "10000 0.72173\n",
      "15000 0.750554\n",
      "20000 0.761641\n",
      "25000 0.783814\n",
      "30000 0.792683\n"
     ]
    }
   ],
   "source": [
    "correct_prediction = tf.equal(tf.argmax(model, 1), tf.argmax(actual_classes, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "\n",
    "for i in range(1, 30001):\n",
    "  sess.run(\n",
    "    training_step, \n",
    "    feed_dict={\n",
    "      feature_data: training_predictors_tf.values, \n",
    "      actual_classes: training_classes_tf.values.reshape(len(training_classes_tf.values), 2)\n",
    "    }\n",
    "  )\n",
    "  if i%5000 == 0:\n",
    "    print i, sess.run(\n",
    "      accuracy,\n",
    "      feed_dict={\n",
    "        feature_data: training_predictors_tf.values, \n",
    "        actual_classes: training_classes_tf.values.reshape(len(training_classes_tf.values), 2)\n",
    "      }\n",
    "    )\n",
    "    \n",
    "# Expect:\n",
    "# 5000 0.603862\n",
    "# 10000 0.630778\n",
    "# 15000 0.639555\n",
    "# 20000 0.654184\n",
    "# 25000 0.660035\n",
    "# 30000 0.666764\n",
    "\n",
    "# After using < -0.5% and > +0.5%\n",
    "# 5000 0.928906\n",
    "# 10000 0.928906\n",
    "# 15000 0.928906\n",
    "# 20000 0.928906\n",
    "# 25000 0.928906\n",
    "# 30000 0.928906\n",
    "\n",
    "# Noise Removal: [-0010, +0010]\n",
    "# 5000 0.617201\n",
    "# 10000 0.64399\n",
    "# 15000 0.665139\n",
    "# 20000 0.683468\n",
    "# 25000 0.688403\n",
    "# 30000 0.691928\n",
    "\n",
    "# Noise Removal: [-0020, +0020]\n",
    "# 5000 0.625809\n",
    "# 10000 0.6612\n",
    "# 15000 0.680622\n",
    "# 20000 0.695727\n",
    "# 25000 0.708675\n",
    "# 30000 0.712128\n",
    "\n",
    "# Noise Removal: [-0030, +0030]\n",
    "# 5000 0.635084\n",
    "# 10000 0.677315\n",
    "# 15000 0.701678\n",
    "# 20000 0.714672\n",
    "# 25000 0.729291\n",
    "# 30000 0.738495\n",
    "\n",
    "# Noise Removal: [-0040, +0040]\n",
    "# 5000 0.645492\n",
    "# 10000 0.689891\n",
    "# 15000 0.717213\n",
    "# 20000 0.734973\n",
    "# 25000 0.748634\n",
    "# 30000 0.758197\n",
    "\n",
    "# Noise Removal: [-0050, +0050]\n",
    "# 5000 0.653846\n",
    "# 10000 0.702797\n",
    "# 15000 0.730769\n",
    "# 20000 0.740385\n",
    "# 25000 0.75\n",
    "# 30000 0.768357\n",
    "\n",
    "# Noise Removal: [-0060, +0060]\n",
    "# 5000 0.672949\n",
    "# 10000 0.72173\n",
    "# 15000 0.750554\n",
    "# 20000 0.761641\n",
    "# 25000 0.783814\n",
    "# 30000 0.792683\n",
    "\n",
    "# Noise Removal: [-0070, +0070]\n",
    "\n",
    "# Noise Removal: [-0080, +0080]\n",
    "\n",
    "# Noise Removal: [-0090, +0090]\n",
    "\n",
    "# Noise Removal: [-0100, +0100]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using all data:\n",
    "Accuracy  66.6% on training data\n",
    "That is OK, better than random."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After noise removed, We get 77 % accuracy on training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision =  0.644736842105\n",
      "Recall =  0.453703703704\n",
      "F1 Score =  0.532608695652\n",
      "Accuracy =  0.619469026549\n"
     ]
    }
   ],
   "source": [
    "feed_dict= {\n",
    "  feature_data: test_predictors_tf.values,\n",
    "  actual_classes: test_classes_tf.values.reshape(len(test_classes_tf.values), 2)\n",
    "}\n",
    "\n",
    "tf_confusion_metrics(model, actual_classes, sess, feed_dict)\n",
    "\n",
    "# Precision =  0.541176470588\n",
    "# Recall =  0.555555555556\n",
    "# F1 Score =  0.548271752086\n",
    "# Accuracy =  0.556725146199\n",
    "\n",
    "# After removing noise\n",
    "# Precision =  0.684210526316\n",
    "# Recall =  0.534246575342\n",
    "# F1 Score =  0.6\n",
    "# Accuracy =  0.636363636364\n",
    "\n",
    "# Noise Removal: [-0010, +0010]\n",
    "# Precision =  0.577235772358\n",
    "# Recall =  0.605113636364\n",
    "# F1 Score =  0.590846047157\n",
    "# Accuracy =  0.584507042254\n",
    "\n",
    "# Noise Removal: [-0020, +0020]\n",
    "# Precision =  0.588850174216\n",
    "# Recall =  0.603571428571\n",
    "# F1 Score =  0.596119929453\n",
    "# Accuracy =  0.605172413793\n",
    "\n",
    "# Noise Removal: [-0030, +0030]\n",
    "# Precision =  0.616740088106\n",
    "# Recall =  0.619469026549\n",
    "# F1 Score =  0.618101545254\n",
    "# Accuracy =  0.625541125541\n",
    "\n",
    "# Noise Removal: [-0040, +0040]\n",
    "# Precision =  0.622093023256\n",
    "# Recall =  0.58152173913\n",
    "# F1 Score =  0.601123595506\n",
    "# Accuracy =  0.612021857923\n",
    "\n",
    "# Noise Removal: [-0050, +0050]\n",
    "# Precision =  0.684210526316\n",
    "# Recall =  0.534246575342\n",
    "# F1 Score =  0.6\n",
    "# Accuracy =  0.636363636364\n",
    "\n",
    "# Noise Removal: [-0060, +0060]\n",
    "# Precision =  0.644736842105\n",
    "# Recall =  0.453703703704\n",
    "# F1 Score =  0.532608695652\n",
    "# Accuracy =  0.619469026549\n",
    "\n",
    "# Noise Removal: [-0070, +0070]\n",
    "\n",
    "# Noise Removal: [-0080, +0080]\n",
    "\n",
    "# Noise Removal: [-0090, +0090]\n",
    "\n",
    "# Noise Removal: [-0100, +0100]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using all data, Accuracy 55.6% on test data.\n",
    "After removing noise, Accuracy 64% on test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2.96132183 -2.96138859]\n",
      " [-2.81565118  2.81564021]\n",
      " [-2.91209364  2.91208458]\n",
      " [-1.96235788  1.96251559]\n",
      " [ 2.8368926  -2.83711863]\n",
      " [ 2.75949311 -2.75942016]\n",
      " [-1.98157299  1.98153412]\n",
      " [ 2.53368449 -2.53349996]\n",
      " [ 2.89654422 -2.89671016]\n",
      " [-0.23813249  0.23814453]\n",
      " [-2.81490278  2.81470704]\n",
      " [-2.73814321  2.73822236]\n",
      " [ 0.22130398 -0.22119404]\n",
      " [-2.75612378  2.7560482 ]\n",
      " [-0.17750882  0.17736995]\n",
      " [-1.80428767  1.80440652]\n",
      " [ 0.07182407 -0.0720861 ]\n",
      " [-0.07663839  0.07702472]\n",
      " [-0.00562787  0.00448106]\n",
      " [ 0.93367016 -0.93386751]\n",
      " [-0.08873752  0.08757472]\n",
      " [-2.79255915  2.79248691]\n",
      " [-0.00924385  0.00885756]\n",
      " [-1.18252063  1.1822232 ]\n",
      " [ 2.76867199 -2.76866579]\n",
      " [-2.60087967  2.60098171]\n",
      " [-2.60856295  2.60860109]\n",
      " [-2.54400396  2.54422426]\n",
      " [-0.5156337   0.51574874]\n",
      " [-0.04585199  0.0454029 ]\n",
      " [-0.33829668  0.33898139]\n",
      " [-0.3114343   0.31094575]\n",
      " [-0.27259123  0.27281645]\n",
      " [ 0.10814866 -0.10801176]\n",
      " [ 3.05603099 -3.05604649]\n",
      " [ 0.65164852 -0.65340197]\n",
      " [-2.87353039  2.87359715]\n",
      " [ 0.13384308 -0.13452148]\n",
      " [-2.84986854  2.84989047]\n",
      " [-1.19416058  1.19343698]\n",
      " [ 0.18279283 -0.18376578]\n",
      " [-2.89434028  2.89444733]\n",
      " [-2.92950153  2.92946792]\n",
      " [ 2.89931488 -2.89912033]\n",
      " [-2.94512844  2.94504833]\n",
      " [-2.52276874  2.52283144]\n",
      " [-1.66174948  1.6619184 ]\n",
      " [ 2.78863025 -2.78839111]\n",
      " [ 2.76449966 -2.76452756]\n",
      " [-2.59381461  2.59384918]\n",
      " [ 0.35393935 -0.3538484 ]\n",
      " [-0.09722968  0.0962326 ]\n",
      " [-0.25459489  0.25454429]\n",
      " [ 2.18501925 -2.1848681 ]\n",
      " [-2.83858871  2.83835626]\n",
      " [-2.31984472  2.32014656]\n",
      " [-2.08586121  2.08579731]\n",
      " [ 2.93603826 -2.93595409]\n",
      " [ 2.94576645 -2.94575524]\n",
      " [ 1.98468816 -1.98478448]\n",
      " [-0.88337761  0.88341659]\n",
      " [ 1.24527562 -1.24523866]\n",
      " [ 1.49746799 -1.49727249]\n",
      " [ 1.32361472 -1.3235817 ]\n",
      " [-2.46868062  2.46859407]\n",
      " [-0.98793542  0.98807925]\n",
      " [-2.44804263  2.4481914 ]\n",
      " [-2.59847713  2.59848571]\n",
      " [-2.63626981  2.63633704]\n",
      " [ 2.09217    -2.09218478]\n",
      " [ 1.85861599 -1.85858345]\n",
      " [-0.77262449  0.77261943]\n",
      " [ 0.26886699 -0.26883864]\n",
      " [-2.86140966  2.86135793]\n",
      " [ 2.13986826 -2.13989091]\n",
      " [ 1.60073698 -1.60083807]\n",
      " [ 1.48903799 -1.48923874]\n",
      " [-0.78718209  0.78724432]\n",
      " [ 0.2802695  -0.28123733]\n",
      " [ 0.0952343  -0.09630137]\n",
      " [-0.0110385   0.01063544]\n",
      " [ 1.87171924 -1.87176991]\n",
      " [ 1.83849442 -1.83859408]\n",
      " [-1.99312603  1.99298716]\n",
      " [ 1.57980084 -1.5796684 ]\n",
      " [-2.77546072  2.77551198]]\n"
     ]
    }
   ],
   "source": [
    "# AFTER the model has been run, ie. trained\n",
    "# display weights\n",
    "w_86_2 = sess.run(weights)    \n",
    "print w_86_2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>77</th>\n",
       "      <th>78</th>\n",
       "      <th>79</th>\n",
       "      <th>80</th>\n",
       "      <th>81</th>\n",
       "      <th>82</th>\n",
       "      <th>83</th>\n",
       "      <th>84</th>\n",
       "      <th>85</th>\n",
       "      <th>86</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2837</th>\n",
       "      <td>0.001287</td>\n",
       "      <td>0.025642</td>\n",
       "      <td>0.036871</td>\n",
       "      <td>-0.006917</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.001215</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.001086</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.006211</td>\n",
       "      <td>...</td>\n",
       "      <td>0.020790</td>\n",
       "      <td>0.022198</td>\n",
       "      <td>0.014982</td>\n",
       "      <td>0.014815</td>\n",
       "      <td>0.024571</td>\n",
       "      <td>0.023684</td>\n",
       "      <td>0.020283</td>\n",
       "      <td>0.017523</td>\n",
       "      <td>0.018177</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2838</th>\n",
       "      <td>-0.004996</td>\n",
       "      <td>0.074108</td>\n",
       "      <td>0.030992</td>\n",
       "      <td>-0.008273</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.001400</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.003413</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000298</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011130</td>\n",
       "      <td>0.007887</td>\n",
       "      <td>0.003781</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.014815</td>\n",
       "      <td>0.012799</td>\n",
       "      <td>0.012440</td>\n",
       "      <td>0.012383</td>\n",
       "      <td>0.010842</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2839</th>\n",
       "      <td>-0.011748</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.004831</td>\n",
       "      <td>0.007302</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000746</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.001342</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.014738</td>\n",
       "      <td>-0.013037</td>\n",
       "      <td>0.015267</td>\n",
       "      <td>0.015038</td>\n",
       "      <td>0.024815</td>\n",
       "      <td>-0.009982</td>\n",
       "      <td>-0.008310</td>\n",
       "      <td>-0.007673</td>\n",
       "      <td>-0.006944</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2840</th>\n",
       "      <td>0.008148</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.004808</td>\n",
       "      <td>0.002475</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.000373</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.003274</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002930</td>\n",
       "      <td>0.002236</td>\n",
       "      <td>0.007722</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005038</td>\n",
       "      <td>-0.010126</td>\n",
       "      <td>-0.010860</td>\n",
       "      <td>-0.010956</td>\n",
       "      <td>-0.010878</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2841</th>\n",
       "      <td>-0.008101</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.001769</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.000187</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.001342</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000149</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.010107</td>\n",
       "      <td>-0.010464</td>\n",
       "      <td>0.007782</td>\n",
       "      <td>0.007605</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003648</td>\n",
       "      <td>0.003075</td>\n",
       "      <td>0.002895</td>\n",
       "      <td>0.004204</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2842</th>\n",
       "      <td>-0.006687</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007290</td>\n",
       "      <td>0.005704</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000280</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.001211</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.010863</td>\n",
       "      <td>-0.010792</td>\n",
       "      <td>0.007722</td>\n",
       "      <td>0.007547</td>\n",
       "      <td>0.004963</td>\n",
       "      <td>-0.017285</td>\n",
       "      <td>-0.013983</td>\n",
       "      <td>-0.012811</td>\n",
       "      <td>-0.008835</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2843</th>\n",
       "      <td>0.017320</td>\n",
       "      <td>0.024780</td>\n",
       "      <td>-0.007290</td>\n",
       "      <td>0.002685</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000374</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.002059</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.018012</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008681</td>\n",
       "      <td>0.007110</td>\n",
       "      <td>-0.015385</td>\n",
       "      <td>-0.022473</td>\n",
       "      <td>-0.019705</td>\n",
       "      <td>-0.002345</td>\n",
       "      <td>-0.001982</td>\n",
       "      <td>-0.003231</td>\n",
       "      <td>-0.005776</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2844</th>\n",
       "      <td>-0.002766</td>\n",
       "      <td>0.054505</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000448</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000561</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.002837</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000761</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.005868</td>\n",
       "      <td>-0.005391</td>\n",
       "      <td>-0.018904</td>\n",
       "      <td>-0.011050</td>\n",
       "      <td>-0.014528</td>\n",
       "      <td>0.010121</td>\n",
       "      <td>0.008394</td>\n",
       "      <td>0.008639</td>\n",
       "      <td>0.005240</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2845</th>\n",
       "      <td>0.027181</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001345</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.000935</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.001302</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001828</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010236</td>\n",
       "      <td>0.010159</td>\n",
       "      <td>0.011300</td>\n",
       "      <td>0.022223</td>\n",
       "      <td>0.009662</td>\n",
       "      <td>-0.005426</td>\n",
       "      <td>-0.002658</td>\n",
       "      <td>-0.002167</td>\n",
       "      <td>0.001824</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2846</th>\n",
       "      <td>0.004090</td>\n",
       "      <td>-0.065201</td>\n",
       "      <td>-0.014423</td>\n",
       "      <td>0.007112</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002058</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002474</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.005198</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.025290</td>\n",
       "      <td>-0.024677</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.003738</td>\n",
       "      <td>0.009756</td>\n",
       "      <td>-0.016336</td>\n",
       "      <td>-0.015477</td>\n",
       "      <td>-0.012690</td>\n",
       "      <td>-0.012489</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 86 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             1         2         3         4    5         6    7         8  \\\n",
       "2837  0.001287  0.025642  0.036871 -0.006917  0.0 -0.001215  0.0 -0.001086   \n",
       "2838 -0.004996  0.074108  0.030992 -0.008273  0.0 -0.001400  0.0 -0.003413   \n",
       "2839 -0.011748  0.000000 -0.004831  0.007302  0.0  0.000746  0.0  0.000000   \n",
       "2840  0.008148  0.000000 -0.004808  0.002475  0.0 -0.000373  0.0  0.000000   \n",
       "2841 -0.008101  0.000000  0.000000 -0.001769  0.0 -0.000187  0.0 -0.001342   \n",
       "2842 -0.006687  0.000000  0.007290  0.005704  0.0  0.000280  0.0 -0.001211   \n",
       "2843  0.017320  0.024780 -0.007290  0.002685  0.0  0.000374  0.0 -0.002059   \n",
       "2844 -0.002766  0.054505  0.000000 -0.000448  0.0  0.000561  0.0 -0.002837   \n",
       "2845  0.027181  0.000000  0.000000  0.001345  0.0 -0.000935  0.0 -0.001302   \n",
       "2846  0.004090 -0.065201 -0.014423  0.007112  0.0  0.002058  0.0  0.002474   \n",
       "\n",
       "        9        10 ...         77        78        79        80        81  \\\n",
       "2837  0.0 -0.006211 ...   0.020790  0.022198  0.014982  0.014815  0.024571   \n",
       "2838  0.0  0.000298 ...   0.011130  0.007887  0.003781  0.000000 -0.014815   \n",
       "2839  0.0 -0.001342 ...  -0.014738 -0.013037  0.015267  0.015038  0.024815   \n",
       "2840  0.0 -0.003274 ...   0.002930  0.002236  0.007722  0.000000  0.005038   \n",
       "2841  0.0  0.000149 ...  -0.010107 -0.010464  0.007782  0.007605  0.000000   \n",
       "2842  0.0  0.000000 ...  -0.010863 -0.010792  0.007722  0.007547  0.004963   \n",
       "2843  0.0  0.018012 ...   0.008681  0.007110 -0.015385 -0.022473 -0.019705   \n",
       "2844  0.0  0.000761 ...  -0.005868 -0.005391 -0.018904 -0.011050 -0.014528   \n",
       "2845  0.0  0.001828 ...   0.010236  0.010159  0.011300  0.022223  0.009662   \n",
       "2846  0.0  0.005198 ...  -0.025290 -0.024677  0.000000 -0.003738  0.009756   \n",
       "\n",
       "            82        83        84        85   86  \n",
       "2837  0.023684  0.020283  0.017523  0.018177  0.0  \n",
       "2838  0.012799  0.012440  0.012383  0.010842  0.0  \n",
       "2839 -0.009982 -0.008310 -0.007673 -0.006944  0.0  \n",
       "2840 -0.010126 -0.010860 -0.010956 -0.010878  0.0  \n",
       "2841  0.003648  0.003075  0.002895  0.004204  0.0  \n",
       "2842 -0.017285 -0.013983 -0.012811 -0.008835  0.0  \n",
       "2843 -0.002345 -0.001982 -0.003231 -0.005776  0.0  \n",
       "2844  0.010121  0.008394  0.008639  0.005240  0.0  \n",
       "2845 -0.005426 -0.002658 -0.002167  0.001824  0.0  \n",
       "2846 -0.016336 -0.015477 -0.012690 -0.012489  0.0  \n",
       "\n",
       "[10 rows x 86 columns]"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# grab a sample input row from the test set\n",
    "sample_input_1_86 = test_predictors_tf[:10]\n",
    "# HTML(pd.DataFrame(sample_input_1_86).to_html())\n",
    "sample_input_1_86"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>77</th>\n",
       "      <th>78</th>\n",
       "      <th>79</th>\n",
       "      <th>80</th>\n",
       "      <th>81</th>\n",
       "      <th>82</th>\n",
       "      <th>83</th>\n",
       "      <th>84</th>\n",
       "      <th>85</th>\n",
       "      <th>86</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.002373</td>\n",
       "      <td>0.011384</td>\n",
       "      <td>0.004380</td>\n",
       "      <td>0.000922</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.000009</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.001078</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001542</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001310</td>\n",
       "      <td>-0.001477</td>\n",
       "      <td>0.003427</td>\n",
       "      <td>0.002997</td>\n",
       "      <td>0.002976</td>\n",
       "      <td>-0.001125</td>\n",
       "      <td>-0.000908</td>\n",
       "      <td>-0.000809</td>\n",
       "      <td>-0.000463</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.012239</td>\n",
       "      <td>0.037550</td>\n",
       "      <td>0.016630</td>\n",
       "      <td>0.005403</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001044</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001653</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.006522</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014295</td>\n",
       "      <td>0.013848</td>\n",
       "      <td>0.011813</td>\n",
       "      <td>0.013357</td>\n",
       "      <td>0.015593</td>\n",
       "      <td>0.013460</td>\n",
       "      <td>0.011883</td>\n",
       "      <td>0.010832</td>\n",
       "      <td>0.010156</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-0.011748</td>\n",
       "      <td>-0.065201</td>\n",
       "      <td>-0.014423</td>\n",
       "      <td>-0.008273</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.001400</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.003413</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.006211</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.025290</td>\n",
       "      <td>-0.024677</td>\n",
       "      <td>-0.018904</td>\n",
       "      <td>-0.022473</td>\n",
       "      <td>-0.019705</td>\n",
       "      <td>-0.017285</td>\n",
       "      <td>-0.015477</td>\n",
       "      <td>-0.012811</td>\n",
       "      <td>-0.012489</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-0.006265</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.004825</td>\n",
       "      <td>-0.001438</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.000794</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.001880</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.001007</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.010674</td>\n",
       "      <td>-0.010710</td>\n",
       "      <td>0.000945</td>\n",
       "      <td>-0.002804</td>\n",
       "      <td>-0.010896</td>\n",
       "      <td>-0.010090</td>\n",
       "      <td>-0.010223</td>\n",
       "      <td>-0.010135</td>\n",
       "      <td>-0.008362</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-0.000739</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001910</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000047</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.001256</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000224</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001469</td>\n",
       "      <td>-0.001578</td>\n",
       "      <td>0.007722</td>\n",
       "      <td>0.003774</td>\n",
       "      <td>0.005000</td>\n",
       "      <td>-0.003886</td>\n",
       "      <td>-0.002320</td>\n",
       "      <td>-0.002699</td>\n",
       "      <td>-0.001976</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.007134</td>\n",
       "      <td>0.025427</td>\n",
       "      <td>0.005468</td>\n",
       "      <td>0.004949</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000514</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.000271</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001561</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009847</td>\n",
       "      <td>0.007693</td>\n",
       "      <td>0.010420</td>\n",
       "      <td>0.013012</td>\n",
       "      <td>0.009733</td>\n",
       "      <td>0.008503</td>\n",
       "      <td>0.007064</td>\n",
       "      <td>0.007203</td>\n",
       "      <td>0.004981</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.027181</td>\n",
       "      <td>0.074108</td>\n",
       "      <td>0.036871</td>\n",
       "      <td>0.007302</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002058</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002474</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.018012</td>\n",
       "      <td>...</td>\n",
       "      <td>0.020790</td>\n",
       "      <td>0.022198</td>\n",
       "      <td>0.015267</td>\n",
       "      <td>0.022223</td>\n",
       "      <td>0.024815</td>\n",
       "      <td>0.023684</td>\n",
       "      <td>0.020283</td>\n",
       "      <td>0.017523</td>\n",
       "      <td>0.018177</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 86 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               1          2          3          4     5          6     7  \\\n",
       "count  10.000000  10.000000  10.000000  10.000000  10.0  10.000000  10.0   \n",
       "mean    0.002373   0.011384   0.004380   0.000922   0.0  -0.000009   0.0   \n",
       "std     0.012239   0.037550   0.016630   0.005403   0.0   0.001044   0.0   \n",
       "min    -0.011748  -0.065201  -0.014423  -0.008273   0.0  -0.001400   0.0   \n",
       "25%    -0.006265   0.000000  -0.004825  -0.001438   0.0  -0.000794   0.0   \n",
       "50%    -0.000739   0.000000   0.000000   0.001910   0.0   0.000047   0.0   \n",
       "75%     0.007134   0.025427   0.005468   0.004949   0.0   0.000514   0.0   \n",
       "max     0.027181   0.074108   0.036871   0.007302   0.0   0.002058   0.0   \n",
       "\n",
       "               8     9         10  ...          77         78         79  \\\n",
       "count  10.000000  10.0  10.000000  ...   10.000000  10.000000  10.000000   \n",
       "mean   -0.001078   0.0   0.001542  ...   -0.001310  -0.001477   0.003427   \n",
       "std     0.001653   0.0   0.006522  ...    0.014295   0.013848   0.011813   \n",
       "min    -0.003413   0.0  -0.006211  ...   -0.025290  -0.024677  -0.018904   \n",
       "25%    -0.001880   0.0  -0.001007  ...   -0.010674  -0.010710   0.000945   \n",
       "50%    -0.001256   0.0   0.000224  ...   -0.001469  -0.001578   0.007722   \n",
       "75%    -0.000271   0.0   0.001561  ...    0.009847   0.007693   0.010420   \n",
       "max     0.002474   0.0   0.018012  ...    0.020790   0.022198   0.015267   \n",
       "\n",
       "              80         81         82         83         84         85    86  \n",
       "count  10.000000  10.000000  10.000000  10.000000  10.000000  10.000000  10.0  \n",
       "mean    0.002997   0.002976  -0.001125  -0.000908  -0.000809  -0.000463   0.0  \n",
       "std     0.013357   0.015593   0.013460   0.011883   0.010832   0.010156   0.0  \n",
       "min    -0.022473  -0.019705  -0.017285  -0.015477  -0.012811  -0.012489   0.0  \n",
       "25%    -0.002804  -0.010896  -0.010090  -0.010223  -0.010135  -0.008362   0.0  \n",
       "50%     0.003774   0.005000  -0.003886  -0.002320  -0.002699  -0.001976   0.0  \n",
       "75%     0.013012   0.009733   0.008503   0.007064   0.007203   0.004981   0.0  \n",
       "max     0.022223   0.024815   0.023684   0.020283   0.017523   0.018177   0.0  \n",
       "\n",
       "[8 rows x 86 columns]"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_input_1_86.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ True False]\n",
      " [False  True]\n",
      " [ True False]\n",
      " [ True False]\n",
      " [False  True]\n",
      " [False  True]\n",
      " [ True False]\n",
      " [False  True]\n",
      " [ True False]\n",
      " [ True False]]\n",
      "usdmxn_pred = 1\n"
     ]
    }
   ],
   "source": [
    "# use the weights from our model to make a prediction\n",
    "output_1_2 = np.dot(sample_input_1_86, w_86_2) > 0\n",
    "print output_1_2\n",
    "usdmxn_up = output_1_2[0,0]\n",
    "usdmxn_dn = output_1_2[0,1]\n",
    "if usdmxn_up >= usdmxn_dn:\n",
    "  usdmxn_pred = 1 # up\n",
    "else:\n",
    "  usdmxn_pred = -1 # down\n",
    "print 'usdmxn_pred = '+ str(usdmxn_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 2: Neural Network (NN): Feed-Forward with 2 Hidden Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll now build a proper feed-forward neural net with two hidden layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tensorflow session new\n",
    "sess1 = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# inputs\n",
    "num_predictors = len(training_predictors_tf.columns)\n",
    "print 'num_predictors=' + str(num_predictors)\n",
    "\n",
    "# outputs\n",
    "num_classes = len(training_classes_tf.columns)\n",
    "print 'num_classes=' + str(num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_data = tf.placeholder(\"float\", [None, num_predictors])\n",
    "actual_classes = tf.placeholder(\"float\", [None, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# layer 1\n",
    "i_01_in = 86\n",
    "i_01_out = 100\n",
    "weights1 = tf.Variable(tf.truncated_normal([i_01_in, i_01_out], stddev=0.0001))\n",
    "biases1 = tf.Variable(tf.ones([i_01_out]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# layer 2\n",
    "i_02_in = i_01_out\n",
    "i_02_out = 25\n",
    "weights2 = tf.Variable(tf.truncated_normal([i_02_in, i_02_out], stddev=0.0001))\n",
    "biases2 = tf.Variable(tf.ones([i_02_out]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# layer 3\n",
    "i_03_in = i_02_out\n",
    "i_03_out = 2\n",
    "weights3 = tf.Variable(tf.truncated_normal([i_03_in, i_03_out], stddev=0.0001))\n",
    "biases3 = tf.Variable(tf.ones([i_03_out]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# neural net\n",
    "hidden_layer_0 = feature_data\n",
    "hidden_layer_1 = tf.nn.relu(tf.matmul(hidden_layer_0, weights1) + biases1)\n",
    "hidden_layer_2 = tf.nn.relu(tf.matmul(hidden_layer_1, weights2) + biases2)\n",
    "hidden_layer_3 = tf.matmul(hidden_layer_2, weights3) + biases3\n",
    "\n",
    "model = tf.nn.softmax(hidden_layer_3)\n",
    "\n",
    "cost = -tf.reduce_sum(actual_classes*tf.log(model))\n",
    "\n",
    "train_op1 = tf.train.AdamOptimizer(learning_rate=0.0001).minimize(cost)\n",
    "\n",
    "init = tf.initialize_all_variables()\n",
    "sess1.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# BEFORE the model has been run, ie. trained\n",
    "nn_e1 = sess1.run(biases1)    \n",
    "# HTML(pd.DataFrame(nn_e1).transpose().to_html())\n",
    "pd.DataFrame(nn_e1).transpose()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# BEFORE the model has been run, ie. trained\n",
    "nn_w1 = sess1.run(weights1)    \n",
    "# HTML(pd.DataFrame(nn_w1).to_html())\n",
    "pd.DataFrame(nn_w1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# BEFORE the model has been run, ie. trained\n",
    "nn_e2 = sess1.run(biases2)    \n",
    "# HTML(pd.DataFrame(nn_e2).transpose().to_html())\n",
    "pd.DataFrame(nn_e2).transpose()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# BEFORE the model has been run, ie. trained\n",
    "# display weights\n",
    "nn_w2 = sess1.run(weights2)    \n",
    "# HTML(pd.DataFrame(nn_w2).to_html())\n",
    "pd.DataFrame(nn_w2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# BEFORE the model has been run, ie. trained\n",
    "nn_e3 = sess1.run(biases3)    \n",
    "# HTML(pd.DataFrame(nn_e3).transpose().to_html())\n",
    "pd.DataFrame(nn_e3).transpose()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# BEFORE the model has been run, ie. trained\n",
    "# display weights\n",
    "nn_w3 = sess1.run(weights3)    \n",
    "# HTML(pd.DataFrame(nn_w3).to_html())\n",
    "pd.DataFrame(nn_w3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, you'll train the model over 10,000 iterations using the full dataset each time. Every thousandth iteration, you'll assess the accuracy of the model on the training data to assess progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "correct_prediction = tf.equal(tf.argmax(model, 1), tf.argmax(actual_classes, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "num_iterations = 10001\n",
    "num_iterations_until_diaplay = 1000\n",
    "\n",
    "for i in range(1, num_iterations):\n",
    "  sess1.run(\n",
    "    train_op1, \n",
    "    feed_dict={\n",
    "      feature_data: training_predictors_tf.values, \n",
    "      actual_classes: training_classes_tf.values.reshape(len(training_classes_tf.values), 2)\n",
    "    }\n",
    "  )\n",
    "  if i%num_iterations_until_diaplay == 0:\n",
    "    print i, sess1.run(\n",
    "      accuracy,\n",
    "      feed_dict={\n",
    "        feature_data: training_predictors_tf.values, \n",
    "        actual_classes: training_classes_tf.values.reshape(len(training_classes_tf.values), 2)\n",
    "      }\n",
    "    )\n",
    "  \n",
    "  \n",
    "  \n",
    "# 1000 0.67086\n",
    "# 2000 0.765652\n",
    "# 3000 0.828847\n",
    "# 4000 0.890287\n",
    "# 5000 0.937975\n",
    "# 6000 0.956115\n",
    "# 7000 0.971913\n",
    "# 8000 0.979813\n",
    "# 9000 0.985079\n",
    "# 10000 0.987712\n",
    "\n",
    "# 5000 0.93505\n",
    "# 10000 0.988297\n",
    "# 15000 0.491223 # overfit?\n",
    "# 20000 0.491223\n",
    "# 25000 0.491223\n",
    "# 30000 0.491223\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A significant improvement in accuracy with the training data shows that the hidden layers are adding additional capacity for learning to the model.\n",
    "\n",
    "Looking at precision, recall, and accuracy, you can see a measurable improvement in performance, but certainly not a [step function](https://wikipedia.org/wiki/Step_function). This indicates that we're likely reaching the limits of this relatively simple feature set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feed_dict= {\n",
    "  feature_data: test_predictors_tf.values,\n",
    "  actual_classes: test_classes_tf.values.reshape(len(test_classes_tf.values), 2)\n",
    "}\n",
    "\n",
    "tf_confusion_metrics(model, actual_classes, sess1, feed_dict)\n",
    "\n",
    "# Precision =  0.936768149883\n",
    "# Recall =  0.966183574879\n",
    "# F1 Score =  0.951248513674\n",
    "# Accuracy =  0.952046783626\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# AFTER the model has been run, ie. trained\n",
    "nn_e1 = sess1.run(biases1)    \n",
    "# HTML(pd.DataFrame(nn_e1).transpose().to_html())\n",
    "pd.DataFrame(nn_e1).transpose()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# AFTER the model has been run, ie. trained\n",
    "# display weights\n",
    "nn_w1 = sess1.run(weights1)    \n",
    "# HTML(pd.DataFrame(nn_w1).to_html())\n",
    "pd.DataFrame(nn_w1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# AFTER the model has been run, ie. trained\n",
    "nn_e2 = sess1.run(biases2)    \n",
    "# HTML(pd.DataFrame(nn_e2).transpose().to_html())\n",
    "pd.DataFrame(nn_e2).transpose()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# AFTER the model has been run, ie. trained\n",
    "# display weights\n",
    "nn_w2 = sess1.run(weights2)    \n",
    "# HTML(pd.DataFrame(nn_w2).to_html())\n",
    "pd.DataFrame(nn_w2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# AFTER the model has been run, ie. trained\n",
    "nn_e3 = sess1.run(biases3)    \n",
    "# HTML(pd.DataFrame(nn_e3).transpose().to_html())\n",
    "pd.DataFrame(nn_e3).transpose()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# AFTER the model has been run, ie. trained\n",
    "# display weights\n",
    "nn_w3 = sess1.run(weights3)    \n",
    "# HTML(pd.DataFrame(nn_w3).to_html())\n",
    "pd.DataFrame(nn_w3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # AFTER the model has been run, ie. trained\n",
    "# nn_hl3 = sess1.run(hidden_layer_3)    \n",
    "# print nn_hl3\n",
    "# # HTML(pd.DataFrame(nn_hl3).to_html())\n",
    "# pd.DataFrame(nn_hl3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You've covered a lot of ground. You moved from sourcing five years of financial time-series data, to munging that data into a more suitable form. You explored and visualized that data with exploratory data analysis and then decided on a machine learning model and the features for that model. You engineered those features, built a binary classifier in TensorFlow, and analyzed its performance. You built a feed forward neural net with two hidden layers in TensorFlow and analyzed its performance.\n",
    "\n",
    "How did the technology fare? It should take most people 1.5 to 3 hours to extract the juice from this solution, and none of that time is spent waiting for infrastructure or software; it's spent reading and thinking. In many organizations, it can take anywhere from days to months to do this sort of data analysis, depending on whether you need to procure any hardware. And you didn't need to do anything with infrastructure or additional software. Rather, you used a web-based console to direct GCP to set up systems on your behalf, which it did—fully managed, maintained, and supported—freeing you up to spend your time analyzing. \n",
    "\n",
    "It was also cost effective. If you took your time with this solution and spent three hours to go through it, the cost would be a few pennies. \n",
    "\n",
    "Cloud Datalab worked admirably, too. iPython/Jupyter has always been a great platform for interactive, iterative work and a fully-managed version of that platform on GCP, with connectors to other GCP technologies such as BigQuery and Google Cloud Storage, is a force multiplier for your analysis needs.  If you haven't used iPython before, this solution might have been eye opening, for you. If you're already familiar with iPython, then you'll love the connectors to other GCP technologies.\n",
    "\n",
    "Of course, R and Matlab are popular tools in machine learning, and we've made no mention either in this solution. Neither R nor Matlab are available as managed services on GCP. Both can be hosted in GCP and accessed through a cloud-friendly, web frontend.\n",
    "\n",
    "TensorFlow is a special piece of technology. It is expressive, performs well, and comes with the weight of Google's machine learning history and expertise to back it up and support it. We've only scratched the surface, but you can already see that within a handful of lines of code we've been able to write two models. Neither of them is cutting edge, by design, but neither of them is trivial either. With some additional tuning they would suit a whole spectrum of machine learning tasks. \n",
    "\n",
    "Finally, how did we do with the data analysis? We did well: over 70% accuracy in predicting the close of the S&P 500 is the highest we've seen achieved on this dataset, so with few steps and a few lines of code we've produced a full-on machine learning model. The reason for the relatively modest accuracy achieved is the dataset itself; there isn't enough signal there to do significantly better. But 7 times out of 10, we were able to correctly determine if the S&P 500 index would close up or down on the day, and that's objectively good."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**When you're finished, shut down the managed VM you used for Cloud Datalab to avoid incurring costs.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
