{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning (ML): Google Cloud Platform (GCP): TensorFlow (TF): Financial Time-Series\n",
    "# Model 01 = Binary Classifier\n",
    "# Model 02 = Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import StringIO\n",
    "\n",
    "import pandas as pd\n",
    "from pandas.tools.plotting import autocorrelation_plot\n",
    "from pandas.tools.plotting import scatter_matrix\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import gcp\n",
    "import gcp.bigquery as bq\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from IPython.core.display import HTML\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Install stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\r\n",
      "  File \"/usr/local/bin/pip\", line 9, in <module>\r\n",
      "    load_entry_point('pip==1.5.6', 'console_scripts', 'pip')()\r\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/pkg_resources/__init__.py\", line 542, in load_entry_point\r\n",
      "    return get_distribution(dist).load_entry_point(group, name)\r\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/pkg_resources/__init__.py\", line 2569, in load_entry_point\r\n",
      "    return ep.load()\r\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/pkg_resources/__init__.py\", line 2229, in load\r\n",
      "    return self.resolve()\r\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/pkg_resources/__init__.py\", line 2235, in resolve\r\n",
      "    module = __import__(self.module_name, fromlist=['__name__'], level=0)\r\n",
      "  File \"/usr/lib/python2.7/dist-packages/pip/__init__.py\", line 74, in <module>\r\n",
      "    from pip.vcs import git, mercurial, subversion, bazaar  # noqa\r\n",
      "  File \"/usr/lib/python2.7/dist-packages/pip/vcs/mercurial.py\", line 9, in <module>\r\n",
      "    from pip.download import path_to_url\r\n",
      "  File \"/usr/lib/python2.7/dist-packages/pip/download.py\", line 25, in <module>\r\n",
      "    from requests.compat import IncompleteRead\r\n",
      "ImportError: cannot import name IncompleteRead\r\n"
     ]
    }
   ],
   "source": [
    "# Install xlrd - required for reading excel files\n",
    "# !pip install xlrd\n",
    "!pip install xlrd\n",
    "# -----------------------------------------------------------------------------------------------\n",
    "# Downloading/unpacking xlrd\n",
    "#   Downloading xlrd-1.0.0.tar.gz (2.6MB): 2.6MB downloaded\n",
    "#   Running setup.py (path:/tmp/pip-build-brL6Op/xlrd/setup.py) egg_info for package xlrd\n",
    "    \n",
    "#     warning: no files found matching 'README.html'\n",
    "# Installing collected packages: xlrd\n",
    "#   Running setup.py install for xlrd\n",
    "#     changing mode of build/scripts-2.7/runxlrd.py from 644 to 755\n",
    "    \n",
    "#     warning: no files found matching 'README.html'\n",
    "#     changing mode of /usr/local/bin/runxlrd.py to 755\n",
    "#   Could not find .egg-info directory in install record for xlrd\n",
    "# Successfully installed xlrd\n",
    "# Cleaning up...\n",
    "# -----------------------------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\r\n",
      "  File \"/usr/local/bin/pip\", line 9, in <module>\r\n",
      "    load_entry_point('pip==1.5.6', 'console_scripts', 'pip')()\r\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/pkg_resources/__init__.py\", line 542, in load_entry_point\r\n",
      "    return get_distribution(dist).load_entry_point(group, name)\r\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/pkg_resources/__init__.py\", line 2569, in load_entry_point\r\n",
      "    return ep.load()\r\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/pkg_resources/__init__.py\", line 2229, in load\r\n",
      "    return self.resolve()\r\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/pkg_resources/__init__.py\", line 2235, in resolve\r\n",
      "    module = __import__(self.module_name, fromlist=['__name__'], level=0)\r\n",
      "  File \"/usr/lib/python2.7/dist-packages/pip/__init__.py\", line 74, in <module>\r\n",
      "    from pip.vcs import git, mercurial, subversion, bazaar  # noqa\r\n",
      "  File \"/usr/lib/python2.7/dist-packages/pip/vcs/mercurial.py\", line 9, in <module>\r\n",
      "    from pip.download import path_to_url\r\n",
      "  File \"/usr/lib/python2.7/dist-packages/pip/download.py\", line 25, in <module>\r\n",
      "    from requests.compat import IncompleteRead\r\n",
      "ImportError: cannot import name IncompleteRead\r\n"
     ]
    }
   ],
   "source": [
    "# Install dropbox - required for writing files to dropbox, not required for reading files from dropbox\n",
    "!pip install dropbox\n",
    "# -----------------------------------------------------------------------------------------------\n",
    "# Downloading/unpacking dropbox\n",
    "#   Downloading dropbox-6.6.2-py2-none-any.whl (261kB): 261kB downloaded\n",
    "# Requirement already satisfied (use --upgrade to upgrade): six>=1.3.0 in /usr/local/lib/python2.7/dist-packages (from dropbox)\n",
    "# Requirement already satisfied (use --upgrade to upgrade): urllib3 in /usr/lib/python2.7/dist-packages (from dropbox)\n",
    "# Downloading/unpacking requests!=2.6.1,>=2.5.1 (from dropbox)\n",
    "#   Downloading requests-2.11.0-py2.py3-none-any.whl (514kB): 514kB downloaded\n",
    "# Downloading/unpacking typing>=3.5.2 (from dropbox)\n",
    "#   Downloading typing-3.5.2.2.tar.gz (51kB): 51kB downloaded\n",
    "#   Running setup.py (path:/tmp/pip-build-xSBpJ2/typing/setup.py) egg_info for package typing\n",
    "    \n",
    "# Installing collected packages: dropbox, requests, typing\n",
    "#   Found existing installation: requests 2.4.3\n",
    "#     Not uninstalling requests at /usr/lib/python2.7/dist-packages, owned by OS\n",
    "#   Running setup.py install for typing\n",
    "    \n",
    "#   Could not find .egg-info directory in install record for typing>=3.5.2 (from dropbox)\n",
    "# Successfully installed dropbox requests typing\n",
    "# Cleaning up...\n",
    "# -----------------------------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Get the data from dropbox excel file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save file on dropbox.com\n",
    "\n",
    "fx_mxn_url_data_raw = 'https://www.dropbox.com/s/foso5ckzl9vrui0/fx_mxn_data_raw.xlsm'\n",
    "\n",
    "# force = 1 # will grab from dropbox to gcp\n",
    "force = 0 # will use if already in gcp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fx_mxn_bn_data_raw=fx_mxn_data_raw.xlsm\n",
      "already_downloaded=True\n"
     ]
    }
   ],
   "source": [
    "# Import file using url to file on dropbox\n",
    "from urlparse import urlparse\n",
    "from os.path import basename\n",
    "\n",
    "fx_mxn_bn_data_raw = basename(urlparse(fx_mxn_url_data_raw).path)\n",
    "print 'fx_mxn_bn_data_raw=' + fx_mxn_bn_data_raw\n",
    "print 'already_downloaded=' + str(already_downloaded)\n",
    "\n",
    "try:\n",
    "    already_downloaded\n",
    "except:\n",
    "    already_downloaded = False\n",
    "    \n",
    "if force or not already_downloaded:\n",
    "    already_downloaded = True\n",
    "    !rm $fx_mxn_bn_data_raw\n",
    "    !wget $fx_mxn_bn_data_raw\n",
    "#!ls -l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Read the Excel file raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>77</th>\n",
       "      <th>78</th>\n",
       "      <th>79</th>\n",
       "      <th>80</th>\n",
       "      <th>81</th>\n",
       "      <th>82</th>\n",
       "      <th>83</th>\n",
       "      <th>84</th>\n",
       "      <th>85</th>\n",
       "      <th>86</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ticker</td>\n",
       "      <td>USDMXN Curncy</td>\n",
       "      <td>ASURB MM Equity</td>\n",
       "      <td>BIMBOA MM Equity</td>\n",
       "      <td>CN1 Comdty</td>\n",
       "      <td>CT350188 Curncy</td>\n",
       "      <td>DU1 Comdty</td>\n",
       "      <td>ED749713@BVAL Corp</td>\n",
       "      <td>EMRUEMRU Index</td>\n",
       "      <td>EURMXN3Y Curncy</td>\n",
       "      <td>...</td>\n",
       "      <td>USSA25 ICPL Curncy</td>\n",
       "      <td>USSA30 ICPL Curncy</td>\n",
       "      <td>USSP15 CMPN Curncy</td>\n",
       "      <td>USSP20 CMPN Curncy</td>\n",
       "      <td>USSP25 CMPN Curncy</td>\n",
       "      <td>USSW15 CMPN Curncy</td>\n",
       "      <td>USSW20 CMPN Curncy</td>\n",
       "      <td>USSW25 CMPN Curncy</td>\n",
       "      <td>USSW30 CMPN Curncy</td>\n",
       "      <td>XQ1 Comdty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Date</td>\n",
       "      <td>USDMXN Curncy</td>\n",
       "      <td>ASURB MM Equity</td>\n",
       "      <td>BIMBOA MM Equity</td>\n",
       "      <td>CN1 Comdty</td>\n",
       "      <td>CT350188 Curncy</td>\n",
       "      <td>DU1 Comdty</td>\n",
       "      <td>ED749713@BVAL Corp</td>\n",
       "      <td>EMRUEMRU Index</td>\n",
       "      <td>EURMXN3Y Curncy</td>\n",
       "      <td>...</td>\n",
       "      <td>USSA25 ICPL Curncy</td>\n",
       "      <td>USSA30 ICPL Curncy</td>\n",
       "      <td>USSP15 CMPN Curncy</td>\n",
       "      <td>USSP20 CMPN Curncy</td>\n",
       "      <td>USSP25 CMPN Curncy</td>\n",
       "      <td>USSW15 CMPN Curncy</td>\n",
       "      <td>USSW20 CMPN Curncy</td>\n",
       "      <td>USSW25 CMPN Curncy</td>\n",
       "      <td>USSW30 CMPN Curncy</td>\n",
       "      <td>XQ1 Comdty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016-05-27 00:00:00</td>\n",
       "      <td>18.4751</td>\n",
       "      <td>289.95</td>\n",
       "      <td>55.4</td>\n",
       "      <td>141.28</td>\n",
       "      <td>424.14</td>\n",
       "      <td>111.85</td>\n",
       "      <td>118.813</td>\n",
       "      <td>66.0342</td>\n",
       "      <td>35645.2</td>\n",
       "      <td>...</td>\n",
       "      <td>2.102</td>\n",
       "      <td>2.138</td>\n",
       "      <td>-10.63</td>\n",
       "      <td>-18</td>\n",
       "      <td>-31.63</td>\n",
       "      <td>1.943</td>\n",
       "      <td>2.074</td>\n",
       "      <td>2.133</td>\n",
       "      <td>2.1738</td>\n",
       "      <td>124.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016-05-26 00:00:00</td>\n",
       "      <td>18.4544</td>\n",
       "      <td>284.68</td>\n",
       "      <td>55.29</td>\n",
       "      <td>141.83</td>\n",
       "      <td>427.999</td>\n",
       "      <td>111.845</td>\n",
       "      <td>118.75</td>\n",
       "      <td>65.282</td>\n",
       "      <td>35316.9</td>\n",
       "      <td>...</td>\n",
       "      <td>2.104</td>\n",
       "      <td>2.139</td>\n",
       "      <td>-10.46</td>\n",
       "      <td>-17.65</td>\n",
       "      <td>-31.43</td>\n",
       "      <td>1.928</td>\n",
       "      <td>2.0575</td>\n",
       "      <td>2.125</td>\n",
       "      <td>2.1598</td>\n",
       "      <td>124.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016-05-25 00:00:00</td>\n",
       "      <td>18.4852</td>\n",
       "      <td>281.4</td>\n",
       "      <td>56.33</td>\n",
       "      <td>140.98</td>\n",
       "      <td>5392.86</td>\n",
       "      <td>111.825</td>\n",
       "      <td>118.438</td>\n",
       "      <td>65.9242</td>\n",
       "      <td>35680.2</td>\n",
       "      <td>...</td>\n",
       "      <td>2.107</td>\n",
       "      <td>2.14</td>\n",
       "      <td>-11.11</td>\n",
       "      <td>-18.4</td>\n",
       "      <td>-32.03</td>\n",
       "      <td>1.955</td>\n",
       "      <td>2.0825</td>\n",
       "      <td>2.1453</td>\n",
       "      <td>2.1785</td>\n",
       "      <td>124.11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 87 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    0              1                2                 3   \\\n",
       "0               Ticker  USDMXN Curncy  ASURB MM Equity  BIMBOA MM Equity   \n",
       "1                 Date  USDMXN Curncy  ASURB MM Equity  BIMBOA MM Equity   \n",
       "2  2016-05-27 00:00:00        18.4751           289.95              55.4   \n",
       "3  2016-05-26 00:00:00        18.4544           284.68             55.29   \n",
       "4  2016-05-25 00:00:00        18.4852            281.4             56.33   \n",
       "\n",
       "           4                5           6                   7   \\\n",
       "0  CN1 Comdty  CT350188 Curncy  DU1 Comdty  ED749713@BVAL Corp   \n",
       "1  CN1 Comdty  CT350188 Curncy  DU1 Comdty  ED749713@BVAL Corp   \n",
       "2      141.28           424.14      111.85             118.813   \n",
       "3      141.83          427.999     111.845              118.75   \n",
       "4      140.98          5392.86     111.825             118.438   \n",
       "\n",
       "               8                9      ...                      77  \\\n",
       "0  EMRUEMRU Index  EURMXN3Y Curncy     ...      USSA25 ICPL Curncy   \n",
       "1  EMRUEMRU Index  EURMXN3Y Curncy     ...      USSA25 ICPL Curncy   \n",
       "2         66.0342          35645.2     ...                   2.102   \n",
       "3          65.282          35316.9     ...                   2.104   \n",
       "4         65.9242          35680.2     ...                   2.107   \n",
       "\n",
       "                   78                  79                  80  \\\n",
       "0  USSA30 ICPL Curncy  USSP15 CMPN Curncy  USSP20 CMPN Curncy   \n",
       "1  USSA30 ICPL Curncy  USSP15 CMPN Curncy  USSP20 CMPN Curncy   \n",
       "2               2.138              -10.63                 -18   \n",
       "3               2.139              -10.46              -17.65   \n",
       "4                2.14              -11.11               -18.4   \n",
       "\n",
       "                   81                  82                  83  \\\n",
       "0  USSP25 CMPN Curncy  USSW15 CMPN Curncy  USSW20 CMPN Curncy   \n",
       "1  USSP25 CMPN Curncy  USSW15 CMPN Curncy  USSW20 CMPN Curncy   \n",
       "2              -31.63               1.943               2.074   \n",
       "3              -31.43               1.928              2.0575   \n",
       "4              -32.03               1.955              2.0825   \n",
       "\n",
       "                   84                  85          86  \n",
       "0  USSW25 CMPN Curncy  USSW30 CMPN Curncy  XQ1 Comdty  \n",
       "1  USSW25 CMPN Curncy  USSW30 CMPN Curncy  XQ1 Comdty  \n",
       "2               2.133              2.1738      124.13  \n",
       "3               2.125              2.1598       124.3  \n",
       "4              2.1453              2.1785      124.11  \n",
       "\n",
       "[5 rows x 87 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import xlrd\n",
    "\n",
    "# Read the excel data\n",
    "fx_mxn_df_data_raw = pd.read_excel(fx_mxn_bn_data_raw, 'data_raw')\n",
    "# data = pd.read_excel(bn_data_raw, 'data_raw', header=0, index_col=0, parse_cols=None)\n",
    "  # parse_cols=None: parse all columns\n",
    "  # header=0:    sets the row 0 as col labels (ie. headers)\n",
    "  # index_col=0: sets the col 0 as row labels (ie. index)\n",
    "fx_mxn_df_data_raw.head() # display the first few lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "print type(fx_mxn_df_data_raw)\n",
    "# <class 'pandas.core.frame.DataFrame'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1) Write the Excel file back to dropbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Include the Dropbox SDK\n",
    "import dropbox"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1.1) Method 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Get your app key and secret from the Dropbox developer website\n",
    "# app_key = '9zgzo4d4s8tco7c'\n",
    "# app_secret = '5iq9409uedmichl'\n",
    "\n",
    "# flow = dropbox.client.DropboxOAuth2FlowNoRedirect(app_key, app_secret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# authorize_url = flow.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print '1. Go to: ' + authorize_url\n",
    "# print '2. Click \"Allow\" (you might have to log in first)'\n",
    "# print '3. Copy the authorization code.'\n",
    "# code = raw_input(\"Enter the authorization code here: \").strip()\n",
    "\n",
    "# # output\n",
    "# # 1. Go to: https://www.dropbox.com/1/oauth2/authorize?response_type=code&client_id=9zgzo4d4s8tco7c\n",
    "# # 2. Click \"Allow\" (you might have to log in first)\n",
    "# # 3. Copy the authorization code.\n",
    "# # Enter the authorization code here: Z0hHmWIPD4AAAAAAAAAAE2kOeZ0UPKiQi9eeQFEyY1s\n",
    "# # Enter the authorization code here: Z0hHmWIPD4AAAAAAAAAAFTmf6b0e8U4eOd96tCcbILs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This will fail if the user enters an invalid authorization code\n",
    "# access_token, user_id = flow.finish(code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# client = dropbox.client.DropboxClient(access_token)\n",
    "# print 'linked account: ', client.account_info()\n",
    "\n",
    "# # linked account:  {u'referral_link': u'https://db.tt/wTxYqge4', u'display_name': u'Eyup Saltik', u'uid': 576395988, \n",
    "# # u'locale': u'en', u'email_verified': True, u'email': u'eyup.saltik.2016@gmail.com', u'is_paired': False, \n",
    "# # u'team': None, u'name_details': {u'familiar_name': u'Eyup', u'surname': u'Saltik', u'given_name': u'Eyup'}, \n",
    "# # u'country': u'SG', u'quota_info': {u'datastores': 0, u'shared': 0, u'quota': 2147483648, u'normal': 40119203}}\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# response = client.put_file('/working-final.csv', str(data_raw))\n",
    "# print \"uploaded:\", response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1.2) Method 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:2: DeprecationWarning: You are using a deprecated client. Please use the new v2 client located at dropbox.Dropbox.\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linked account:  {u'referral_link': u'https://db.tt/wTxYqge4', u'display_name': u'Eyup Saltik', u'uid': 576395988, u'locale': u'en', u'email_verified': True, u'email': u'eyup.saltik.2016@gmail.com', u'is_paired': False, u'team': None, u'name_details': {u'familiar_name': u'Eyup', u'surname': u'Saltik', u'given_name': u'Eyup'}, u'country': u'SG', u'quota_info': {u'datastores': 0, u'shared': 0, u'quota': 2147483648, u'normal': 7193548}}\n"
     ]
    }
   ],
   "source": [
    "s_dropbox_access_token = 'Z0hHmWIPD4AAAAAAAAAADKBYofFVGTUnVcva8HKUpKCj6SDaY15WNHIZcRcEknoO'\n",
    "client = dropbox.client.DropboxClient(s_dropbox_access_token)\n",
    "print 'linked account: ', client.account_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uploaded: {u'icon': u'page_white_excel', u'bytes': 2672933, u'thumb_exists': False, u'rev': u'424a1e9d47', u'modified': u'Tue, 23 Aug 2016 06:52:27 +0000', u'shareable': False, u'client_mtime': u'Tue, 23 Aug 2016 06:52:27 +0000', u'path': u'/fx_mxn_data_raw_from_GCP.csv', u'is_dir': False, u'size': u'2.5 MB', u'root': u'dropbox', u'mime_type': u'text/csv', u'revision': 66}\n"
     ]
    }
   ],
   "source": [
    "response = client.put_file('/fx_mxn_data_raw_from_GCP.csv', str(fx_mxn_df_data_raw.to_csv()), overwrite=True)\n",
    "print \"uploaded:\", response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Select data df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>77</th>\n",
       "      <th>78</th>\n",
       "      <th>79</th>\n",
       "      <th>80</th>\n",
       "      <th>81</th>\n",
       "      <th>82</th>\n",
       "      <th>83</th>\n",
       "      <th>84</th>\n",
       "      <th>85</th>\n",
       "      <th>86</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ticker</td>\n",
       "      <td>USDMXN Curncy</td>\n",
       "      <td>ASURB MM Equity</td>\n",
       "      <td>BIMBOA MM Equity</td>\n",
       "      <td>CN1 Comdty</td>\n",
       "      <td>CT350188 Curncy</td>\n",
       "      <td>DU1 Comdty</td>\n",
       "      <td>ED749713@BVAL Corp</td>\n",
       "      <td>EMRUEMRU Index</td>\n",
       "      <td>EURMXN3Y Curncy</td>\n",
       "      <td>...</td>\n",
       "      <td>USSA25 ICPL Curncy</td>\n",
       "      <td>USSA30 ICPL Curncy</td>\n",
       "      <td>USSP15 CMPN Curncy</td>\n",
       "      <td>USSP20 CMPN Curncy</td>\n",
       "      <td>USSP25 CMPN Curncy</td>\n",
       "      <td>USSW15 CMPN Curncy</td>\n",
       "      <td>USSW20 CMPN Curncy</td>\n",
       "      <td>USSW25 CMPN Curncy</td>\n",
       "      <td>USSW30 CMPN Curncy</td>\n",
       "      <td>XQ1 Comdty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Date</td>\n",
       "      <td>USDMXN Curncy</td>\n",
       "      <td>ASURB MM Equity</td>\n",
       "      <td>BIMBOA MM Equity</td>\n",
       "      <td>CN1 Comdty</td>\n",
       "      <td>CT350188 Curncy</td>\n",
       "      <td>DU1 Comdty</td>\n",
       "      <td>ED749713@BVAL Corp</td>\n",
       "      <td>EMRUEMRU Index</td>\n",
       "      <td>EURMXN3Y Curncy</td>\n",
       "      <td>...</td>\n",
       "      <td>USSA25 ICPL Curncy</td>\n",
       "      <td>USSA30 ICPL Curncy</td>\n",
       "      <td>USSP15 CMPN Curncy</td>\n",
       "      <td>USSP20 CMPN Curncy</td>\n",
       "      <td>USSP25 CMPN Curncy</td>\n",
       "      <td>USSW15 CMPN Curncy</td>\n",
       "      <td>USSW20 CMPN Curncy</td>\n",
       "      <td>USSW25 CMPN Curncy</td>\n",
       "      <td>USSW30 CMPN Curncy</td>\n",
       "      <td>XQ1 Comdty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016-05-27 00:00:00</td>\n",
       "      <td>18.4751</td>\n",
       "      <td>289.95</td>\n",
       "      <td>55.4</td>\n",
       "      <td>141.28</td>\n",
       "      <td>424.14</td>\n",
       "      <td>111.85</td>\n",
       "      <td>118.813</td>\n",
       "      <td>66.0342</td>\n",
       "      <td>35645.2</td>\n",
       "      <td>...</td>\n",
       "      <td>2.102</td>\n",
       "      <td>2.138</td>\n",
       "      <td>-10.63</td>\n",
       "      <td>-18</td>\n",
       "      <td>-31.63</td>\n",
       "      <td>1.943</td>\n",
       "      <td>2.074</td>\n",
       "      <td>2.133</td>\n",
       "      <td>2.1738</td>\n",
       "      <td>124.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016-05-26 00:00:00</td>\n",
       "      <td>18.4544</td>\n",
       "      <td>284.68</td>\n",
       "      <td>55.29</td>\n",
       "      <td>141.83</td>\n",
       "      <td>427.999</td>\n",
       "      <td>111.845</td>\n",
       "      <td>118.75</td>\n",
       "      <td>65.282</td>\n",
       "      <td>35316.9</td>\n",
       "      <td>...</td>\n",
       "      <td>2.104</td>\n",
       "      <td>2.139</td>\n",
       "      <td>-10.46</td>\n",
       "      <td>-17.65</td>\n",
       "      <td>-31.43</td>\n",
       "      <td>1.928</td>\n",
       "      <td>2.0575</td>\n",
       "      <td>2.125</td>\n",
       "      <td>2.1598</td>\n",
       "      <td>124.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016-05-25 00:00:00</td>\n",
       "      <td>18.4852</td>\n",
       "      <td>281.4</td>\n",
       "      <td>56.33</td>\n",
       "      <td>140.98</td>\n",
       "      <td>5392.86</td>\n",
       "      <td>111.825</td>\n",
       "      <td>118.438</td>\n",
       "      <td>65.9242</td>\n",
       "      <td>35680.2</td>\n",
       "      <td>...</td>\n",
       "      <td>2.107</td>\n",
       "      <td>2.14</td>\n",
       "      <td>-11.11</td>\n",
       "      <td>-18.4</td>\n",
       "      <td>-32.03</td>\n",
       "      <td>1.955</td>\n",
       "      <td>2.0825</td>\n",
       "      <td>2.1453</td>\n",
       "      <td>2.1785</td>\n",
       "      <td>124.11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 87 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    0              1                2                 3   \\\n",
       "0               Ticker  USDMXN Curncy  ASURB MM Equity  BIMBOA MM Equity   \n",
       "1                 Date  USDMXN Curncy  ASURB MM Equity  BIMBOA MM Equity   \n",
       "2  2016-05-27 00:00:00        18.4751           289.95              55.4   \n",
       "3  2016-05-26 00:00:00        18.4544           284.68             55.29   \n",
       "4  2016-05-25 00:00:00        18.4852            281.4             56.33   \n",
       "\n",
       "           4                5           6                   7   \\\n",
       "0  CN1 Comdty  CT350188 Curncy  DU1 Comdty  ED749713@BVAL Corp   \n",
       "1  CN1 Comdty  CT350188 Curncy  DU1 Comdty  ED749713@BVAL Corp   \n",
       "2      141.28           424.14      111.85             118.813   \n",
       "3      141.83          427.999     111.845              118.75   \n",
       "4      140.98          5392.86     111.825             118.438   \n",
       "\n",
       "               8                9      ...                      77  \\\n",
       "0  EMRUEMRU Index  EURMXN3Y Curncy     ...      USSA25 ICPL Curncy   \n",
       "1  EMRUEMRU Index  EURMXN3Y Curncy     ...      USSA25 ICPL Curncy   \n",
       "2         66.0342          35645.2     ...                   2.102   \n",
       "3          65.282          35316.9     ...                   2.104   \n",
       "4         65.9242          35680.2     ...                   2.107   \n",
       "\n",
       "                   78                  79                  80  \\\n",
       "0  USSA30 ICPL Curncy  USSP15 CMPN Curncy  USSP20 CMPN Curncy   \n",
       "1  USSA30 ICPL Curncy  USSP15 CMPN Curncy  USSP20 CMPN Curncy   \n",
       "2               2.138              -10.63                 -18   \n",
       "3               2.139              -10.46              -17.65   \n",
       "4                2.14              -11.11               -18.4   \n",
       "\n",
       "                   81                  82                  83  \\\n",
       "0  USSP25 CMPN Curncy  USSW15 CMPN Curncy  USSW20 CMPN Curncy   \n",
       "1  USSP25 CMPN Curncy  USSW15 CMPN Curncy  USSW20 CMPN Curncy   \n",
       "2              -31.63               1.943               2.074   \n",
       "3              -31.43               1.928              2.0575   \n",
       "4              -32.03               1.955              2.0825   \n",
       "\n",
       "                   84                  85          86  \n",
       "0  USSW25 CMPN Curncy  USSW30 CMPN Curncy  XQ1 Comdty  \n",
       "1  USSW25 CMPN Curncy  USSW30 CMPN Curncy  XQ1 Comdty  \n",
       "2               2.133              2.1738      124.13  \n",
       "3               2.125              2.1598       124.3  \n",
       "4              2.1453              2.1785      124.11  \n",
       "\n",
       "[5 rows x 87 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = fx_mxn_df_data_raw\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6) df_ticker_desc = Just a table to map column id to the ticker and name of the variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>77</th>\n",
       "      <th>78</th>\n",
       "      <th>79</th>\n",
       "      <th>80</th>\n",
       "      <th>81</th>\n",
       "      <th>82</th>\n",
       "      <th>83</th>\n",
       "      <th>84</th>\n",
       "      <th>85</th>\n",
       "      <th>86</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ticker</td>\n",
       "      <td>USDMXN Curncy</td>\n",
       "      <td>ASURB MM Equity</td>\n",
       "      <td>BIMBOA MM Equity</td>\n",
       "      <td>CN1 Comdty</td>\n",
       "      <td>CT350188 Curncy</td>\n",
       "      <td>DU1 Comdty</td>\n",
       "      <td>ED749713@BVAL Corp</td>\n",
       "      <td>EMRUEMRU Index</td>\n",
       "      <td>EURMXN3Y Curncy</td>\n",
       "      <td>...</td>\n",
       "      <td>USSA25 ICPL Curncy</td>\n",
       "      <td>USSA30 ICPL Curncy</td>\n",
       "      <td>USSP15 CMPN Curncy</td>\n",
       "      <td>USSP20 CMPN Curncy</td>\n",
       "      <td>USSP25 CMPN Curncy</td>\n",
       "      <td>USSW15 CMPN Curncy</td>\n",
       "      <td>USSW20 CMPN Curncy</td>\n",
       "      <td>USSW25 CMPN Curncy</td>\n",
       "      <td>USSW30 CMPN Curncy</td>\n",
       "      <td>XQ1 Comdty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Date</td>\n",
       "      <td>USDMXN Curncy</td>\n",
       "      <td>ASURB MM Equity</td>\n",
       "      <td>BIMBOA MM Equity</td>\n",
       "      <td>CN1 Comdty</td>\n",
       "      <td>CT350188 Curncy</td>\n",
       "      <td>DU1 Comdty</td>\n",
       "      <td>ED749713@BVAL Corp</td>\n",
       "      <td>EMRUEMRU Index</td>\n",
       "      <td>EURMXN3Y Curncy</td>\n",
       "      <td>...</td>\n",
       "      <td>USSA25 ICPL Curncy</td>\n",
       "      <td>USSA30 ICPL Curncy</td>\n",
       "      <td>USSP15 CMPN Curncy</td>\n",
       "      <td>USSP20 CMPN Curncy</td>\n",
       "      <td>USSP25 CMPN Curncy</td>\n",
       "      <td>USSW15 CMPN Curncy</td>\n",
       "      <td>USSW20 CMPN Curncy</td>\n",
       "      <td>USSW25 CMPN Curncy</td>\n",
       "      <td>USSW30 CMPN Curncy</td>\n",
       "      <td>XQ1 Comdty</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 87 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0              1                2                 3           4   \\\n",
       "0  Ticker  USDMXN Curncy  ASURB MM Equity  BIMBOA MM Equity  CN1 Comdty   \n",
       "1    Date  USDMXN Curncy  ASURB MM Equity  BIMBOA MM Equity  CN1 Comdty   \n",
       "\n",
       "                5           6                   7               8   \\\n",
       "0  CT350188 Curncy  DU1 Comdty  ED749713@BVAL Corp  EMRUEMRU Index   \n",
       "1  CT350188 Curncy  DU1 Comdty  ED749713@BVAL Corp  EMRUEMRU Index   \n",
       "\n",
       "                9      ...                      77                  78  \\\n",
       "0  EURMXN3Y Curncy     ...      USSA25 ICPL Curncy  USSA30 ICPL Curncy   \n",
       "1  EURMXN3Y Curncy     ...      USSA25 ICPL Curncy  USSA30 ICPL Curncy   \n",
       "\n",
       "                   79                  80                  81  \\\n",
       "0  USSP15 CMPN Curncy  USSP20 CMPN Curncy  USSP25 CMPN Curncy   \n",
       "1  USSP15 CMPN Curncy  USSP20 CMPN Curncy  USSP25 CMPN Curncy   \n",
       "\n",
       "                   82                  83                  84  \\\n",
       "0  USSW15 CMPN Curncy  USSW20 CMPN Curncy  USSW25 CMPN Curncy   \n",
       "1  USSW15 CMPN Curncy  USSW20 CMPN Curncy  USSW25 CMPN Curncy   \n",
       "\n",
       "                   85          86  \n",
       "0  USSW30 CMPN Curncy  XQ1 Comdty  \n",
       "1  USSW30 CMPN Curncy  XQ1 Comdty  \n",
       "\n",
       "[2 rows x 87 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ticker_desc = df.ix[:1] # 1st 2 rows\n",
    "df_ticker_desc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7) df_values = the actual values of the 86 variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>77</th>\n",
       "      <th>78</th>\n",
       "      <th>79</th>\n",
       "      <th>80</th>\n",
       "      <th>81</th>\n",
       "      <th>82</th>\n",
       "      <th>83</th>\n",
       "      <th>84</th>\n",
       "      <th>85</th>\n",
       "      <th>86</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016-05-27 00:00:00</td>\n",
       "      <td>18.4751</td>\n",
       "      <td>289.95</td>\n",
       "      <td>55.4</td>\n",
       "      <td>141.28</td>\n",
       "      <td>424.14</td>\n",
       "      <td>111.85</td>\n",
       "      <td>118.813</td>\n",
       "      <td>66.0342</td>\n",
       "      <td>35645.2</td>\n",
       "      <td>...</td>\n",
       "      <td>2.102</td>\n",
       "      <td>2.138</td>\n",
       "      <td>-10.63</td>\n",
       "      <td>-18</td>\n",
       "      <td>-31.63</td>\n",
       "      <td>1.943</td>\n",
       "      <td>2.074</td>\n",
       "      <td>2.133</td>\n",
       "      <td>2.1738</td>\n",
       "      <td>124.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016-05-26 00:00:00</td>\n",
       "      <td>18.4544</td>\n",
       "      <td>284.68</td>\n",
       "      <td>55.29</td>\n",
       "      <td>141.83</td>\n",
       "      <td>427.999</td>\n",
       "      <td>111.845</td>\n",
       "      <td>118.75</td>\n",
       "      <td>65.282</td>\n",
       "      <td>35316.9</td>\n",
       "      <td>...</td>\n",
       "      <td>2.104</td>\n",
       "      <td>2.139</td>\n",
       "      <td>-10.46</td>\n",
       "      <td>-17.65</td>\n",
       "      <td>-31.43</td>\n",
       "      <td>1.928</td>\n",
       "      <td>2.0575</td>\n",
       "      <td>2.125</td>\n",
       "      <td>2.1598</td>\n",
       "      <td>124.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016-05-25 00:00:00</td>\n",
       "      <td>18.4852</td>\n",
       "      <td>281.4</td>\n",
       "      <td>56.33</td>\n",
       "      <td>140.98</td>\n",
       "      <td>5392.86</td>\n",
       "      <td>111.825</td>\n",
       "      <td>118.438</td>\n",
       "      <td>65.9242</td>\n",
       "      <td>35680.2</td>\n",
       "      <td>...</td>\n",
       "      <td>2.107</td>\n",
       "      <td>2.14</td>\n",
       "      <td>-11.11</td>\n",
       "      <td>-18.4</td>\n",
       "      <td>-32.03</td>\n",
       "      <td>1.955</td>\n",
       "      <td>2.0825</td>\n",
       "      <td>2.1453</td>\n",
       "      <td>2.1785</td>\n",
       "      <td>124.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2016-05-24 00:00:00</td>\n",
       "      <td>18.487</td>\n",
       "      <td>282.81</td>\n",
       "      <td>56.06</td>\n",
       "      <td>141.16</td>\n",
       "      <td>5392.86</td>\n",
       "      <td>111.815</td>\n",
       "      <td>118.5</td>\n",
       "      <td>66.8658</td>\n",
       "      <td>36154.1</td>\n",
       "      <td>...</td>\n",
       "      <td>2.126</td>\n",
       "      <td>2.158</td>\n",
       "      <td>-11.18</td>\n",
       "      <td>-18.43</td>\n",
       "      <td>-31.94</td>\n",
       "      <td>1.9469</td>\n",
       "      <td>2.0703</td>\n",
       "      <td>2.1305</td>\n",
       "      <td>2.1615</td>\n",
       "      <td>124.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2016-05-23 00:00:00</td>\n",
       "      <td>18.5161</td>\n",
       "      <td>282.02</td>\n",
       "      <td>55.27</td>\n",
       "      <td>141.31</td>\n",
       "      <td>5392.86</td>\n",
       "      <td>111.815</td>\n",
       "      <td>120.188</td>\n",
       "      <td>67.0104</td>\n",
       "      <td>36684.3</td>\n",
       "      <td>...</td>\n",
       "      <td>2.092</td>\n",
       "      <td>2.125</td>\n",
       "      <td>-10.81</td>\n",
       "      <td>-18.14</td>\n",
       "      <td>-31.55</td>\n",
       "      <td>1.9245</td>\n",
       "      <td>2.0503</td>\n",
       "      <td>2.112</td>\n",
       "      <td>2.1435</td>\n",
       "      <td>124.27</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 87 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    0        1       2      3       4        5        6   \\\n",
       "2  2016-05-27 00:00:00  18.4751  289.95   55.4  141.28   424.14   111.85   \n",
       "3  2016-05-26 00:00:00  18.4544  284.68  55.29  141.83  427.999  111.845   \n",
       "4  2016-05-25 00:00:00  18.4852   281.4  56.33  140.98  5392.86  111.825   \n",
       "5  2016-05-24 00:00:00   18.487  282.81  56.06  141.16  5392.86  111.815   \n",
       "6  2016-05-23 00:00:00  18.5161  282.02  55.27  141.31  5392.86  111.815   \n",
       "\n",
       "        7        8        9    ...       77     78     79     80     81  \\\n",
       "2  118.813  66.0342  35645.2   ...    2.102  2.138 -10.63    -18 -31.63   \n",
       "3   118.75   65.282  35316.9   ...    2.104  2.139 -10.46 -17.65 -31.43   \n",
       "4  118.438  65.9242  35680.2   ...    2.107   2.14 -11.11  -18.4 -32.03   \n",
       "5    118.5  66.8658  36154.1   ...    2.126  2.158 -11.18 -18.43 -31.94   \n",
       "6  120.188  67.0104  36684.3   ...    2.092  2.125 -10.81 -18.14 -31.55   \n",
       "\n",
       "       82      83      84      85      86  \n",
       "2   1.943   2.074   2.133  2.1738  124.13  \n",
       "3   1.928  2.0575   2.125  2.1598   124.3  \n",
       "4   1.955  2.0825  2.1453  2.1785  124.11  \n",
       "5  1.9469  2.0703  2.1305  2.1615  124.24  \n",
       "6  1.9245  2.0503   2.112  2.1435  124.27  \n",
       "\n",
       "[5 rows x 87 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_values = fx_mxn_df_data_raw_pca.ix[2:] # From row 2 onwards\n",
    "df_values.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8) Set the dates as index of the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_values_indexed = data_values.set_index([0])\n",
    "data_values_indexed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_values_indexed.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9) Fill any gaps in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Pandas includes a very convenient function for filling gaps in the data.\n",
    "data_values_indexed = data_values_indexed.fillna(method='ffill')\n",
    "data_values_indexed.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory data analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploratory Data Analysis (EDA) is foundational to working with machine learning, and any other sort of analysis. EDA means getting to know your data, getting your fingers dirty with your data, feeling it and seeing it. The end result is you know your data very well, so when you build models you build them based on an actual, practical, physical understanding of the data, not assumptions or vaguely held notions. You can still make assumptions of course, but EDA means you will understand your assumptions and why you're making those assumptions. \n",
    "\n",
    "First, take a look at the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10) Describe the data briefly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_values_indexed.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that the various indices operate on scales differing by orders of magnitude. It's best to scale the data so that, for example, operations involving multiple indices aren't unduly influenced by a single, massive index.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11) Plot the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# N.B. A super-useful trick-ette is to assign the return value of plot to _ \n",
    "# so that you don't get text printed before the plot itself.\n",
    "\n",
    "# _ = pd.concat([data_values_indexed[1],\n",
    "#   data_values_indexed[1070],\n",
    "#   data_values_indexed[788],\n",
    "#   data_values_indexed[926]], axis=1).plot(figsize=(20, 15))\n",
    "\n",
    "_ = data_values_indexed.plot(figsize=(20, 15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the structure isn't uniformly visible for the indices. Divide each value in an individual index by the maximum value for that index., and then replot. The maximum value of all indices will be 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12) Calculate the max value for each column, prepare to scale data for each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_values_indexed_max = data_values_indexed.max(axis=0) # max across axis 0 = rows\n",
    "data_values_indexed_max.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13) Scale the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_values_indexed_scaled = data_values_indexed / data_values_indexed_max\n",
    "data_values_indexed_scaled.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14) Plot the scaled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "_ = data_values_indexed_scaled.plot(figsize=(20, 15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15) Auto-correlations\n",
    "\n",
    "Next, plot autocorrelations for each of the indices. The autocorrelations determine correlations between current values of the index and lagged values of the same index. The goal is to determine whether the lagged values are reliable indicators of the current values. If they are, then we've identified a correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "fig.set_figwidth(20)\n",
    "fig.set_figheight(10)\n",
    "\n",
    "_ = autocorrelation_plot(data_values_indexed_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For 2300 lagged days, we observe positive auto-correlations.\n",
    "This suggests that as the variables increase, they tend to keep on increasing. Momentum.\n",
    "\n",
    "After 2300 lagged days, we observe negative auto-correlations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16) Skip: Just a reminder of the PCA columns we selected earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# list_cols_pca = [ 0,1,\n",
    "#                   1070,788,926,112,69,574,654,1160,527,323,\n",
    "#                   397,118,774,1028,1034,655,907,736,251,388,\n",
    "#                   327,243,705,303,1146,467,136,1006,600,15,\n",
    "#                   231,290,131,782,20,1048,630,1173,431,856,\n",
    "#                   67,299,838,639,53,932,870,938,1061\n",
    "#                 ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 17) Scatter plots of the first 20 of our 86 variables vs USDMXN(varid=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# _ = scatter_matrix(data_values_indexed_scaled)\n",
    "dvis = data_values_indexed_scaled\n",
    "\n",
    "_ = scatter_matrix(\n",
    "      pd.concat(\n",
    "      [ \n",
    "        dvis[ 1],dvis[ 2],dvis[ 3],dvis[ 4],dvis[ 5],dvis[ 6],dvis[ 7],dvis[ 8],dvis[ 9],dvis[10],\n",
    "        dvis[11],dvis[12],dvis[13],dvis[14],dvis[15],dvis[16],dvis[17],dvis[18],dvis[19],dvis[20],\n",
    "      ], axis=1), figsize=(15, 15), diagonal='kde')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 18) Remind ourselves what our scaled data (of price or index LEVELS) looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_values_indexed_scaled.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 19) Calculate Log Returns on our scaled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_values_indexed_scaled_logret = pd.DataFrame()\n",
    "data_values_indexed_scaled_logret = np.log(data_values_indexed_scaled/data_values_indexed_scaled.shift(-1)) # note dates are reverse-chrono\n",
    "\n",
    "data_values_indexed_scaled_logret.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_values_indexed_scaled_logret.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_values_indexed_scaled_logret.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 2000/01/07 to 2016/05/27 = 1 header row + 4279 data rows\n",
    "# print data_values_indexed_scaled_logret.head()\n",
    "# print data_values_indexed_scaled_logret.tail()\n",
    "print 'NumRowsIncludeHeader = len(data_values_indexed_scaled_logret) = ' + str(len(data_values_indexed_scaled_logret))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 20) Replace inf, NaN in data with Zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_values_indexed_scaled_logret = data_values_indexed_scaled_logret.replace([np.inf, -np.inf, np.nan], 0)\n",
    "data_values_indexed_scaled_logret.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 21) Skip: Fill the Gaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Pandas includes a very convenient function for filling gaps in the data.\n",
    "# data_values_indexed_scaled_logret = data_values_indexed_scaled_logret.fillna(method='ffill')\n",
    "data_values_indexed_scaled_logret.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 22) Plot log returns of scaled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "_ = data_values_indexed_scaled_logret.plot(figsize=(20, 15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 23) Auto-Correlations of log returns of scaled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "fig.set_figwidth(20)\n",
    "fig.set_figheight(10)\n",
    "\n",
    "_ = autocorrelation_plot(data_values_indexed_scaled_logret)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no auto-correlations, so we are good."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 24) Skip: Just a reminder of the PCA50 Columns we selected earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# list_cols_pca = [ 0,1,\n",
    "#                   1070,788,926,112,69,574,654,1160,527,323,\n",
    "#                   397,118,774,1028,1034,655,907,736,251,388,\n",
    "#                   327,243,705,303,1146,467,136,1006,600,15,\n",
    "#                   231,290,131,782,20,1048,630,1173,431,856,\n",
    "#                   67,299,838,639,53,932,870,938,1061\n",
    "#                 ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 25) Scatter plots of log returns of first 20 of the 86 variables where log-returns of USDMXN(varid=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# _ = scatter_matrix(data_values_indexed_scaled_logret)   # takes long time, becareful, save work first\n",
    "dvislr = data_values_indexed_scaled_logret\n",
    "\n",
    "_ = scatter_matrix(\n",
    "      pd.concat(\n",
    "      [ \n",
    "        dvislr[ 1],dvislr[ 2],dvislr[ 3],dvislr[ 4],dvislr[ 5],dvislr[ 6],dvislr[ 7],dvislr[ 8],dvislr[ 9],dvislr[10],\n",
    "        dvislr[11],dvislr[12],dvislr[13],dvislr[14],dvislr[15],dvislr[16],dvislr[17],dvislr[18],dvislr[19],dvislr[20],\n",
    "      ], axis=1), figsize=(15, 15), diagonal='kde')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_values_indexed_scaled_logret.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summing up the EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, you've done a good enough job of exploratory data analysis. You've visualized our data and come to know it better. \n",
    "You've transformed it into a form that is useful for modelling, log returns, and looked at how indices relate to each other. \n",
    "\n",
    "What should we think so far?\n",
    "\n",
    "Cloud Datalab is working great. With just a few lines of code, you were able to munge the data, visualize the changes, and make decisions. You could easily analyze and iterate. This is a common feature of iPython, but the advantage here is that Cloud Datalab is a managed service that you can simply click and use, so you can focus on your analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we can see a model:\n",
    "\n",
    "* We'll predict whether the USDMXN close today will be higher or lower than yesterday.\n",
    "\n",
    "Predicting whether the log return of the USDMXN is positive or negative is a classification problem. \n",
    "That is, we want to choose one option from a finite set of options, in this case positive or negative. \n",
    "This is the base case of classification where we have only two values to choose from, known as binary classification, or logistic regression.\n",
    "\n",
    "Machine learning models are very good at finding weak signals from data.\n",
    "In machine learning, as in most things, there are subtle tradeoffs happening, but in general good data is better than good algorithms, which are better than good frameworks. \n",
    "You need all three pillars but in that order of importance: data, algorithms, frameworks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[TensorFlow](https://tensorflow.org) is an open source software library, initiated by Google, for numerical computation using data flow graphs. TensorFlow is based on Google's machine learning expertise and is the next generation framework used internally at Google for tasks such as translation and image recognition. It's a wonderful framework for machine learning because it's expressive, efficient, and easy to use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature engineering for TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From a training and testing perspective, time series data is easy. Training data should come from events that happened before test data events, and be contiguous in time.  Otherwise,  your model would be trained on events from \"the future\", at least as compared to the test data. It would then likely perform badly in practice, because you can’t really have access to data from the future. That means random sampling or cross validation don't apply to time series data. Decide on a training-versus-testing split, and divide your data into training and test datasets.\n",
    "\n",
    "In this case, you'll create the features together with two additional columns:\n",
    "\n",
    "* usdmxn_logret_positive, which is 1 if the log return of the USDMXN close is positive, and 0 otherwise. \n",
    "* usdmxn_logret_negative, which is 1 if the log return of the USDMXN close is negative, and 1 otherwise. \n",
    "\n",
    "We'll use 80% of our data for training and 20% for testing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 1: Binary Classification (BC) = Logistic Regression (LR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1: Step 01: Indicator Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 00: Original: Initialize indicator columns to 0\n",
    "data_values_indexed_scaled_logret['usdmxn_logret_positive'] = 0\n",
    "data_values_indexed_scaled_logret['usdmxn_logret_negative'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 00: Populate results columns according to actual usdmxn returns (positive or negative)\n",
    "data_values_indexed_scaled_logret.ix[data_values_indexed_scaled_logret[1] >= 0, 'usdmxn_logret_positive'] = 1\n",
    "data_values_indexed_scaled_logret.ix[data_values_indexed_scaled_logret[1] <  0, 'usdmxn_logret_negative'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 01: Noise Cancellation\n",
    "data_values_indexed_scaled_logret['usdmxn_logret_positive_0050bp'] = 0\n",
    "data_values_indexed_scaled_logret['usdmxn_logret_negative_0050bp'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 01: Populate results columns according to actual usdmxn returns (outside noise zone above or below)\n",
    "data_values_indexed_scaled_logret.ix[data_values_indexed_scaled_logret[1] >= 0.0050, 'usdmxn_logret_positive_0050bp'] = 1\n",
    "data_values_indexed_scaled_logret.ix[data_values_indexed_scaled_logret[1] <  -0.0050, 'usdmxn_logret_negative_0050bp'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # 02: Stratification\n",
    "# data_values_indexed_scaled_logret['usdmxn_logret_+0150bp_and_above'] = 0; data_values_indexed_scaled_logret['usdmxn_logret_+0150bp_and_below'] = 0;\n",
    "# data_values_indexed_scaled_logret['usdmxn_logret_+0140bp_and_above'] = 0; data_values_indexed_scaled_logret['usdmxn_logret_+0140bp_and_below'] = 0;\n",
    "# data_values_indexed_scaled_logret['usdmxn_logret_+0130bp_and_above'] = 0; data_values_indexed_scaled_logret['usdmxn_logret_+0130bp_and_below'] = 0;\n",
    "# data_values_indexed_scaled_logret['usdmxn_logret_+0120bp_and_above'] = 0; data_values_indexed_scaled_logret['usdmxn_logret_+0120bp_and_below'] = 0;\n",
    "# data_values_indexed_scaled_logret['usdmxn_logret_+0110bp_and_above'] = 0; data_values_indexed_scaled_logret['usdmxn_logret_+0110bp_and_below'] = 0;\n",
    "# data_values_indexed_scaled_logret['usdmxn_logret_+0100bp_and_above'] = 0; data_values_indexed_scaled_logret['usdmxn_logret_+0100bp_and_below'] = 0;\n",
    "# data_values_indexed_scaled_logret['usdmxn_logret_+0090bp_and_above'] = 0; data_values_indexed_scaled_logret['usdmxn_logret_+0090bp_and_below'] = 0;\n",
    "# data_values_indexed_scaled_logret['usdmxn_logret_+0080bp_and_above'] = 0; data_values_indexed_scaled_logret['usdmxn_logret_+0080bp_and_below'] = 0;\n",
    "# data_values_indexed_scaled_logret['usdmxn_logret_+0070bp_and_above'] = 0; data_values_indexed_scaled_logret['usdmxn_logret_+0070bp_and_below'] = 0;\n",
    "# data_values_indexed_scaled_logret['usdmxn_logret_+0060bp_and_above'] = 0; data_values_indexed_scaled_logret['usdmxn_logret_+0060bp_and_below'] = 0;\n",
    "# data_values_indexed_scaled_logret['usdmxn_logret_+0050bp_and_above'] = 0; data_values_indexed_scaled_logret['usdmxn_logret_+0050bp_and_below'] = 0;\n",
    "# data_values_indexed_scaled_logret['usdmxn_logret_+0040bp_and_above'] = 0; data_values_indexed_scaled_logret['usdmxn_logret_+0040bp_and_below'] = 0;\n",
    "# data_values_indexed_scaled_logret['usdmxn_logret_+0030bp_and_above'] = 0; data_values_indexed_scaled_logret['usdmxn_logret_+0030bp_and_below'] = 0;\n",
    "# data_values_indexed_scaled_logret['usdmxn_logret_+0020bp_and_above'] = 0; data_values_indexed_scaled_logret['usdmxn_logret_+0020bp_and_below'] = 0;\n",
    "# data_values_indexed_scaled_logret['usdmxn_logret_+0010bp_and_above'] = 0; data_values_indexed_scaled_logret['usdmxn_logret_+0010bp_and_below'] = 0;\n",
    "# data_values_indexed_scaled_logret['usdmxn_logret_+0000bp_and_above'] = 0; data_values_indexed_scaled_logret['usdmxn_logret_+0000bp_and_below'] = 0;\n",
    "# data_values_indexed_scaled_logret['usdmxn_logret_-0010bp_and_above'] = 0; data_values_indexed_scaled_logret['usdmxn_logret_-0010bp_and_below'] = 0;\n",
    "# data_values_indexed_scaled_logret['usdmxn_logret_-0020bp_and_above'] = 0; data_values_indexed_scaled_logret['usdmxn_logret_-0020bp_and_below'] = 0;\n",
    "# data_values_indexed_scaled_logret['usdmxn_logret_-0030bp_and_above'] = 0; data_values_indexed_scaled_logret['usdmxn_logret_-0030bp_and_below'] = 0;\n",
    "# data_values_indexed_scaled_logret['usdmxn_logret_-0040bp_and_above'] = 0; data_values_indexed_scaled_logret['usdmxn_logret_-0040bp_and_below'] = 0;\n",
    "# data_values_indexed_scaled_logret['usdmxn_logret_-0050bp_and_above'] = 0; data_values_indexed_scaled_logret['usdmxn_logret_-0050bp_and_below'] = 0;\n",
    "# data_values_indexed_scaled_logret['usdmxn_logret_-0060bp_and_above'] = 0; data_values_indexed_scaled_logret['usdmxn_logret_-0060bp_and_below'] = 0;\n",
    "# data_values_indexed_scaled_logret['usdmxn_logret_-0070bp_and_above'] = 0; data_values_indexed_scaled_logret['usdmxn_logret_-0070bp_and_below'] = 0;\n",
    "# data_values_indexed_scaled_logret['usdmxn_logret_-0080bp_and_above'] = 0; data_values_indexed_scaled_logret['usdmxn_logret_-0080bp_and_below'] = 0;\n",
    "# data_values_indexed_scaled_logret['usdmxn_logret_-0090bp_and_above'] = 0; data_values_indexed_scaled_logret['usdmxn_logret_-0090bp_and_below'] = 0;\n",
    "# data_values_indexed_scaled_logret['usdmxn_logret_-0100bp_and_above'] = 0; data_values_indexed_scaled_logret['usdmxn_logret_-0100bp_and_below'] = 0;\n",
    "# data_values_indexed_scaled_logret['usdmxn_logret_-0110bp_and_above'] = 0; data_values_indexed_scaled_logret['usdmxn_logret_-0110bp_and_below'] = 0;\n",
    "# data_values_indexed_scaled_logret['usdmxn_logret_-0120bp_and_above'] = 0; data_values_indexed_scaled_logret['usdmxn_logret_-0120bp_and_below'] = 0;\n",
    "# data_values_indexed_scaled_logret['usdmxn_logret_-0130bp_and_above'] = 0; data_values_indexed_scaled_logret['usdmxn_logret_-0130bp_and_below'] = 0;\n",
    "# data_values_indexed_scaled_logret['usdmxn_logret_-0140bp_and_above'] = 0; data_values_indexed_scaled_logret['usdmxn_logret_-0140bp_and_below'] = 0;\n",
    "# data_values_indexed_scaled_logret['usdmxn_logret_-0150bp_and_above'] = 0; data_values_indexed_scaled_logret['usdmxn_logret_-0150bp_and_below'] = 0;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 02: Stratification: Initialize\n",
    "for d_bp in xrange(-150,160,10): # -150:10:+150\n",
    "  d = d_bp / 10000.00\n",
    "  s_above = 'usdmxn_logret_' + '{0:+05d}'.format(d_bp) + 'bp_and_above'\n",
    "  s_below = 'usdmxn_logret_' + '{0:+05d}'.format(d_bp) + 'bp_and_below'\n",
    "  # print d\n",
    "  # print s_above\n",
    "  # print s_below\n",
    "  data_values_indexed_scaled_logret[s_above] = 0;\n",
    "  data_values_indexed_scaled_logret[s_below] = 0;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 02: Stratification: Populate results columns according to actual usdmxn returns (above or below each level)\n",
    "for d_bp in xrange(-150,160,10): # -150:10:+150\n",
    "  d = d_bp / 10000.00\n",
    "  s_above = 'usdmxn_logret_' + '{0:+05d}'.format(d_bp) + 'bp_and_above'\n",
    "  s_below = 'usdmxn_logret_' + '{0:+05d}'.format(d_bp) + 'bp_and_below'\n",
    "  # print d\n",
    "  # print s_above\n",
    "  # print s_below\n",
    "  data_values_indexed_scaled_logret.ix[data_values_indexed_scaled_logret[1] >= d, s_above] = 1\n",
    "  data_values_indexed_scaled_logret.ix[data_values_indexed_scaled_logret[1] <  d, s_below] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_values_indexed_scaled_logret.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_values_indexed_scaled_logret.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1: Step 01.01: Remove data noise in a stratified way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dvislr = data_values_indexed_scaled_logret\n",
    "# dvislrnr = data_values_indexed_scaled_logret (noise removed)\n",
    "\n",
    "# Example Filter: df[(df.A == 1) & (df.D == 6)]\n",
    "dvislr_nr_m0010_z0000_p0010 = dvislr[ (dvislr['usdmxn_logret_-0010bp_and_below'] == 1) | (dvislr['usdmxn_logret_+0010bp_and_above'] == 1) ] # remove data between [-0.10%, +0.10%], center at +0.00%\n",
    "dvislr_nr_m0020_z0000_p0020 = dvislr[ (dvislr['usdmxn_logret_-0020bp_and_below'] == 1) | (dvislr['usdmxn_logret_+0020bp_and_above'] == 1) ] # remove data between [-0.10%, +0.10%], center at +0.00%\n",
    "dvislr_nr_m0030_z0000_p0030 = dvislr[ (dvislr['usdmxn_logret_-0030bp_and_below'] == 1) | (dvislr['usdmxn_logret_+0030bp_and_above'] == 1) ] # remove data between [-0.10%, +0.10%], center at +0.00%\n",
    "dvislr_nr_m0040_z0000_p0040 = dvislr[ (dvislr['usdmxn_logret_-0040bp_and_below'] == 1) | (dvislr['usdmxn_logret_+0040bp_and_above'] == 1) ] # remove data between [-0.10%, +0.10%], center at +0.00%\n",
    "dvislr_nr_m0050_z0000_p0050 = dvislr[ (dvislr['usdmxn_logret_-0050bp_and_below'] == 1) | (dvislr['usdmxn_logret_+0050bp_and_above'] == 1) ] # remove data between [-0.50%, +0.50%], center at +0.00%\n",
    "dvislr_nr_m0060_z0000_p0060 = dvislr[ (dvislr['usdmxn_logret_-0060bp_and_below'] == 1) | (dvislr['usdmxn_logret_+0060bp_and_above'] == 1) ] # remove data between [-0.50%, +0.50%], center at +0.00%\n",
    "dvislr_nr_m0070_z0000_p0070 = dvislr[ (dvislr['usdmxn_logret_-0070bp_and_below'] == 1) | (dvislr['usdmxn_logret_+0070bp_and_above'] == 1) ] # remove data between [-0.50%, +0.50%], center at +0.00%\n",
    "dvislr_nr_m0080_z0000_p0080 = dvislr[ (dvislr['usdmxn_logret_-0080bp_and_below'] == 1) | (dvislr['usdmxn_logret_+0080bp_and_above'] == 1) ] # remove data between [-0.50%, +0.50%], center at +0.00%\n",
    "dvislr_nr_m0090_z0000_p0090 = dvislr[ (dvislr['usdmxn_logret_-0090bp_and_below'] == 1) | (dvislr['usdmxn_logret_+0090bp_and_above'] == 1) ] # remove data between [-0.50%, +0.50%], center at +0.00%\n",
    "dvislr_nr_m0100_z0000_p0100 = dvislr[ (dvislr['usdmxn_logret_-0100bp_and_below'] == 1) | (dvislr['usdmxn_logret_+0100bp_and_above'] == 1) ] # remove data between [-0.50%, +0.50%], center at +0.00%\n",
    "\n",
    "# check rows\n",
    "print 'dvislr_nr_m0010_z0000_p0010 rows =' + str(len(dvislr_nr_m0010_z0000_p0010))\n",
    "print 'dvislr_nr_m0020_z0000_p0020 rows =' + str(len(dvislr_nr_m0020_z0000_p0020))\n",
    "print 'dvislr_nr_m0030_z0000_p0030 rows =' + str(len(dvislr_nr_m0030_z0000_p0030))\n",
    "print 'dvislr_nr_m0040_z0000_p0040 rows =' + str(len(dvislr_nr_m0040_z0000_p0040))\n",
    "print 'dvislr_nr_m0050_z0000_p0050 rows =' + str(len(dvislr_nr_m0050_z0000_p0050))\n",
    "print 'dvislr_nr_m0060_z0000_p0060 rows =' + str(len(dvislr_nr_m0060_z0000_p0060))\n",
    "print 'dvislr_nr_m0070_z0000_p0070 rows =' + str(len(dvislr_nr_m0070_z0000_p0070))\n",
    "print 'dvislr_nr_m0080_z0000_p0080 rows =' + str(len(dvislr_nr_m0080_z0000_p0080))\n",
    "print 'dvislr_nr_m0090_z0000_p0090 rows =' + str(len(dvislr_nr_m0090_z0000_p0090))\n",
    "print 'dvislr_nr_m0100_z0000_p0100 rows =' + str(len(dvislr_nr_m0100_z0000_p0100))\n",
    "\n",
    "# check rows results\n",
    "# dvislr_nr_m0010_z0000_p0010 rows =3554\n",
    "# dvislr_nr_m0020_z0000_p0020 rows =2904\n",
    "# dvislr_nr_m0030_z0000_p0030 rows =2316\n",
    "# dvislr_nr_m0040_z0000_p0040 rows =1837\n",
    "# dvislr_nr_m0050_z0000_p0050 rows =1437\n",
    "# dvislr_nr_m0060_z0000_p0060 rows =1135\n",
    "# dvislr_nr_m0070_z0000_p0070 rows =893\n",
    "# dvislr_nr_m0080_z0000_p0080 rows =697\n",
    "# dvislr_nr_m0090_z0000_p0090 rows =559\n",
    "# dvislr_nr_m0100_z0000_p0100 rows =439"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1: Step 02: Split data into training, test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### training test data empty shell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# training_test_data\n",
    "# col 01-02 = 02 cols = indicators of results\n",
    "# col 03-88 = 86 cols = inputs\n",
    "training_test_data = pd.DataFrame(\n",
    "  columns=[\n",
    "    # 'usdmxn_logret_positive', 'usdmxn_logret_negative',\n",
    "    # 'usdmxn_logret_positive_0050bp', 'usdmxn_logret_negative_0050bp',\n",
    "\n",
    "    'bc_up', 'bc_dn',\n",
    "    \n",
    "    'usdmxn_logret_-0150bp_and_above', 'usdmxn_logret_-0150bp_and_below',\n",
    "    'usdmxn_logret_-0140bp_and_above', 'usdmxn_logret_-0140bp_and_below',\n",
    "    'usdmxn_logret_-0130bp_and_above', 'usdmxn_logret_-0130bp_and_below',\n",
    "    'usdmxn_logret_-0120bp_and_above', 'usdmxn_logret_-0120bp_and_below',\n",
    "    'usdmxn_logret_-0110bp_and_above', 'usdmxn_logret_-0110bp_and_below',\n",
    "    'usdmxn_logret_-0100bp_and_above', 'usdmxn_logret_-0100bp_and_below',\n",
    "    'usdmxn_logret_-0090bp_and_above', 'usdmxn_logret_-0090bp_and_below',\n",
    "    'usdmxn_logret_-0080bp_and_above', 'usdmxn_logret_-0080bp_and_below',\n",
    "    'usdmxn_logret_-0070bp_and_above', 'usdmxn_logret_-0070bp_and_below',\n",
    "    'usdmxn_logret_-0060bp_and_above', 'usdmxn_logret_-0060bp_and_below',\n",
    "    'usdmxn_logret_-0050bp_and_above', 'usdmxn_logret_-0050bp_and_below',\n",
    "    'usdmxn_logret_-0040bp_and_above', 'usdmxn_logret_-0040bp_and_below',\n",
    "    'usdmxn_logret_-0030bp_and_above', 'usdmxn_logret_-0030bp_and_below',\n",
    "    'usdmxn_logret_-0020bp_and_above', 'usdmxn_logret_-0020bp_and_below',\n",
    "    'usdmxn_logret_-0010bp_and_above', 'usdmxn_logret_-0010bp_and_below',\n",
    "    'usdmxn_logret_+0000bp_and_above', 'usdmxn_logret_+0000bp_and_below',\n",
    "    'usdmxn_logret_+0010bp_and_above', 'usdmxn_logret_+0010bp_and_below',\n",
    "    'usdmxn_logret_+0020bp_and_above', 'usdmxn_logret_+0020bp_and_below',\n",
    "    'usdmxn_logret_+0030bp_and_above', 'usdmxn_logret_+0030bp_and_below',\n",
    "    'usdmxn_logret_+0040bp_and_above', 'usdmxn_logret_+0040bp_and_below',\n",
    "    'usdmxn_logret_+0050bp_and_above', 'usdmxn_logret_+0050bp_and_below',\n",
    "    'usdmxn_logret_+0060bp_and_above', 'usdmxn_logret_+0060bp_and_below',\n",
    "    'usdmxn_logret_+0070bp_and_above', 'usdmxn_logret_+0070bp_and_below',\n",
    "    'usdmxn_logret_+0080bp_and_above', 'usdmxn_logret_+0080bp_and_below',\n",
    "    'usdmxn_logret_+0090bp_and_above', 'usdmxn_logret_+0090bp_and_below',\n",
    "    'usdmxn_logret_+0100bp_and_above', 'usdmxn_logret_+0100bp_and_below',\n",
    "    'usdmxn_logret_+0110bp_and_above', 'usdmxn_logret_+0110bp_and_below',\n",
    "    'usdmxn_logret_+0120bp_and_above', 'usdmxn_logret_+0120bp_and_below',\n",
    "    'usdmxn_logret_+0130bp_and_above', 'usdmxn_logret_+0130bp_and_below',\n",
    "    'usdmxn_logret_+0140bp_and_above', 'usdmxn_logret_+0140bp_and_below',\n",
    "    'usdmxn_logret_+0150bp_and_above', 'usdmxn_logret_+0150bp_and_below',\n",
    "\n",
    "    '1','2','3','4','5','6','7','8','9','10',\n",
    "    '11','12','13','14','15','16','17','18','19','20',\n",
    "    '21','22','23','24','25','26','27','28','29','30',\n",
    "    '31','32','33','34','35','36','37','38','39','40',\n",
    "    '41','42','43','44','45','46','47','48','49','50',\n",
    "    '51','52','53','54','55','56','57','58','59','60',\n",
    "    '61','62','63','64','65','66','67','68','69','70',\n",
    "    '71','72','73','74','75','76','77','78','79','80',\n",
    "    '81','82','83','84','85','86',\n",
    "  ])\n",
    "\n",
    "training_test_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data into variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# row 0      = header\n",
    "# row 1-4279 = data\n",
    "# NumRowsIncludeHeader = len(data_values_indexed_scaled_logret) = 4280 = 0-4279\n",
    "# Start from row 7, so we can have up to (7-1) lookback days\n",
    "\n",
    "# check rows results\n",
    "# dvislr_nr_m0010_z0000_p0010 rows =3554\n",
    "# dvislr_nr_m0020_z0000_p0020 rows =2904\n",
    "# dvislr_nr_m0030_z0000_p0030 rows =2316\n",
    "# dvislr_nr_m0040_z0000_p0040 rows =1837\n",
    "# dvislr_nr_m0050_z0000_p0050 rows =1437\n",
    "# dvislr_nr_m0060_z0000_p0060 rows =1135\n",
    "# dvislr_nr_m0070_z0000_p0070 rows =893\n",
    "# dvislr_nr_m0080_z0000_p0080 rows =697\n",
    "# dvislr_nr_m0090_z0000_p0090 rows =559\n",
    "# dvislr_nr_m0100_z0000_p0100 rows =439\n",
    "\n",
    "#==========================================================================================\n",
    "# Select the data\n",
    "#==========================================================================================\n",
    "# dvislr =  dvislr                          # default\n",
    "# dvislr =  dvislr_nr_m0010_z0000_p0010\n",
    "# dvislr =  dvislr_nr_m0020_z0000_p0020\n",
    "# dvislr =  dvislr_nr_m0030_z0000_p0030\n",
    "# dvislr =  dvislr_nr_m0040_z0000_p0040\n",
    "# dvislr =  dvislr_nr_m0050_z0000_p0050\n",
    "# dvislr =  dvislr_nr_m0060_z0000_p0060\n",
    "# dvislr =  dvislr_nr_m0070_z0000_p0070\n",
    "# dvislr =  dvislr_nr_m0080_z0000_p0080\n",
    "# dvislr =  dvislr_nr_m0090_z0000_p0090\n",
    "# dvislr =  dvislr_nr_m0100_z0000_p0100\n",
    "\n",
    "for i in range(7, len(dvislr)): # [7, 4280) = [7,4279] \n",
    "\n",
    "  # 01: original\n",
    "  usdmxn_logret_positive = data_values_indexed_scaled_logret['usdmxn_logret_positive'].ix[i]\n",
    "  usdmxn_logret_negative = data_values_indexed_scaled_logret['usdmxn_logret_negative'].ix[i]\n",
    "\n",
    "  # 02: stratification  \n",
    "  usdmxn_logret_p0150bp_and_below = dvislr['usdmxn_logret_+0150bp_and_below'].ix[i]; usdmxn_logret_p0150bp_and_above = dvislr['usdmxn_logret_+0150bp_and_above'].ix[i];\n",
    "  usdmxn_logret_p0140bp_and_below = dvislr['usdmxn_logret_+0140bp_and_below'].ix[i]; usdmxn_logret_p0140bp_and_above = dvislr['usdmxn_logret_+0140bp_and_above'].ix[i];\n",
    "  usdmxn_logret_p0130bp_and_below = dvislr['usdmxn_logret_+0130bp_and_below'].ix[i]; usdmxn_logret_p0130bp_and_above = dvislr['usdmxn_logret_+0130bp_and_above'].ix[i];\n",
    "  usdmxn_logret_p0120bp_and_below = dvislr['usdmxn_logret_+0120bp_and_below'].ix[i]; usdmxn_logret_p0120bp_and_above = dvislr['usdmxn_logret_+0120bp_and_above'].ix[i];\n",
    "  usdmxn_logret_p0110bp_and_below = dvislr['usdmxn_logret_+0110bp_and_below'].ix[i]; usdmxn_logret_p0110bp_and_above = dvislr['usdmxn_logret_+0110bp_and_above'].ix[i];\n",
    "  \n",
    "  usdmxn_logret_p0100bp_and_below = dvislr['usdmxn_logret_+0100bp_and_below'].ix[i]; usdmxn_logret_p0100bp_and_above = dvislr['usdmxn_logret_+0100bp_and_above'].ix[i];\n",
    "  usdmxn_logret_p0090bp_and_below = dvislr['usdmxn_logret_+0090bp_and_below'].ix[i]; usdmxn_logret_p0090bp_and_above = dvislr['usdmxn_logret_+0090bp_and_above'].ix[i];\n",
    "  usdmxn_logret_p0080bp_and_below = dvislr['usdmxn_logret_+0080bp_and_below'].ix[i]; usdmxn_logret_p0080bp_and_above = dvislr['usdmxn_logret_+0080bp_and_above'].ix[i];\n",
    "  usdmxn_logret_p0070bp_and_below = dvislr['usdmxn_logret_+0070bp_and_below'].ix[i]; usdmxn_logret_p0070bp_and_above = dvislr['usdmxn_logret_+0070bp_and_above'].ix[i];\n",
    "  usdmxn_logret_p0060bp_and_below = dvislr['usdmxn_logret_+0060bp_and_below'].ix[i]; usdmxn_logret_p0060bp_and_above = dvislr['usdmxn_logret_+0060bp_and_above'].ix[i];\n",
    "\n",
    "  usdmxn_logret_p0050bp_and_below = dvislr['usdmxn_logret_+0050bp_and_below'].ix[i]; usdmxn_logret_p0050bp_and_above = dvislr['usdmxn_logret_+0050bp_and_above'].ix[i];\n",
    "  usdmxn_logret_p0040bp_and_below = dvislr['usdmxn_logret_+0040bp_and_below'].ix[i]; usdmxn_logret_p0040bp_and_above = dvislr['usdmxn_logret_+0040bp_and_above'].ix[i];\n",
    "  usdmxn_logret_p0030bp_and_below = dvislr['usdmxn_logret_+0030bp_and_below'].ix[i]; usdmxn_logret_p0030bp_and_above = dvislr['usdmxn_logret_+0030bp_and_above'].ix[i];\n",
    "  usdmxn_logret_p0020bp_and_below = dvislr['usdmxn_logret_+0020bp_and_below'].ix[i]; usdmxn_logret_p0020bp_and_above = dvislr['usdmxn_logret_+0020bp_and_above'].ix[i];\n",
    "  usdmxn_logret_p0010bp_and_below = dvislr['usdmxn_logret_+0010bp_and_below'].ix[i]; usdmxn_logret_p0010bp_and_above = dvislr['usdmxn_logret_+0010bp_and_above'].ix[i];\n",
    "\n",
    "  usdmxn_logret_p0000bp_and_below = dvislr['usdmxn_logret_+0000bp_and_below'].ix[i]; usdmxn_logret_p0000bp_and_above = dvislr['usdmxn_logret_+0000bp_and_above'].ix[i];\n",
    "\n",
    "  usdmxn_logret_m0010bp_and_below = dvislr['usdmxn_logret_-0010bp_and_below'].ix[i]; usdmxn_logret_m0010bp_and_above = dvislr['usdmxn_logret_-0010bp_and_above'].ix[i];\n",
    "  usdmxn_logret_m0020bp_and_below = dvislr['usdmxn_logret_-0020bp_and_below'].ix[i]; usdmxn_logret_m0020bp_and_above = dvislr['usdmxn_logret_-0020bp_and_above'].ix[i];\n",
    "  usdmxn_logret_m0030bp_and_below = dvislr['usdmxn_logret_-0030bp_and_below'].ix[i]; usdmxn_logret_m0030bp_and_above = dvislr['usdmxn_logret_-0030bp_and_above'].ix[i];\n",
    "  usdmxn_logret_m0040bp_and_below = dvislr['usdmxn_logret_-0040bp_and_below'].ix[i]; usdmxn_logret_m0040bp_and_above = dvislr['usdmxn_logret_-0040bp_and_above'].ix[i];\n",
    "  usdmxn_logret_m0050bp_and_below = dvislr['usdmxn_logret_-0050bp_and_below'].ix[i]; usdmxn_logret_m0050bp_and_above = dvislr['usdmxn_logret_-0050bp_and_above'].ix[i];\n",
    "\n",
    "  usdmxn_logret_m0060bp_and_below = dvislr['usdmxn_logret_-0060bp_and_below'].ix[i]; usdmxn_logret_m0060bp_and_above = dvislr['usdmxn_logret_-0060bp_and_above'].ix[i];\n",
    "  usdmxn_logret_m0070bp_and_below = dvislr['usdmxn_logret_-0070bp_and_below'].ix[i]; usdmxn_logret_m0070bp_and_above = dvislr['usdmxn_logret_-0070bp_and_above'].ix[i];\n",
    "  usdmxn_logret_m0080bp_and_below = dvislr['usdmxn_logret_-0080bp_and_below'].ix[i]; usdmxn_logret_m0080bp_and_above = dvislr['usdmxn_logret_-0080bp_and_above'].ix[i];\n",
    "  usdmxn_logret_m0090bp_and_below = dvislr['usdmxn_logret_-0090bp_and_below'].ix[i]; usdmxn_logret_m0090bp_and_above = dvislr['usdmxn_logret_-0090bp_and_above'].ix[i];\n",
    "  usdmxn_logret_m0100bp_and_below = dvislr['usdmxn_logret_-0100bp_and_below'].ix[i]; usdmxn_logret_m0100bp_and_above = dvislr['usdmxn_logret_-0100bp_and_above'].ix[i];\n",
    "\n",
    "  usdmxn_logret_m0110bp_and_below = dvislr['usdmxn_logret_-0110bp_and_below'].ix[i]; usdmxn_logret_m0110bp_and_above = dvislr['usdmxn_logret_-0110bp_and_above'].ix[i];\n",
    "  usdmxn_logret_m0120bp_and_below = dvislr['usdmxn_logret_-0120bp_and_below'].ix[i]; usdmxn_logret_m0120bp_and_above = dvislr['usdmxn_logret_-0120bp_and_above'].ix[i];\n",
    "  usdmxn_logret_m0130bp_and_below = dvislr['usdmxn_logret_-0130bp_and_below'].ix[i]; usdmxn_logret_m0130bp_and_above = dvislr['usdmxn_logret_-0130bp_and_above'].ix[i];\n",
    "  usdmxn_logret_m0140bp_and_below = dvislr['usdmxn_logret_-0140bp_and_below'].ix[i]; usdmxn_logret_m0140bp_and_above = dvislr['usdmxn_logret_-0140bp_and_above'].ix[i];\n",
    "  usdmxn_logret_m0150bp_and_below = dvislr['usdmxn_logret_-0150bp_and_below'].ix[i]; usdmxn_logret_m0150bp_and_above = dvislr['usdmxn_logret_-0150bp_and_above'].ix[i];\n",
    "\n",
    "  #==========================================================================================\n",
    "  # Select 2 levels: up and dn\n",
    "  #==========================================================================================\n",
    "  bc_dn = usdmxn_logret_negative; bc_up = usdmxn_logret_positive; # 01: original\n",
    "  #==========================================================================================  \n",
    "#   bc_dn = usdmxn_logret_m0010bp_and_below; bc_up = usdmxn_logret_p0010bp_and_above;\n",
    "#   bc_dn = usdmxn_logret_m0020bp_and_below; bc_up = usdmxn_logret_p0020bp_and_above;\n",
    "#   bc_dn = usdmxn_logret_m0030bp_and_below; bc_up = usdmxn_logret_p0030bp_and_above;\n",
    "#   bc_dn = usdmxn_logret_m0040bp_and_below; bc_up = usdmxn_logret_p0040bp_and_above;\n",
    "#   bc_dn = usdmxn_logret_m0050bp_and_below; bc_up = usdmxn_logret_p0050bp_and_above;\n",
    "#   bc_dn = usdmxn_logret_m0060bp_and_below; bc_up = usdmxn_logret_p0060bp_and_above;\n",
    "#   bc_dn = usdmxn_logret_m0070bp_and_below; bc_up = usdmxn_logret_p0070bp_and_above;\n",
    "#   bc_dn = usdmxn_logret_m0080bp_and_below; bc_up = usdmxn_logret_p0080bp_and_above;\n",
    "#   bc_dn = usdmxn_logret_m0090bp_and_below; bc_up = usdmxn_logret_p0090bp_and_above;\n",
    "#   bc_dn = usdmxn_logret_m0100bp_and_below; bc_up = usdmxn_logret_p0100bp_and_above;\n",
    "  \n",
    "  #   v_001_Tm005 = data_values_indexed_scaled_logret['1'].ix[i-5]  # lookback 1d\n",
    "\n",
    "  v_001_Tm000 = dvislr[1].ix[i];  v_002_Tm000 = dvislr[2].ix[i];  v_003_Tm000 = dvislr[3].ix[i];  v_004_Tm000 = dvislr[4].ix[i];  v_005_Tm000 = dvislr[5].ix[i];\n",
    "  v_006_Tm000 = dvislr[6].ix[i];  v_007_Tm000 = dvislr[7].ix[i];  v_008_Tm000 = dvislr[8].ix[i];  v_009_Tm000 = dvislr[9].ix[i];  v_010_Tm000 = dvislr[10].ix[i]\n",
    "\n",
    "  v_011_Tm000 = dvislr[11].ix[i]; v_012_Tm000 = dvislr[12].ix[i]; v_013_Tm000 = dvislr[13].ix[i]; v_014_Tm000 = dvislr[14].ix[i]; v_015_Tm000 = dvislr[15].ix[i];\n",
    "  v_016_Tm000 = dvislr[16].ix[i]; v_017_Tm000 = dvislr[17].ix[i]; v_018_Tm000 = dvislr[18].ix[i]; v_019_Tm000 = dvislr[19].ix[i]; v_020_Tm000 = dvislr[20].ix[i];\n",
    "  \n",
    "  v_021_Tm000 = dvislr[21].ix[i]; v_022_Tm000 = dvislr[22].ix[i]; v_023_Tm000 = dvislr[23].ix[i]; v_024_Tm000 = dvislr[24].ix[i]; v_025_Tm000 = dvislr[25].ix[i];\n",
    "  v_026_Tm000 = dvislr[26].ix[i]; v_027_Tm000 = dvislr[27].ix[i]; v_028_Tm000 = dvislr[28].ix[i]; v_029_Tm000 = dvislr[29].ix[i]; v_030_Tm000 = dvislr[30].ix[i];\n",
    "\n",
    "  v_031_Tm000 = dvislr[31].ix[i]; v_032_Tm000 = dvislr[32].ix[i]; v_033_Tm000 = dvislr[33].ix[i]; v_034_Tm000 = dvislr[34].ix[i]; v_035_Tm000 = dvislr[35].ix[i];\n",
    "  v_036_Tm000 = dvislr[36].ix[i]; v_037_Tm000 = dvislr[37].ix[i]; v_038_Tm000 = dvislr[38].ix[i]; v_039_Tm000 = dvislr[39].ix[i]; v_040_Tm000 = dvislr[40].ix[i];\n",
    "\n",
    "  v_041_Tm000 = dvislr[41].ix[i]; v_042_Tm000 = dvislr[42].ix[i]; v_043_Tm000 = dvislr[43].ix[i]; v_044_Tm000 = dvislr[44].ix[i]; v_045_Tm000 = dvislr[45].ix[i];\n",
    "  v_046_Tm000 = dvislr[46].ix[i]; v_047_Tm000 = dvislr[47].ix[i]; v_048_Tm000 = dvislr[48].ix[i]; v_049_Tm000 = dvislr[49].ix[i]; v_050_Tm000 = dvislr[50].ix[i];\n",
    "\n",
    "  v_051_Tm000 = dvislr[51].ix[i]; v_052_Tm000 = dvislr[52].ix[i]; v_053_Tm000 = dvislr[53].ix[i]; v_054_Tm000 = dvislr[54].ix[i]; v_055_Tm000 = dvislr[55].ix[i];\n",
    "  v_056_Tm000 = dvislr[56].ix[i]; v_057_Tm000 = dvislr[57].ix[i]; v_058_Tm000 = dvislr[58].ix[i]; v_059_Tm000 = dvislr[59].ix[i]; v_060_Tm000 = dvislr[60].ix[i];\n",
    "\n",
    "  v_061_Tm000 = dvislr[61].ix[i]; v_062_Tm000 = dvislr[62].ix[i]; v_063_Tm000 = dvislr[63].ix[i]; v_064_Tm000 = dvislr[64].ix[i]; v_065_Tm000 = dvislr[65].ix[i];\n",
    "  v_066_Tm000 = dvislr[66].ix[i]; v_067_Tm000 = dvislr[67].ix[i]; v_068_Tm000 = dvislr[68].ix[i]; v_069_Tm000 = dvislr[69].ix[i]; v_070_Tm000 = dvislr[70].ix[i];\n",
    "\n",
    "  v_071_Tm000 = dvislr[71].ix[i]; v_072_Tm000 = dvislr[72].ix[i]; v_073_Tm000 = dvislr[73].ix[i]; v_074_Tm000 = dvislr[74].ix[i]; v_075_Tm000 = dvislr[75].ix[i];\n",
    "  v_076_Tm000 = dvislr[76].ix[i]; v_077_Tm000 = dvislr[77].ix[i]; v_078_Tm000 = dvislr[78].ix[i]; v_079_Tm000 = dvislr[79].ix[i]; v_080_Tm000 = dvislr[80].ix[i];\n",
    "\n",
    "  v_081_Tm000 = dvislr[81].ix[i]; v_082_Tm000 = dvislr[82].ix[i]; v_083_Tm000 = dvislr[83].ix[i]; v_084_Tm000 = dvislr[84].ix[i]; v_085_Tm000 = dvislr[85].ix[i];\n",
    "  v_086_Tm000 = dvislr[86].ix[i];\n",
    "\n",
    "  # training_test_data: append data\n",
    "  training_test_data = training_test_data.append(\n",
    "    {\n",
    "#       'usdmxn_logret_positive':usdmxn_logret_positive,\n",
    "#       'usdmxn_logret_negative':usdmxn_logret_negative,\n",
    "#       'usdmxn_logret_positive_0050bp':usdmxn_logret_positive_0050bp,\n",
    "#       'usdmxn_logret_negative_0050bp':usdmxn_logret_negative_0050bp,\n",
    "      \n",
    "      'bc_up':bc_up,\n",
    "      'bc_dn':bc_dn,\n",
    "      \n",
    "      '1':v_001_Tm000,  '2':v_002_Tm000, '3':v_003_Tm000, '4':v_004_Tm000, '5':v_005_Tm000,\n",
    "      '6':v_006_Tm000,  '7':v_007_Tm000, '8':v_008_Tm000, '9':v_009_Tm000, '10':v_010_Tm000,\n",
    "\n",
    "      '11':v_011_Tm000, '12':v_012_Tm000, '13':v_013_Tm000, '14':v_014_Tm000, '15':v_015_Tm000,\n",
    "      '16':v_016_Tm000, '17':v_017_Tm000, '18':v_018_Tm000, '19':v_019_Tm000, '20':v_020_Tm000,\n",
    "\n",
    "      '21':v_021_Tm000, '22':v_022_Tm000, '23':v_023_Tm000, '24':v_024_Tm000, '25':v_025_Tm000,\n",
    "      '26':v_026_Tm000, '27':v_027_Tm000, '28':v_028_Tm000, '29':v_029_Tm000, '30':v_030_Tm000,\n",
    "\n",
    "      '31':v_031_Tm000, '32':v_032_Tm000, '33':v_033_Tm000, '34':v_034_Tm000, '35':v_035_Tm000,\n",
    "      '36':v_036_Tm000, '37':v_037_Tm000, '38':v_038_Tm000, '39':v_039_Tm000, '40':v_040_Tm000,\n",
    "\n",
    "      '41':v_041_Tm000, '42':v_042_Tm000, '43':v_043_Tm000, '44':v_044_Tm000, '45':v_045_Tm000,\n",
    "      '46':v_046_Tm000, '47':v_047_Tm000, '48':v_048_Tm000, '49':v_049_Tm000, '50':v_050_Tm000,\n",
    "\n",
    "      '51':v_051_Tm000, '52':v_052_Tm000, '53':v_053_Tm000, '54':v_054_Tm000, '55':v_055_Tm000,\n",
    "      '56':v_056_Tm000, '57':v_057_Tm000, '58':v_058_Tm000, '59':v_059_Tm000, '60':v_060_Tm000,\n",
    "\n",
    "      '61':v_061_Tm000, '62':v_062_Tm000, '63':v_063_Tm000, '64':v_064_Tm000, '65':v_065_Tm000,\n",
    "      '66':v_066_Tm000, '67':v_067_Tm000, '68':v_068_Tm000, '69':v_069_Tm000, '70':v_070_Tm000,\n",
    "\n",
    "      '71':v_071_Tm000, '72':v_072_Tm000, '73':v_073_Tm000, '74':v_074_Tm000, '75':v_075_Tm000,\n",
    "      '76':v_076_Tm000, '77':v_077_Tm000, '78':v_078_Tm000, '79':v_079_Tm000, '80':v_080_Tm000,\n",
    "      \n",
    "      '81':v_081_Tm000, '82':v_082_Tm000, '83':v_083_Tm000, '84':v_084_Tm000, '85':v_085_Tm000,\n",
    "      '86':v_086_Tm000,\n",
    "      \n",
    "    },\n",
    "    ignore_index=True)\n",
    "  \n",
    "# data_values_indexed_scaled_logret: row [7,4279] \n",
    "# training_test_data               : row [0,4272] = 4273 rows\n",
    "# training_test_data: col 01-02 = 02 cols = binary outputs\n",
    "# training_test_data: col 03-88 = 86 cols = inputs  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# training_test_data.describe()\n",
    "training_test_data[['bc_up','bc_dn']].describe()\n",
    "\n",
    "# check rows results\n",
    "# dvislr_nr_m0010_z0000_p0010 rows =3554 - 7 = 3547\n",
    "# dvislr_nr_m0020_z0000_p0020 rows =2904 - 7 = 2897\n",
    "# dvislr_nr_m0030_z0000_p0030 rows =2316 - 7 = 2309\n",
    "# dvislr_nr_m0040_z0000_p0040 rows =1837 - 7 = 1830\n",
    "# dvislr_nr_m0050_z0000_p0050 rows =1437 - 7 = 1430\n",
    "# dvislr_nr_m0060_z0000_p0060 rows =1135 - 7 = 1128\n",
    "# dvislr_nr_m0070_z0000_p0070 rows = 893 - 7 =  886\n",
    "# dvislr_nr_m0080_z0000_p0080 rows = 697 - 7 =  690\n",
    "# dvislr_nr_m0090_z0000_p0090 rows = 559 - 7 =  552\n",
    "# dvislr_nr_m0100_z0000_p0100 rows = 439 - 7 =  432"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create the training and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 86 input columns\n",
    "predictors_tf = training_test_data[training_test_data.columns[0+2+(31*2):0+2+(31*2)+86]]\n",
    "predictors_tf.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 2 output columns\n",
    "classes_tf = training_test_data[training_test_data.columns[0:2]] # col 0, 1\n",
    "# classes_tf = training_test_data[training_test_data.columns[2:4]] # col 2, 3\n",
    "classes_tf.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Split: train data = 80%\n",
    "training_set_size = int(len(training_test_data) * 0.8)\n",
    "print 'training_set_size=' + str(training_set_size )\n",
    "# Split: test data = 20%\n",
    "test_set_size = len(training_test_data) - training_set_size\n",
    "print 'test_set_size=' + str(test_set_size )\n",
    "\n",
    "# noise remove stratification: [-0010,+0010]: training_set_size=2837, test_set_size=710\n",
    "# noise remove stratification: [-0020,+0020]: training_set_size=2317, test_set_size=580\n",
    "# noise remove stratification: [-0030,+0030]: training_set_size=1847, test_set_size=462\n",
    "# noise remove stratification: [-0040,+0040]: training_set_size=1464, test_set_size=366\n",
    "# noise remove stratification: [-0050,+0050]: training_set_size=1144, test_set_size=286\n",
    "# noise remove stratification: [-0060,+0060]: training_set_size=902,  test_set_size=226\n",
    "# noise remove stratification: [-0070,+0070]: training_set_size=708,  test_set_size=178\n",
    "# noise remove stratification: [-0080,+0080]: training_set_size=552,  test_set_size=138\n",
    "# noise remove stratification: [-0090,+0090]: training_set_size=441,  test_set_size=111\n",
    "# noise remove stratification: [-0100,+0100]: training_set_size=345,  test_set_size=87\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split into training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "training_predictors_tf = predictors_tf[:training_set_size]\n",
    "training_classes_tf = classes_tf[:training_set_size]\n",
    "test_predictors_tf = predictors_tf[training_set_size:]\n",
    "test_classes_tf = classes_tf[training_set_size:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define some metrics here to evaluate the models.\n",
    "\n",
    "* [Precision](https://en.wikipedia.org/wiki/Precision_and_recall#Precision) -  The ability of the classifier not to label as positive a sample that is negative.\n",
    "* [Recall](https://en.wikipedia.org/wiki/Precision_and_recall#Recall) - The ability of the classifier to find all the positive samples.\n",
    "* [F1 Score](https://en.wikipedia.org/wiki/F1_score) - A weighted average of the precision and recall, where an F1 score reaches its best value at 1 and worst score at 0.\n",
    "* Accuracy - The percentage correctly predicted in the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def tf_confusion_metrics(model, actual_classes, session, feed_dict):\n",
    "  predictions = tf.argmax(model, 1)\n",
    "  actuals = tf.argmax(actual_classes, 1)\n",
    "\n",
    "  ones_like_actuals = tf.ones_like(actuals)\n",
    "  zeros_like_actuals = tf.zeros_like(actuals)\n",
    "  ones_like_predictions = tf.ones_like(predictions)\n",
    "  zeros_like_predictions = tf.zeros_like(predictions)\n",
    "\n",
    "  tp_op = tf.reduce_sum(\n",
    "    tf.cast(\n",
    "      tf.logical_and(\n",
    "        tf.equal(actuals, ones_like_actuals), \n",
    "        tf.equal(predictions, ones_like_predictions)\n",
    "      ), \n",
    "      \"float\"\n",
    "    )\n",
    "  )\n",
    "\n",
    "  tn_op = tf.reduce_sum(\n",
    "    tf.cast(\n",
    "      tf.logical_and(\n",
    "        tf.equal(actuals, zeros_like_actuals), \n",
    "        tf.equal(predictions, zeros_like_predictions)\n",
    "      ), \n",
    "      \"float\"\n",
    "    )\n",
    "  )\n",
    "\n",
    "  fp_op = tf.reduce_sum(\n",
    "    tf.cast(\n",
    "      tf.logical_and(\n",
    "        tf.equal(actuals, zeros_like_actuals), \n",
    "        tf.equal(predictions, ones_like_predictions)\n",
    "      ), \n",
    "      \"float\"\n",
    "    )\n",
    "  )\n",
    "\n",
    "  fn_op = tf.reduce_sum(\n",
    "    tf.cast(\n",
    "      tf.logical_and(\n",
    "        tf.equal(actuals, ones_like_actuals), \n",
    "        tf.equal(predictions, zeros_like_predictions)\n",
    "      ), \n",
    "      \"float\"\n",
    "    )\n",
    "  )\n",
    "\n",
    "  tp, tn, fp, fn = \\\n",
    "    session.run(\n",
    "      [tp_op, tn_op, fp_op, fn_op], \n",
    "      feed_dict\n",
    "    )\n",
    "\n",
    "  tpr = float(tp)/(float(tp) + float(fn))\n",
    "  fpr = float(fp)/(float(tp) + float(fn))\n",
    "\n",
    "  accuracy = (float(tp) + float(tn))/(float(tp) + float(fp) + float(fn) + float(tn))\n",
    "\n",
    "  recall = tpr\n",
    "  precision = float(tp)/(float(tp) + float(fp))\n",
    "  \n",
    "  f1_score = (2 * (precision * recall)) / (precision + recall)\n",
    "  \n",
    "  print 'Precision = ', precision\n",
    "  print 'Recall = ', recall\n",
    "  print 'F1 Score = ', f1_score\n",
    "  print 'Accuracy = ', accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary classification with TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, get some tensors flowing. The model is binary classification expressed in TensorFlow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# tensorflow session\n",
    "sess = tf.Session()\n",
    "# Define variables for the number of predictors and number of classes to remove magic numbers from our code.\n",
    "num_predictors = len(training_predictors_tf.columns) # 24-6=18 in the default case\n",
    "num_classes = len(training_classes_tf.columns) # 2 in the default case\n",
    "print 'num_predictors=' + str(num_predictors)\n",
    "print 'num_classes=' + str(num_classes)\n",
    "# Define placeholders for the data we feed into the process - feature data and actual classes.\n",
    "feature_data = tf.placeholder(\"float\", [None, num_predictors])\n",
    "actual_classes = tf.placeholder(\"float\", [None, num_classes])\n",
    "# Define a matrix of weights and initialize it with some small random values.\n",
    "weights = tf.Variable(tf.truncated_normal([num_predictors, num_classes], stddev=0.0001))\n",
    "biases = tf.Variable(tf.ones([num_classes]))\n",
    "# Define our model...\n",
    "# Here we take a softmax regression of the product of our feature data and weights.\n",
    "model = tf.nn.softmax(tf.matmul(feature_data, weights) + biases)\n",
    "\n",
    "# Define a cost function (we're using the cross entropy).\n",
    "cost = -tf.reduce_sum(actual_classes*tf.log(model))\n",
    "\n",
    "# Define a training step...\n",
    "# Here we use gradient descent with a learning rate of 0.01 using the cost function we just defined.\n",
    "training_step = tf.train.AdamOptimizer(learning_rate=0.0001).minimize(cost)\n",
    "\n",
    "init = tf.initialize_all_variables()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# BEFORE the model has been run, ie. not yet trained\n",
    "# display weights\n",
    "w_86_2 = sess.run(weights)    \n",
    "# print w_86_2 \n",
    "\n",
    "# Expect something like this (small numbers e-05): \n",
    "# [[ -6.66152628e-05  -6.49469657e-05]\n",
    "#  [  2.86067752e-05   3.27789494e-05]\n",
    "#  [  4.73170294e-05   1.85125769e-04]\n",
    "#  [ -6.42955492e-05   4.16024632e-05]\n",
    "#  [ -1.54488771e-05  -2.10903818e-05]\n",
    "#  [  4.62506796e-05  -2.98624745e-05]\n",
    "#  [ -5.35508079e-05  -1.30856875e-04]\n",
    "#  [ -1.70812025e-04   1.33106529e-04]\n",
    "#  [ -1.45097118e-04  -1.80459567e-04]\n",
    "#  [  1.02448161e-04   8.27739277e-05]\n",
    "#  [ -1.33277281e-04  -4.04360726e-05]\n",
    "#  [ -1.36186543e-04   8.62382149e-05]\n",
    "#  [  2.80324894e-05  -2.20580205e-05]\n",
    "#  [ -4.52000713e-05   2.54859442e-05]\n",
    "#  [ -6.20259525e-05  -6.95227573e-05]\n",
    "#  [  1.68935469e-04  -4.03221093e-05]\n",
    "#  [ -9.51008842e-05   4.10227585e-05]\n",
    "#  [ -5.90909331e-05  -9.20566745e-05]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll train our model in the following snippet. The approach of TensorFlow to executing graph operations allows fine-grained control over the process. Any operation you provide to the session as part of the run operation will be executed and the results returned. You can provide a list of multiple operations.\n",
    "\n",
    "You'll train the model over 30,000 iterations using the full dataset each time. Every thousandth iteration we'll assess the accuracy of the model on the training data to assess progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "correct_prediction = tf.equal(tf.argmax(model, 1), tf.argmax(actual_classes, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "\n",
    "for i in range(1, 30001):\n",
    "  sess.run(\n",
    "    training_step, \n",
    "    feed_dict={\n",
    "      feature_data: training_predictors_tf.values, \n",
    "      actual_classes: training_classes_tf.values.reshape(len(training_classes_tf.values), 2)\n",
    "    }\n",
    "  )\n",
    "  if i%5000 == 0:\n",
    "    print i, sess.run(\n",
    "      accuracy,\n",
    "      feed_dict={\n",
    "        feature_data: training_predictors_tf.values, \n",
    "        actual_classes: training_classes_tf.values.reshape(len(training_classes_tf.values), 2)\n",
    "      }\n",
    "    )\n",
    "    \n",
    "# Expect:\n",
    "# 5000 0.603862\n",
    "# 10000 0.630778\n",
    "# 15000 0.639555\n",
    "# 20000 0.654184\n",
    "# 25000 0.660035\n",
    "# 30000 0.666764\n",
    "\n",
    "# After using < -0.5% and > +0.5%\n",
    "# 5000 0.928906\n",
    "# 10000 0.928906\n",
    "# 15000 0.928906\n",
    "# 20000 0.928906\n",
    "# 25000 0.928906\n",
    "# 30000 0.928906\n",
    "\n",
    "# Noise Removal: [-0010, +0010]\n",
    "# 5000 0.617201\n",
    "# 10000 0.64399\n",
    "# 15000 0.665139\n",
    "# 20000 0.683468\n",
    "# 25000 0.688403\n",
    "# 30000 0.691928\n",
    "\n",
    "# Noise Removal: [-0020, +0020]\n",
    "# 5000 0.625809\n",
    "# 10000 0.6612\n",
    "# 15000 0.680622\n",
    "# 20000 0.695727\n",
    "# 25000 0.708675\n",
    "# 30000 0.712128\n",
    "\n",
    "# Noise Removal: [-0030, +0030]\n",
    "# 5000 0.635084\n",
    "# 10000 0.677315\n",
    "# 15000 0.701678\n",
    "# 20000 0.714672\n",
    "# 25000 0.729291\n",
    "# 30000 0.738495\n",
    "\n",
    "# Noise Removal: [-0040, +0040]\n",
    "# 5000 0.645492\n",
    "# 10000 0.689891\n",
    "# 15000 0.717213\n",
    "# 20000 0.734973\n",
    "# 25000 0.748634\n",
    "# 30000 0.758197\n",
    "\n",
    "# Noise Removal: [-0050, +0050]\n",
    "# 5000 0.653846\n",
    "# 10000 0.702797\n",
    "# 15000 0.730769\n",
    "# 20000 0.740385\n",
    "# 25000 0.75\n",
    "# 30000 0.768357\n",
    "\n",
    "# Noise Removal: [-0060, +0060]\n",
    "# 5000 0.672949\n",
    "# 10000 0.72173\n",
    "# 15000 0.750554\n",
    "# 20000 0.761641\n",
    "# 25000 0.783814\n",
    "# 30000 0.792683\n",
    "\n",
    "# Noise Removal: [-0070, +0070]\n",
    "# 5000 0.673729\n",
    "# 10000 0.735876\n",
    "# 15000 0.771186\n",
    "# 20000 0.789548\n",
    "# 25000 0.809322\n",
    "# 30000 0.817797\n",
    "\n",
    "# Noise Removal: [-0080, +0080]\n",
    "# 5000 0.692029\n",
    "# 10000 0.748188\n",
    "# 15000 0.780797\n",
    "# 20000 0.815217\n",
    "# 25000 0.826087\n",
    "# 30000 0.833333\n",
    "\n",
    "# Noise Removal: [-0090, +0090]\n",
    "# 5000 0.696145\n",
    "# 10000 0.750567\n",
    "# 15000 0.773243\n",
    "# 20000 0.795918\n",
    "# 25000 0.811791\n",
    "# 30000 0.816327\n",
    "\n",
    "# Noise Removal: [-0100, +0100]\n",
    "# 5000 0.718841\n",
    "# 10000 0.75942\n",
    "# 15000 0.788406\n",
    "# 20000 0.808696\n",
    "# 25000 0.823188\n",
    "# 30000 0.84058\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using all data:\n",
    "Accuracy  66.6% on training data\n",
    "That is OK, better than random."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After noise removed, We get 77 % accuracy on training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feed_dict= {\n",
    "  feature_data: test_predictors_tf.values,\n",
    "  actual_classes: test_classes_tf.values.reshape(len(test_classes_tf.values), 2)\n",
    "}\n",
    "\n",
    "tf_confusion_metrics(model, actual_classes, sess, feed_dict)\n",
    "\n",
    "# Precision =  0.541176470588\n",
    "# Recall =  0.555555555556\n",
    "# F1 Score =  0.548271752086\n",
    "# Accuracy =  0.556725146199\n",
    "\n",
    "# After removing noise\n",
    "# Precision =  0.684210526316\n",
    "# Recall =  0.534246575342\n",
    "# F1 Score =  0.6\n",
    "# Accuracy =  0.636363636364\n",
    "\n",
    "# Noise Removal: [-0010, +0010]\n",
    "# Precision =  0.577235772358\n",
    "# Recall =  0.605113636364\n",
    "# F1 Score =  0.590846047157\n",
    "# Accuracy =  0.584507042254\n",
    "\n",
    "# Noise Removal: [-0020, +0020]\n",
    "# Precision =  0.588850174216\n",
    "# Recall =  0.603571428571\n",
    "# F1 Score =  0.596119929453\n",
    "# Accuracy =  0.605172413793\n",
    "\n",
    "# Noise Removal: [-0030, +0030]\n",
    "# Precision =  0.616740088106\n",
    "# Recall =  0.619469026549\n",
    "# F1 Score =  0.618101545254\n",
    "# Accuracy =  0.625541125541\n",
    "\n",
    "# Noise Removal: [-0040, +0040]\n",
    "# Precision =  0.622093023256\n",
    "# Recall =  0.58152173913\n",
    "# F1 Score =  0.601123595506\n",
    "# Accuracy =  0.612021857923\n",
    "\n",
    "# Noise Removal: [-0050, +0050]\n",
    "# Precision =  0.684210526316\n",
    "# Recall =  0.534246575342\n",
    "# F1 Score =  0.6\n",
    "# Accuracy =  0.636363636364\n",
    "\n",
    "# Noise Removal: [-0060, +0060]\n",
    "# Precision =  0.644736842105\n",
    "# Recall =  0.453703703704\n",
    "# F1 Score =  0.532608695652\n",
    "# Accuracy =  0.619469026549\n",
    "\n",
    "# Noise Removal: [-0070, +0070]\n",
    "# Precision =  0.666666666667\n",
    "# Recall =  0.47619047619\n",
    "# F1 Score =  0.555555555556\n",
    "# Accuracy =  0.640449438202\n",
    "\n",
    "# Noise Removal: [-0080, +0080]\n",
    "# Precision =  0.6\n",
    "# Recall =  0.406779661017\n",
    "# F1 Score =  0.484848484848\n",
    "# Accuracy =  0.630434782609\n",
    "\n",
    "# Noise Removal: [-0090, +0090]\n",
    "# Precision =  0.478260869565\n",
    "# Recall =  0.261904761905\n",
    "# F1 Score =  0.338461538462\n",
    "# Accuracy =  0.612612612613\n",
    "\n",
    "# Noise Removal: [-0100, +0100]\n",
    "# Precision =  0.588235294118\n",
    "# Recall =  0.37037037037\n",
    "# F1 Score =  0.454545454545\n",
    "# Accuracy =  0.724137931034\n",
    "\n",
    "\n",
    "\n",
    "# check rows results                         numrows   AccuOnTrainData    AccuOnTestData\n",
    "# dvislr_nr_m0010_z0000_p0010 rows =3554 - 7 = 3547    # 30000 0.691928   # Accuracy =  0.584507042254\n",
    "# dvislr_nr_m0020_z0000_p0020 rows =2904 - 7 = 2897    # 30000 0.712128   # Accuracy =  0.605172413793\n",
    "# dvislr_nr_m0030_z0000_p0030 rows =2316 - 7 = 2309    # 30000 0.738495   # Accuracy =  0.625541125541\n",
    "# dvislr_nr_m0040_z0000_p0040 rows =1837 - 7 = 1830    # 30000 0.758197   # Accuracy =  0.612021857923\n",
    "# dvislr_nr_m0050_z0000_p0050 rows =1437 - 7 = 1430    # 30000 0.768357   # Accuracy =  0.636363636364\n",
    "# dvislr_nr_m0060_z0000_p0060 rows =1135 - 7 = 1128    # 30000 0.792683   # Accuracy =  0.619469026549\n",
    "# dvislr_nr_m0070_z0000_p0070 rows = 893 - 7 =  886    # 30000 0.817797   # Accuracy =  0.640449438202\n",
    "# dvislr_nr_m0080_z0000_p0080 rows = 697 - 7 =  690    # 30000 0.833333   # Accuracy =  0.630434782609\n",
    "# dvislr_nr_m0090_z0000_p0090 rows = 559 - 7 =  552    # 30000 0.816327   # Accuracy =  0.612612612613\n",
    "# dvislr_nr_m0100_z0000_p0100 rows = 439 - 7 =  432    # 30000 0.84058    # Accuracy =  0.724137931034\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using all data, Accuracy 55.6% on test data.\n",
    "After removing noise, Accuracy 64% on test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# AFTER the model has been run, ie. trained\n",
    "# display weights\n",
    "w_86_2 = sess.run(weights)    \n",
    "print w_86_2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# response = client.put_file('/output.csv', str(pd.DataFrame(npa).to_csv()), overwrite=True)\n",
    "# response = client.put_file('/output.csv', str(               df.to_csv()), overwrite=True)\n",
    "response = client.put_file('/m01_BC_w_86_2.csv', str(pd.DataFrame(w_86_2).to_csv()), overwrite=True)\n",
    "print \"uploaded:\", response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# grab a sample input row from the test set\n",
    "sample_input_1_86 = test_predictors_tf[:10]\n",
    "# HTML(pd.DataFrame(sample_input_1_86).to_html())\n",
    "sample_input_1_86"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sample_input_1_86.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# use the weights from our model to make a prediction\n",
    "output_1_2 = np.dot(sample_input_1_86, w_86_2) > 0\n",
    "print output_1_2\n",
    "usdmxn_up = output_1_2[0,0]\n",
    "usdmxn_dn = output_1_2[0,1]\n",
    "if usdmxn_up >= usdmxn_dn:\n",
    "  usdmxn_pred = 1 # up\n",
    "else:\n",
    "  usdmxn_pred = -1 # down\n",
    "print 'usdmxn_pred = '+ str(usdmxn_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 2: Neural Network (NN): Feed-Forward with 2 Hidden Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll now build a proper feed-forward neural net with two hidden layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tensorflow session new\n",
    "sess1 = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# inputs\n",
    "num_predictors = len(training_predictors_tf.columns)\n",
    "print 'num_predictors=' + str(num_predictors)\n",
    "\n",
    "# outputs\n",
    "num_classes = len(training_classes_tf.columns)\n",
    "print 'num_classes=' + str(num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_data = tf.placeholder(\"float\", [None, num_predictors])\n",
    "actual_classes = tf.placeholder(\"float\", [None, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# layer 1\n",
    "i_01_in = 86\n",
    "i_01_out = 100\n",
    "weights1 = tf.Variable(tf.truncated_normal([i_01_in, i_01_out], stddev=0.0001))\n",
    "biases1 = tf.Variable(tf.ones([i_01_out]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# layer 2\n",
    "i_02_in = i_01_out\n",
    "i_02_out = 25\n",
    "weights2 = tf.Variable(tf.truncated_normal([i_02_in, i_02_out], stddev=0.0001))\n",
    "biases2 = tf.Variable(tf.ones([i_02_out]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# layer 3\n",
    "i_03_in = i_02_out\n",
    "i_03_out = 2\n",
    "weights3 = tf.Variable(tf.truncated_normal([i_03_in, i_03_out], stddev=0.0001))\n",
    "biases3 = tf.Variable(tf.ones([i_03_out]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# neural net\n",
    "hidden_layer_0 = feature_data\n",
    "hidden_layer_1 = tf.nn.relu(tf.matmul(hidden_layer_0, weights1) + biases1)\n",
    "hidden_layer_2 = tf.nn.relu(tf.matmul(hidden_layer_1, weights2) + biases2)\n",
    "hidden_layer_3 = tf.matmul(hidden_layer_2, weights3) + biases3\n",
    "\n",
    "model = tf.nn.softmax(hidden_layer_3)\n",
    "\n",
    "cost = -tf.reduce_sum(actual_classes*tf.log(model))\n",
    "\n",
    "train_op1 = tf.train.AdamOptimizer(learning_rate=0.0001).minimize(cost)\n",
    "\n",
    "init = tf.initialize_all_variables()\n",
    "sess1.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# BEFORE the model has been run, ie. trained\n",
    "nn_e1 = sess1.run(biases1)    \n",
    "# HTML(pd.DataFrame(nn_e1).transpose().to_html())\n",
    "pd.DataFrame(nn_e1).transpose()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# BEFORE the model has been run, ie. trained\n",
    "nn_w1 = sess1.run(weights1)    \n",
    "# HTML(pd.DataFrame(nn_w1).to_html())\n",
    "pd.DataFrame(nn_w1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# BEFORE the model has been run, ie. trained\n",
    "nn_e2 = sess1.run(biases2)    \n",
    "# HTML(pd.DataFrame(nn_e2).transpose().to_html())\n",
    "pd.DataFrame(nn_e2).transpose()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# BEFORE the model has been run, ie. trained\n",
    "# display weights\n",
    "nn_w2 = sess1.run(weights2)    \n",
    "# HTML(pd.DataFrame(nn_w2).to_html())\n",
    "pd.DataFrame(nn_w2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# BEFORE the model has been run, ie. trained\n",
    "nn_e3 = sess1.run(biases3)    \n",
    "# HTML(pd.DataFrame(nn_e3).transpose().to_html())\n",
    "pd.DataFrame(nn_e3).transpose()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# BEFORE the model has been run, ie. trained\n",
    "# display weights\n",
    "nn_w3 = sess1.run(weights3)    \n",
    "# HTML(pd.DataFrame(nn_w3).to_html())\n",
    "pd.DataFrame(nn_w3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, you'll train the model over 10,000 iterations using the full dataset each time. Every thousandth iteration, you'll assess the accuracy of the model on the training data to assess progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "correct_prediction = tf.equal(tf.argmax(model, 1), tf.argmax(actual_classes, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "num_iterations = 5001\n",
    "num_iterations_until_display = 1000\n",
    "\n",
    "for i in range(1, num_iterations):\n",
    "  sess1.run(\n",
    "    train_op1, \n",
    "    feed_dict={\n",
    "      feature_data: training_predictors_tf.values, \n",
    "      actual_classes: training_classes_tf.values.reshape(len(training_classes_tf.values), 2)\n",
    "    }\n",
    "  )\n",
    "  if i%num_iterations_until_display == 0:\n",
    "    print i, sess1.run(\n",
    "      accuracy,\n",
    "      feed_dict={\n",
    "        feature_data: training_predictors_tf.values, \n",
    "        actual_classes: training_classes_tf.values.reshape(len(training_classes_tf.values), 2)\n",
    "      }\n",
    "    )\n",
    "  \n",
    "# Original\n",
    "# 1000 0.676126\n",
    "# 2000 0.768578\n",
    "# 3000 0.83031\n",
    "# 4000 0.891164\n",
    "# 5000 0.93739\n",
    "# 6000 0.956115\n",
    "# 7000 0.972206\n",
    "# 8000 0.978935\n",
    "# 9000 0.985079\n",
    "# 10000 0.987127\n",
    "\n",
    "# Original\n",
    "# 1000 0.685489\n",
    "# 2000 0.777063\n",
    "# 3000 0.84055\n",
    "# 4000 0.902867\n",
    "# 5000 0.941486\n",
    "# 6000 0.957578\n",
    "# 7000 0.974254\n",
    "# 8000 0.980105\n",
    "# 9000 0.985079\n",
    "# 10000 0.987712\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A significant improvement in accuracy with the training data shows that the hidden layers are adding additional capacity for learning to the model.\n",
    "\n",
    "Looking at precision, recall, and accuracy, you can see a measurable improvement in performance, but certainly not a [step function](https://wikipedia.org/wiki/Step_function). This indicates that we're likely reaching the limits of this relatively simple feature set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feed_dict= {\n",
    "  feature_data: test_predictors_tf.values,\n",
    "  actual_classes: test_classes_tf.values.reshape(len(test_classes_tf.values), 2)\n",
    "}\n",
    "\n",
    "tf_confusion_metrics(model, actual_classes, sess1, feed_dict)\n",
    "\n",
    "# Original\n",
    "# Precision =  0.934731934732\n",
    "# Recall =  0.968599033816\n",
    "# F1 Score =  0.951364175563\n",
    "# Accuracy =  0.952046783626\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# AFTER the model has been run, ie. trained\n",
    "nn_e1 = sess1.run(biases1)    \n",
    "HTML(pd.DataFrame(nn_e1).transpose().to_html())\n",
    "# pd.DataFrame(nn_e1).transpose()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# response = client.put_file('/output.csv', str(pd.DataFrame(npa).to_csv()), overwrite=True)\n",
    "# response = client.put_file('/output.csv', str(               df.to_csv()), overwrite=True)\n",
    "response = client.put_file('/m02_NN_nn_e1.csv', str(pd.DataFrame(nn_e1).to_csv()), overwrite=True)\n",
    "print \"uploaded:\", response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# AFTER the model has been run, ie. trained\n",
    "# display weights\n",
    "nn_w1 = sess1.run(weights1)    \n",
    "HTML(pd.DataFrame(nn_w1).to_html())\n",
    "# pd.DataFrame(nn_w1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# AFTER the model has been run, ie. trained\n",
    "nn_e2 = sess1.run(biases2)    \n",
    "HTML(pd.DataFrame(nn_e2).transpose().to_html())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# AFTER the model has been run, ie. trained\n",
    "# display weights\n",
    "nn_w2 = sess1.run(weights2)    \n",
    "HTML(pd.DataFrame(nn_w2).to_html())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# AFTER the model has been run, ie. trained\n",
    "nn_e3 = sess1.run(biases3)    \n",
    "HTML(pd.DataFrame(nn_e3).transpose().to_html())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# AFTER the model has been run, ie. trained\n",
    "# display weights\n",
    "nn_w3 = sess1.run(weights3)    \n",
    "HTML(pd.DataFrame(nn_w3).to_html())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# response = client.put_file('/output.csv', str(pd.DataFrame(npa).to_csv()), overwrite=True)\n",
    "# response = client.put_file('/output.csv', str(               df.to_csv()), overwrite=True)\n",
    "response = client.put_file('/m02_NN_nn_e1.csv', str(pd.DataFrame(nn_e1).to_csv()), overwrite=True); print \"uploaded:\", response\n",
    "response = client.put_file('/m02_NN_nn_w1.csv', str(pd.DataFrame(nn_w1).to_csv()), overwrite=True); print \"uploaded:\", response\n",
    "response = client.put_file('/m02_NN_nn_e2.csv', str(pd.DataFrame(nn_e2).to_csv()), overwrite=True); print \"uploaded:\", response\n",
    "response = client.put_file('/m02_NN_nn_w2.csv', str(pd.DataFrame(nn_w2).to_csv()), overwrite=True); print \"uploaded:\", response\n",
    "response = client.put_file('/m02_NN_nn_e3.csv', str(pd.DataFrame(nn_e3).to_csv()), overwrite=True); print \"uploaded:\", response\n",
    "response = client.put_file('/m02_NN_nn_w3.csv', str(pd.DataFrame(nn_w3).to_csv()), overwrite=True); print \"uploaded:\", response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # AFTER the model has been run, ie. trained\n",
    "# nn_hl3 = sess1.run(hidden_layer_3)    \n",
    "# HTML(pd.DataFrame(nn_hl3).to_html())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You've covered a lot of ground. You moved from sourcing five years of financial time-series data, to munging that data into a more suitable form. You explored and visualized that data with exploratory data analysis and then decided on a machine learning model and the features for that model. You engineered those features, built a binary classifier in TensorFlow, and analyzed its performance. You built a feed forward neural net with two hidden layers in TensorFlow and analyzed its performance.\n",
    "\n",
    "How did the technology fare? It should take most people 1.5 to 3 hours to extract the juice from this solution, and none of that time is spent waiting for infrastructure or software; it's spent reading and thinking. In many organizations, it can take anywhere from days to months to do this sort of data analysis, depending on whether you need to procure any hardware. And you didn't need to do anything with infrastructure or additional software. Rather, you used a web-based console to direct GCP to set up systems on your behalf, which it did—fully managed, maintained, and supported—freeing you up to spend your time analyzing. \n",
    "\n",
    "It was also cost effective. If you took your time with this solution and spent three hours to go through it, the cost would be a few pennies. \n",
    "\n",
    "Cloud Datalab worked admirably, too. iPython/Jupyter has always been a great platform for interactive, iterative work and a fully-managed version of that platform on GCP, with connectors to other GCP technologies such as BigQuery and Google Cloud Storage, is a force multiplier for your analysis needs.  If you haven't used iPython before, this solution might have been eye opening, for you. If you're already familiar with iPython, then you'll love the connectors to other GCP technologies.\n",
    "\n",
    "Of course, R and Matlab are popular tools in machine learning, and we've made no mention either in this solution. Neither R nor Matlab are available as managed services on GCP. Both can be hosted in GCP and accessed through a cloud-friendly, web frontend.\n",
    "\n",
    "TensorFlow is a special piece of technology. It is expressive, performs well, and comes with the weight of Google's machine learning history and expertise to back it up and support it. We've only scratched the surface, but you can already see that within a handful of lines of code we've been able to write two models. Neither of them is cutting edge, by design, but neither of them is trivial either. With some additional tuning they would suit a whole spectrum of machine learning tasks. \n",
    "\n",
    "Finally, how did we do with the data analysis? We did well: over 70% accuracy in predicting the close of the S&P 500 is the highest we've seen achieved on this dataset, so with few steps and a few lines of code we've produced a full-on machine learning model. The reason for the relatively modest accuracy achieved is the dataset itself; there isn't enough signal there to do significantly better. But 7 times out of 10, we were able to correctly determine if the S&P 500 index would close up or down on the day, and that's objectively good."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**When you're finished, shut down the managed VM you used for Cloud Datalab to avoid incurring costs.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
