{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning (ML): Google Cloud Platform (GCP): TensorFlow (TF): Financial Time-Series\n",
    "# Model 01 = Binary Classifier\n",
    "# Model 02 = Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import StringIO\n",
    "\n",
    "import pandas as pd\n",
    "from pandas.tools.plotting import autocorrelation_plot\n",
    "from pandas.tools.plotting import scatter_matrix\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import gcp\n",
    "import gcp.bigquery as bq\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from IPython.core.display import HTML\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Install stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Install xlrd - required for reading excel files\n",
    "# !pip install xlrd\n",
    "!pip install xlrd\n",
    "# -----------------------------------------------------------------------------------------------\n",
    "# Downloading/unpacking xlrd\n",
    "#   Downloading xlrd-1.0.0.tar.gz (2.6MB): 2.6MB downloaded\n",
    "#   Running setup.py (path:/tmp/pip-build-brL6Op/xlrd/setup.py) egg_info for package xlrd\n",
    "    \n",
    "#     warning: no files found matching 'README.html'\n",
    "# Installing collected packages: xlrd\n",
    "#   Running setup.py install for xlrd\n",
    "#     changing mode of build/scripts-2.7/runxlrd.py from 644 to 755\n",
    "    \n",
    "#     warning: no files found matching 'README.html'\n",
    "#     changing mode of /usr/local/bin/runxlrd.py to 755\n",
    "#   Could not find .egg-info directory in install record for xlrd\n",
    "# Successfully installed xlrd\n",
    "# Cleaning up...\n",
    "# -----------------------------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Install dropbox - required for writing files to dropbox, not required for reading files from dropbox\n",
    "!pip install dropbox\n",
    "# -----------------------------------------------------------------------------------------------\n",
    "# Downloading/unpacking dropbox\n",
    "#   Downloading dropbox-6.6.2-py2-none-any.whl (261kB): 261kB downloaded\n",
    "# Requirement already satisfied (use --upgrade to upgrade): six>=1.3.0 in /usr/local/lib/python2.7/dist-packages (from dropbox)\n",
    "# Requirement already satisfied (use --upgrade to upgrade): urllib3 in /usr/lib/python2.7/dist-packages (from dropbox)\n",
    "# Downloading/unpacking requests!=2.6.1,>=2.5.1 (from dropbox)\n",
    "#   Downloading requests-2.11.0-py2.py3-none-any.whl (514kB): 514kB downloaded\n",
    "# Downloading/unpacking typing>=3.5.2 (from dropbox)\n",
    "#   Downloading typing-3.5.2.2.tar.gz (51kB): 51kB downloaded\n",
    "#   Running setup.py (path:/tmp/pip-build-xSBpJ2/typing/setup.py) egg_info for package typing\n",
    "    \n",
    "# Installing collected packages: dropbox, requests, typing\n",
    "#   Found existing installation: requests 2.4.3\n",
    "#     Not uninstalling requests at /usr/lib/python2.7/dist-packages, owned by OS\n",
    "#   Running setup.py install for typing\n",
    "    \n",
    "#   Could not find .egg-info directory in install record for typing>=3.5.2 (from dropbox)\n",
    "# Successfully installed dropbox requests typing\n",
    "# Cleaning up...\n",
    "# -----------------------------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Get the data from dropbox excel file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save file on dropbox.com\n",
    "\n",
    "fx_mxn_url_data_raw = 'https://www.dropbox.com/s/foso5ckzl9vrui0/fx_mxn_data_raw.xlsm'\n",
    "\n",
    "# force = 1 # will grab from dropbox to gcp\n",
    "force = 0 # will use if already in gcp\n",
    "already_downloaded = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import file using url to file on dropbox\n",
    "from urlparse import urlparse\n",
    "from os.path import basename\n",
    "\n",
    "fx_mxn_bn_data_raw = basename(urlparse(fx_mxn_url_data_raw).path)\n",
    "print 'fx_mxn_bn_data_raw=' + fx_mxn_bn_data_raw\n",
    "\n",
    "try:\n",
    "    already_downloaded\n",
    "except:\n",
    "    already_downloaded = False\n",
    "print 'already_downloaded=' + str(already_downloaded)\n",
    "    \n",
    "if force or not already_downloaded:\n",
    "    already_downloaded = True\n",
    "    !rm $fx_mxn_bn_data_raw\n",
    "    !wget $fx_mxn_bn_data_raw\n",
    "#!ls -l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Read the Excel file raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import xlrd\n",
    "\n",
    "# Read the excel data\n",
    "fx_mxn_df_data_raw = pd.read_excel(fx_mxn_bn_data_raw, 'data_raw')\n",
    "# data = pd.read_excel(bn_data_raw, 'data_raw', header=0, index_col=0, parse_cols=None)\n",
    "  # parse_cols=None: parse all columns\n",
    "  # header=0:    sets the row 0 as col labels (ie. headers)\n",
    "  # index_col=0: sets the col 0 as row labels (ie. index)\n",
    "fx_mxn_df_data_raw.head() # display the first few lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fx_mxn_df_data_raw.tail() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print type(fx_mxn_df_data_raw)\n",
    "# <class 'pandas.core.frame.DataFrame'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1) Write the Excel file back to dropbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Include the Dropbox SDK\n",
    "import dropbox"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1.1) Method 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Get your app key and secret from the Dropbox developer website\n",
    "# app_key = '9zgzo4d4s8tco7c'\n",
    "# app_secret = '5iq9409uedmichl'\n",
    "\n",
    "# flow = dropbox.client.DropboxOAuth2FlowNoRedirect(app_key, app_secret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# authorize_url = flow.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print '1. Go to: ' + authorize_url\n",
    "# print '2. Click \"Allow\" (you might have to log in first)'\n",
    "# print '3. Copy the authorization code.'\n",
    "# code = raw_input(\"Enter the authorization code here: \").strip()\n",
    "\n",
    "# # output\n",
    "# # 1. Go to: https://www.dropbox.com/1/oauth2/authorize?response_type=code&client_id=9zgzo4d4s8tco7c\n",
    "# # 2. Click \"Allow\" (you might have to log in first)\n",
    "# # 3. Copy the authorization code.\n",
    "# # Enter the authorization code here: Z0hHmWIPD4AAAAAAAAAAE2kOeZ0UPKiQi9eeQFEyY1s\n",
    "# # Enter the authorization code here: Z0hHmWIPD4AAAAAAAAAAFTmf6b0e8U4eOd96tCcbILs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This will fail if the user enters an invalid authorization code\n",
    "# access_token, user_id = flow.finish(code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# client = dropbox.client.DropboxClient(access_token)\n",
    "# print 'linked account: ', client.account_info()\n",
    "\n",
    "# # linked account:  {u'referral_link': u'https://db.tt/wTxYqge4', u'display_name': u'Eyup Saltik', u'uid': 576395988, \n",
    "# # u'locale': u'en', u'email_verified': True, u'email': u'eyup.saltik.2016@gmail.com', u'is_paired': False, \n",
    "# # u'team': None, u'name_details': {u'familiar_name': u'Eyup', u'surname': u'Saltik', u'given_name': u'Eyup'}, \n",
    "# # u'country': u'SG', u'quota_info': {u'datastores': 0, u'shared': 0, u'quota': 2147483648, u'normal': 40119203}}\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# response = client.put_file('/working-final.csv', str(data_raw))\n",
    "# print \"uploaded:\", response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1.2) Method 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "s_dropbox_access_token = 'Z0hHmWIPD4AAAAAAAAAADKBYofFVGTUnVcva8HKUpKCj6SDaY15WNHIZcRcEknoO'\n",
    "client = dropbox.client.DropboxClient(s_dropbox_access_token)\n",
    "print 'linked account: ', client.account_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "response = client.put_file('/fx_mxn_data_raw_from_GCP.csv', str(fx_mxn_df_data_raw.to_csv()), overwrite=True)\n",
    "print \"uploaded:\", response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Select data df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = fx_mxn_df_data_raw\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6) df_ticker_desc = Just a table to map column id to the ticker and name of the variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_ticker_desc = df.ix[:1] # 1st 2 rows\n",
    "df_ticker_desc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7) df_values = date + col 1:87"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_values = df.ix[2:] # row 2 onwards\n",
    "df_values.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8) Set the dates as index of the dataframe = 2000-01-03 to 2016-05-27 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_values_indexed = df_values.set_index([0])\n",
    "df_values_indexed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_values_indexed.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9) Fill any gaps in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Pandas includes a very convenient function for filling gaps in the data.\n",
    "df_values_indexed = df_values_indexed.fillna(method='ffill')\n",
    "df_values_indexed.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory data analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploratory Data Analysis (EDA) is foundational to working with machine learning, and any other sort of analysis. EDA means getting to know your data, getting your fingers dirty with your data, feeling it and seeing it. The end result is you know your data very well, so when you build models you build them based on an actual, practical, physical understanding of the data, not assumptions or vaguely held notions. You can still make assumptions of course, but EDA means you will understand your assumptions and why you're making those assumptions. \n",
    "\n",
    "First, take a look at the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10) Describe the data briefly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_values_indexed.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that the various indices operate on scales differing by orders of magnitude. It's best to scale the data so that, for example, operations involving multiple indices aren't unduly influenced by a single, massive index.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11) Plot the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# N.B. A super-useful trick-ette is to assign the return value of plot to _ \n",
    "# so that you don't get text printed before the plot itself.\n",
    "\n",
    "# _ = pd.concat([data_values_indexed[1],\n",
    "#   data_values_indexed[1070],\n",
    "#   data_values_indexed[788],\n",
    "#   data_values_indexed[926]], axis=1).plot(figsize=(20, 15))\n",
    "\n",
    "_ = df_values_indexed.plot(figsize=(20, 15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the structure isn't uniformly visible for the indices. Divide each value in an individual index by the maximum value for that index., and then replot. The maximum value of all indices will be 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12) Calculate the max value for each column, prepare to scale data for each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_values_indexed_max = df_values_indexed.max(axis=0) # max across axis 0 = rows\n",
    "df_values_indexed_max.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13) Scale the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_values_indexed_scaled = df_values_indexed / df_values_indexed_max\n",
    "df_values_indexed_scaled.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14) Plot the scaled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "_ = df_values_indexed_scaled.plot(figsize=(20, 15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shortform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dvis = df_values_indexed_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15) Auto-correlations\n",
    "\n",
    "Next, plot autocorrelations for each of the indices. The autocorrelations determine correlations between current values of the index and lagged values of the same index. The goal is to determine whether the lagged values are reliable indicators of the current values. If they are, then we've identified a correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "fig.set_figwidth(20)\n",
    "fig.set_figheight(10)\n",
    "\n",
    "_ = autocorrelation_plot(dvis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For 2300 lagged days, we observe positive auto-correlations.\n",
    "This suggests that as the variables increase, they tend to keep on increasing. Momentum.\n",
    "\n",
    "After 2300 lagged days, we observe negative auto-correlations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16) Skip: Just a reminder of the PCA columns we selected earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# list_cols_pca = [ 0,1,\n",
    "#                   1070,788,926,112,69,574,654,1160,527,323,\n",
    "#                   397,118,774,1028,1034,655,907,736,251,388,\n",
    "#                   327,243,705,303,1146,467,136,1006,600,15,\n",
    "#                   231,290,131,782,20,1048,630,1173,431,856,\n",
    "#                   67,299,838,639,53,932,870,938,1061\n",
    "#                 ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 17) Scatter plots of the first 20 of our 86 variables vs USDMXN(varid=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# _ = scatter_matrix(data_values_indexed_scaled)\n",
    "# dvis = df_values_indexed_scaled\n",
    "\n",
    "_ = scatter_matrix(\n",
    "      pd.concat(\n",
    "      [ \n",
    "        dvis[ 1],dvis[ 2],dvis[ 3],dvis[ 4],dvis[ 5],dvis[ 6],dvis[ 7],dvis[ 8],dvis[ 9],dvis[10],\n",
    "        dvis[11],dvis[12],dvis[13],dvis[14],dvis[15],dvis[16],dvis[17],dvis[18],dvis[19],dvis[20],\n",
    "      ], axis=1), figsize=(15, 15), diagonal='kde')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 18) Remind ourselves what our scaled data (of price or index LEVELS) looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dvis.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 19) Calculate Log Returns on our scaled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_values_indexed_scaled_logret = pd.DataFrame()\n",
    "df_values_indexed_scaled_logret = np.log(dvis/dvis.shift(-1)) # note dates are reverse-chrono\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shortform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dvislr = df_values_indexed_scaled_logret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dvislr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dvislr.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dvislr.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 2000/01/07 to 2016/05/27 = 1 header row + 4279 data rows\n",
    "# print dvislr.head()\n",
    "# print dvislr.tail()\n",
    "print 'NumRowsIncludeHeader = len(dvislr) = ' + str(len(dvislr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 20) Replace inf, NaN in data with Zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dvislr = dvislr.replace([np.inf, -np.inf, np.nan], 0)\n",
    "dvislr.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dvislr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dvislr.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 21) Skip: Fill the Gaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Pandas includes a very convenient function for filling gaps in the data.\n",
    "# dvislr = dvislr.fillna(method='ffill')\n",
    "# dvislr.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 22) Plot log returns of scaled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "_ = dvislr.plot(figsize=(20, 15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 23) Auto-Correlations of log returns of scaled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "fig.set_figwidth(20)\n",
    "fig.set_figheight(10)\n",
    "\n",
    "_ = autocorrelation_plot(dvislr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no auto-correlations, so we are good."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 24) Skip: Just a reminder of the PCA50 Columns we selected earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# list_cols_pca = [ 0,1,\n",
    "#                   1070,788,926,112,69,574,654,1160,527,323,\n",
    "#                   397,118,774,1028,1034,655,907,736,251,388,\n",
    "#                   327,243,705,303,1146,467,136,1006,600,15,\n",
    "#                   231,290,131,782,20,1048,630,1173,431,856,\n",
    "#                   67,299,838,639,53,932,870,938,1061\n",
    "#                 ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 25) Scatter plots of log returns of first 20 of the 86 variables where log-returns of USDMXN(varid=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# _ = scatter_matrix(data_values_indexed_scaled_logret)   # takes long time, becareful, save work first\n",
    "# dvislr = df_values_indexed_scaled_logret\n",
    "\n",
    "_ = scatter_matrix(\n",
    "      pd.concat(\n",
    "      [ \n",
    "        dvislr[ 1],dvislr[ 2],dvislr[ 3],dvislr[ 4],dvislr[ 5],dvislr[ 6],dvislr[ 7],dvislr[ 8],dvislr[ 9],dvislr[10],\n",
    "        dvislr[11],dvislr[12],dvislr[13],dvislr[14],dvislr[15],dvislr[16],dvislr[17],dvislr[18],dvislr[19],dvislr[20],\n",
    "      ], axis=1), figsize=(15, 15), diagonal='kde')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summing up the EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, you've done a good enough job of exploratory data analysis. You've visualized our data and come to know it better. \n",
    "You've transformed it into a form that is useful for modelling, log returns, and looked at how indices relate to each other. \n",
    "\n",
    "What should we think so far?\n",
    "\n",
    "Cloud Datalab is working great. With just a few lines of code, you were able to munge the data, visualize the changes, and make decisions. You could easily analyze and iterate. This is a common feature of iPython, but the advantage here is that Cloud Datalab is a managed service that you can simply click and use, so you can focus on your analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we can see a model:\n",
    "\n",
    "* We'll predict whether the USDMXN close today will be higher or lower than yesterday.\n",
    "\n",
    "Predicting whether the log return of the USDMXN is positive or negative is a classification problem. \n",
    "That is, we want to choose one option from a finite set of options, in this case positive or negative. \n",
    "This is the base case of classification where we have only two values to choose from, known as binary classification, or logistic regression.\n",
    "\n",
    "Machine learning models are very good at finding weak signals from data.\n",
    "In machine learning, as in most things, there are subtle tradeoffs happening, but in general good data is better than good algorithms, which are better than good frameworks. \n",
    "You need all three pillars but in that order of importance: data, algorithms, frameworks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[TensorFlow](https://tensorflow.org) is an open source software library, initiated by Google, for numerical computation using data flow graphs. TensorFlow is based on Google's machine learning expertise and is the next generation framework used internally at Google for tasks such as translation and image recognition. It's a wonderful framework for machine learning because it's expressive, efficient, and easy to use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature engineering for TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From a training and testing perspective, time series data is easy. Training data should come from events that happened before test data events, and be contiguous in time.  Otherwise,  your model would be trained on events from \"the future\", at least as compared to the test data. It would then likely perform badly in practice, because you can’t really have access to data from the future. That means random sampling or cross validation don't apply to time series data. Decide on a training-versus-testing split, and divide your data into training and test datasets.\n",
    "\n",
    "In this case, you'll create the features together with two additional columns:\n",
    "\n",
    "* usdmxn_logret_positive, which is 1 if the log return of the USDMXN close is positive, and 0 otherwise. \n",
    "* usdmxn_logret_negative, which is 1 if the log return of the USDMXN close is negative, and 1 otherwise. \n",
    "\n",
    "We'll use 80% of our data for training and 20% for testing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 1: Binary Classification (BC) = Logistic Regression (LR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1: Step 01: Indicator Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 00: Original: Initialize indicator columns to 0\n",
    "dvislr['usdmxn_Tp1_logret_positive'] = 0 # col 88 \n",
    "dvislr['usdmxn_Tp1_logret_negative'] = 0 # col 89\n",
    "\n",
    "dvislr['usdmxn_Tp0_logret_positive'] = 0 # col 90\n",
    "dvislr['usdmxn_Tp0_logret_negative'] = 0 # col 91\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dvislr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 00: Populate results columns according to actual usdmxn returns (positive or negative)\n",
    "\n",
    "# Predicting T+1: use col 1\n",
    "dvislr.ix[dvislr[1] >= 0, 'usdmxn_Tp1_logret_positive'] = 1\n",
    "dvislr.ix[dvislr[1] <  0, 'usdmxn_Tp1_logret_negative'] = 1\n",
    "\n",
    "# Predicting T+0: use col 2\n",
    "dvislr.ix[dvislr[2] >= 0, 'usdmxn_Tp0_logret_positive'] = 1\n",
    "dvislr.ix[dvislr[2] <  0, 'usdmxn_Tp0_logret_negative'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 01: Noise Cancellation\n",
    "# dvislr['usdmxn_Tp0_logret_positive_0050bp'] = 0\n",
    "# dvislr['usdmxn_Tp0_logret_negative_0050bp'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 01: Populate results columns according to actual usdmxn returns (outside noise zone above or below)\n",
    "# dvislr.ix[dvislr[2] >= 0.0050, 'usdmxn_Tp0_logret_positive_0050bp'] = 1\n",
    "# dvislr.ix[dvislr[2] <  -0.0050, 'usdmxn_Tp0_logret_negative_0050bp'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # 02: Stratification\n",
    "# dvislr['usdmxn_Tp0_logret_+0150bp_and_above'] = 0; dvislr['usdmxn_Tp0_logret_+0150bp_and_below'] = 0;\n",
    "# dvislr['usdmxn_Tp0_logret_+0140bp_and_above'] = 0; dvislr['usdmxn_Tp0_logret_+0140bp_and_below'] = 0;\n",
    "# dvislr['usdmxn_Tp0_logret_+0130bp_and_above'] = 0; dvislr['usdmxn_Tp0_logret_+0130bp_and_below'] = 0;\n",
    "# dvislr['usdmxn_Tp0_logret_+0120bp_and_above'] = 0; dvislr['usdmxn_Tp0_logret_+0120bp_and_below'] = 0;\n",
    "# dvislr['usdmxn_Tp0_logret_+0110bp_and_above'] = 0; dvislr['usdmxn_Tp0_logret_+0110bp_and_below'] = 0;\n",
    "# dvislr['usdmxn_Tp0_logret_+0100bp_and_above'] = 0; dvislr['usdmxn_Tp0_logret_+0100bp_and_below'] = 0;\n",
    "# dvislr['usdmxn_Tp0_logret_+0090bp_and_above'] = 0; dvislr['usdmxn_Tp0_logret_+0090bp_and_below'] = 0;\n",
    "# dvislr['usdmxn_Tp0_logret_+0080bp_and_above'] = 0; dvislr['usdmxn_Tp0_logret_+0080bp_and_below'] = 0;\n",
    "# dvislr['usdmxn_Tp0_logret_+0070bp_and_above'] = 0; dvislr['usdmxn_Tp0_logret_+0070bp_and_below'] = 0;\n",
    "# dvislr['usdmxn_Tp0_logret_+0060bp_and_above'] = 0; dvislr['usdmxn_Tp0_logret_+0060bp_and_below'] = 0;\n",
    "# dvislr['usdmxn_Tp0_logret_+0050bp_and_above'] = 0; dvislr['usdmxn_Tp0_logret_+0050bp_and_below'] = 0;\n",
    "# dvislr['usdmxn_Tp0_logret_+0040bp_and_above'] = 0; dvislr['usdmxn_Tp0_logret_+0040bp_and_below'] = 0;\n",
    "# dvislr['usdmxn_Tp0_logret_+0030bp_and_above'] = 0; dvislr['usdmxn_Tp0_logret_+0030bp_and_below'] = 0;\n",
    "# dvislr['usdmxn_Tp0_logret_+0020bp_and_above'] = 0; dvislr['usdmxn_Tp0_logret_+0020bp_and_below'] = 0;\n",
    "# dvislr['usdmxn_Tp0_logret_+0010bp_and_above'] = 0; dvislr['usdmxn_Tp0_logret_+0010bp_and_below'] = 0;\n",
    "# dvislr['usdmxn_Tp0_logret_+0000bp_and_above'] = 0; dvislr['usdmxn_Tp0_logret_+0000bp_and_below'] = 0;\n",
    "# dvislr['usdmxn_Tp0_logret_-0010bp_and_above'] = 0; dvislr['usdmxn_Tp0_logret_-0010bp_and_below'] = 0;\n",
    "# dvislr['usdmxn_Tp0_logret_-0020bp_and_above'] = 0; dvislr['usdmxn_Tp0_logret_-0020bp_and_below'] = 0;\n",
    "# dvislr['usdmxn_Tp0_logret_-0030bp_and_above'] = 0; dvislr['usdmxn_Tp0_logret_-0030bp_and_below'] = 0;\n",
    "# dvislr['usdmxn_Tp0_logret_-0040bp_and_above'] = 0; dvislr['usdmxn_Tp0_logret_-0040bp_and_below'] = 0;\n",
    "# dvislr['usdmxn_Tp0_logret_-0050bp_and_above'] = 0; dvislr['usdmxn_Tp0_logret_-0050bp_and_below'] = 0;\n",
    "# dvislr['usdmxn_Tp0_logret_-0060bp_and_above'] = 0; dvislr['usdmxn_Tp0_logret_-0060bp_and_below'] = 0;\n",
    "# dvislr['usdmxn_Tp0_logret_-0070bp_and_above'] = 0; dvislr['usdmxn_Tp0_logret_-0070bp_and_below'] = 0;\n",
    "# dvislr['usdmxn_Tp0_logret_-0080bp_and_above'] = 0; dvislr['usdmxn_Tp0_logret_-0080bp_and_below'] = 0;\n",
    "# dvislr['usdmxn_Tp0_logret_-0090bp_and_above'] = 0; dvislr['usdmxn_Tp0_logret_-0090bp_and_below'] = 0;\n",
    "# dvislr['usdmxn_Tp0_logret_-0100bp_and_above'] = 0; dvislr['usdmxn_Tp0_logret_-0100bp_and_below'] = 0;\n",
    "# dvislr['usdmxn_Tp0_logret_-0110bp_and_above'] = 0; dvislr['usdmxn_Tp0_logret_-0110bp_and_below'] = 0;\n",
    "# dvislr['usdmxn_Tp0_logret_-0120bp_and_above'] = 0; dvislr['usdmxn_Tp0_logret_-0120bp_and_below'] = 0;\n",
    "# dvislr['usdmxn_Tp0_logret_-0130bp_and_above'] = 0; dvislr['usdmxn_Tp0_logret_-0130bp_and_below'] = 0;\n",
    "# dvislr['usdmxn_Tp0_logret_-0140bp_and_above'] = 0; dvislr['usdmxn_Tp0_logret_-0140bp_and_below'] = 0;\n",
    "# dvislr['usdmxn_Tp0_logret_-0150bp_and_above'] = 0; dvislr['usdmxn_Tp0_logret_-0150bp_and_below'] = 0;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 02: Tp1 Stratification Initialize\n",
    "# WIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 02: Tp0 Stratification: Populate \n",
    "# WIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 02: Tp0 Stratification: Initialize\n",
    "for d_bp in xrange(-150,160,10): # -150:10:+150\n",
    "  d = d_bp / 10000.00\n",
    "  s_Tp0_above = 'usdmxn_Tp0_logret_' + '{0:+05d}'.format(d_bp) + 'bp_and_above'\n",
    "  s_Tp0_below = 'usdmxn_Tp0_logret_' + '{0:+05d}'.format(d_bp) + 'bp_and_below'\n",
    "  # print d\n",
    "  # print s_above\n",
    "  # print s_below\n",
    "  dvislr[s_Tp0_above] = 0;\n",
    "  dvislr[s_Tp0_below] = 0;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 02: Tp0 Stratification: Populate results columns according to actual usdmxn returns (above or below each level)\n",
    "for d_bp in xrange(-150,160,10): # -150:10:+150\n",
    "  d = d_bp / 10000.00\n",
    "  s_Tp0_above = 'usdmxn_Tp0_logret_' + '{0:+05d}'.format(d_bp) + 'bp_and_above'\n",
    "  s_Tp0_below = 'usdmxn_Tp0_logret_' + '{0:+05d}'.format(d_bp) + 'bp_and_below'\n",
    "  # print d\n",
    "  # print s_above\n",
    "  # print s_below\n",
    "  dvislr.ix[dvislr[1] >= d, s_Tp0_above] = 1\n",
    "  dvislr.ix[dvislr[1] <  d, s_Tp0_below] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dvislr.head()\n",
    "# 87 inputs + 4 user cols + (15+1+15)*2=62   = 155 cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dvislr.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1: Step 01.01: Remove data noise in a stratified way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# dvislr = data_values_indexed_scaled_logret\n",
    "# dvislrnr = data_values_indexed_scaled_logret (noise removed)\n",
    "\n",
    "# Example Filter: df[(df.A == 1) & (df.D == 6)]\n",
    "dvislr_nr_m0010_z0000_p0010 = dvislr[ (dvislr['usdmxn_Tp0_logret_-0010bp_and_below'] == 1) | (dvislr['usdmxn_Tp0_logret_+0010bp_and_above'] == 1) ] # remove data between [-0.10%, +0.10%], center at +0.00%\n",
    "dvislr_nr_m0020_z0000_p0020 = dvislr[ (dvislr['usdmxn_Tp0_logret_-0020bp_and_below'] == 1) | (dvislr['usdmxn_Tp0_logret_+0020bp_and_above'] == 1) ] # remove data between [-0.10%, +0.10%], center at +0.00%\n",
    "dvislr_nr_m0030_z0000_p0030 = dvislr[ (dvislr['usdmxn_Tp0_logret_-0030bp_and_below'] == 1) | (dvislr['usdmxn_Tp0_logret_+0030bp_and_above'] == 1) ] # remove data between [-0.10%, +0.10%], center at +0.00%\n",
    "dvislr_nr_m0040_z0000_p0040 = dvislr[ (dvislr['usdmxn_Tp0_logret_-0040bp_and_below'] == 1) | (dvislr['usdmxn_Tp0_logret_+0040bp_and_above'] == 1) ] # remove data between [-0.10%, +0.10%], center at +0.00%\n",
    "dvislr_nr_m0050_z0000_p0050 = dvislr[ (dvislr['usdmxn_Tp0_logret_-0050bp_and_below'] == 1) | (dvislr['usdmxn_Tp0_logret_+0050bp_and_above'] == 1) ] # remove data between [-0.50%, +0.50%], center at +0.00%\n",
    "dvislr_nr_m0060_z0000_p0060 = dvislr[ (dvislr['usdmxn_Tp0_logret_-0060bp_and_below'] == 1) | (dvislr['usdmxn_Tp0_logret_+0060bp_and_above'] == 1) ] # remove data between [-0.50%, +0.50%], center at +0.00%\n",
    "dvislr_nr_m0070_z0000_p0070 = dvislr[ (dvislr['usdmxn_Tp0_logret_-0070bp_and_below'] == 1) | (dvislr['usdmxn_Tp0_logret_+0070bp_and_above'] == 1) ] # remove data between [-0.50%, +0.50%], center at +0.00%\n",
    "dvislr_nr_m0080_z0000_p0080 = dvislr[ (dvislr['usdmxn_Tp0_logret_-0080bp_and_below'] == 1) | (dvislr['usdmxn_Tp0_logret_+0080bp_and_above'] == 1) ] # remove data between [-0.50%, +0.50%], center at +0.00%\n",
    "dvislr_nr_m0090_z0000_p0090 = dvislr[ (dvislr['usdmxn_Tp0_logret_-0090bp_and_below'] == 1) | (dvislr['usdmxn_Tp0_logret_+0090bp_and_above'] == 1) ] # remove data between [-0.50%, +0.50%], center at +0.00%\n",
    "dvislr_nr_m0100_z0000_p0100 = dvislr[ (dvislr['usdmxn_Tp0_logret_-0100bp_and_below'] == 1) | (dvislr['usdmxn_Tp0_logret_+0100bp_and_above'] == 1) ] # remove data between [-0.50%, +0.50%], center at +0.00%\n",
    "\n",
    "# check rows\n",
    "print 'dvislr_nr_m0010_z0000_p0010 rows =' + str(len(dvislr_nr_m0010_z0000_p0010))\n",
    "print 'dvislr_nr_m0020_z0000_p0020 rows =' + str(len(dvislr_nr_m0020_z0000_p0020))\n",
    "print 'dvislr_nr_m0030_z0000_p0030 rows =' + str(len(dvislr_nr_m0030_z0000_p0030))\n",
    "print 'dvislr_nr_m0040_z0000_p0040 rows =' + str(len(dvislr_nr_m0040_z0000_p0040))\n",
    "print 'dvislr_nr_m0050_z0000_p0050 rows =' + str(len(dvislr_nr_m0050_z0000_p0050))\n",
    "print 'dvislr_nr_m0060_z0000_p0060 rows =' + str(len(dvislr_nr_m0060_z0000_p0060))\n",
    "print 'dvislr_nr_m0070_z0000_p0070 rows =' + str(len(dvislr_nr_m0070_z0000_p0070))\n",
    "print 'dvislr_nr_m0080_z0000_p0080 rows =' + str(len(dvislr_nr_m0080_z0000_p0080))\n",
    "print 'dvislr_nr_m0090_z0000_p0090 rows =' + str(len(dvislr_nr_m0090_z0000_p0090))\n",
    "print 'dvislr_nr_m0100_z0000_p0100 rows =' + str(len(dvislr_nr_m0100_z0000_p0100))\n",
    "\n",
    "# check rows results\n",
    "# dvislr_nr_m0010_z0000_p0010 rows =3553\n",
    "# dvislr_nr_m0020_z0000_p0020 rows =2903\n",
    "# dvislr_nr_m0030_z0000_p0030 rows =2315\n",
    "# dvislr_nr_m0040_z0000_p0040 rows =1836\n",
    "# dvislr_nr_m0050_z0000_p0050 rows =1436\n",
    "# dvislr_nr_m0060_z0000_p0060 rows =1134\n",
    "# dvislr_nr_m0070_z0000_p0070 rows =893\n",
    "# dvislr_nr_m0080_z0000_p0080 rows =697\n",
    "# dvislr_nr_m0090_z0000_p0090 rows =559\n",
    "# dvislr_nr_m0100_z0000_p0100 rows =439"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1: Step 02: Split data into training, test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### training test data empty shell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# training_test_data\n",
    "# col 001-002 = 02 cols = Tp1 up dn indicators of results\n",
    "# col 003-004 = 02 cols = Tp0 up dn indicators of results\n",
    "# col 005-066 = 62 cols = (15+1+15)*2\n",
    "# col 067-153 = 87 cols = 87 inputs\n",
    "training_test_data = pd.DataFrame(\n",
    "  columns=[\n",
    "    # 'usdmxn_Tp0_logret_positive',        'usdmxn_Tp0_logret_negative',\n",
    "    # 'usdmxn_Tp0_logret_positive_0050bp', 'usdmxn_Tp0_logret_negative_0050bp',\n",
    "\n",
    "    'bc_Tp1_up', \n",
    "    'bc_Tp1_dn',\n",
    "    \n",
    "    'bc_Tp0_up', \n",
    "    'bc_Tp0_dn',\n",
    "    \n",
    "    'usdmxn_Tp0_logret_-0150bp_and_above', 'usdmxn_Tp0_logret_-0150bp_and_below',\n",
    "    'usdmxn_Tp0_logret_-0140bp_and_above', 'usdmxn_Tp0_logret_-0140bp_and_below',\n",
    "    'usdmxn_Tp0_logret_-0130bp_and_above', 'usdmxn_Tp0_logret_-0130bp_and_below',\n",
    "    'usdmxn_Tp0_logret_-0120bp_and_above', 'usdmxn_Tp0_logret_-0120bp_and_below',\n",
    "    'usdmxn_Tp0_logret_-0110bp_and_above', 'usdmxn_Tp0_logret_-0110bp_and_below',\n",
    "    'usdmxn_Tp0_logret_-0100bp_and_above', 'usdmxn_Tp0_logret_-0100bp_and_below',\n",
    "    'usdmxn_Tp0_logret_-0090bp_and_above', 'usdmxn_Tp0_logret_-0090bp_and_below',\n",
    "    'usdmxn_Tp0_logret_-0080bp_and_above', 'usdmxn_Tp0_logret_-0080bp_and_below',\n",
    "    'usdmxn_Tp0_logret_-0070bp_and_above', 'usdmxn_Tp0_logret_-0070bp_and_below',\n",
    "    'usdmxn_Tp0_logret_-0060bp_and_above', 'usdmxn_Tp0_logret_-0060bp_and_below',\n",
    "    'usdmxn_Tp0_logret_-0050bp_and_above', 'usdmxn_Tp0_logret_-0050bp_and_below',\n",
    "    'usdmxn_Tp0_logret_-0040bp_and_above', 'usdmxn_Tp0_logret_-0040bp_and_below',\n",
    "    'usdmxn_Tp0_logret_-0030bp_and_above', 'usdmxn_Tp0_logret_-0030bp_and_below',\n",
    "    'usdmxn_Tp0_logret_-0020bp_and_above', 'usdmxn_Tp0_logret_-0020bp_and_below',\n",
    "    'usdmxn_Tp0_logret_-0010bp_and_above', 'usdmxn_Tp0_logret_-0010bp_and_below',\n",
    "    'usdmxn_Tp0_logret_+0000bp_and_above', 'usdmxn_Tp0_logret_+0000bp_and_below',\n",
    "    'usdmxn_Tp0_logret_+0010bp_and_above', 'usdmxn_Tp0_logret_+0010bp_and_below',\n",
    "    'usdmxn_Tp0_logret_+0020bp_and_above', 'usdmxn_Tp0_logret_+0020bp_and_below',\n",
    "    'usdmxn_Tp0_logret_+0030bp_and_above', 'usdmxn_Tp0_logret_+0030bp_and_below',\n",
    "    'usdmxn_Tp0_logret_+0040bp_and_above', 'usdmxn_Tp0_logret_+0040bp_and_below',\n",
    "    'usdmxn_Tp0_logret_+0050bp_and_above', 'usdmxn_Tp0_logret_+0050bp_and_below',\n",
    "    'usdmxn_Tp0_logret_+0060bp_and_above', 'usdmxn_Tp0_logret_+0060bp_and_below',\n",
    "    'usdmxn_Tp0_logret_+0070bp_and_above', 'usdmxn_Tp0_logret_+0070bp_and_below',\n",
    "    'usdmxn_Tp0_logret_+0080bp_and_above', 'usdmxn_Tp0_logret_+0080bp_and_below',\n",
    "    'usdmxn_Tp0_logret_+0090bp_and_above', 'usdmxn_Tp0_logret_+0090bp_and_below',\n",
    "    'usdmxn_Tp0_logret_+0100bp_and_above', 'usdmxn_Tp0_logret_+0100bp_and_below',\n",
    "    'usdmxn_Tp0_logret_+0110bp_and_above', 'usdmxn_Tp0_logret_+0110bp_and_below',\n",
    "    'usdmxn_Tp0_logret_+0120bp_and_above', 'usdmxn_Tp0_logret_+0120bp_and_below',\n",
    "    'usdmxn_Tp0_logret_+0130bp_and_above', 'usdmxn_Tp0_logret_+0130bp_and_below',\n",
    "    'usdmxn_Tp0_logret_+0140bp_and_above', 'usdmxn_Tp0_logret_+0140bp_and_below',\n",
    "    'usdmxn_Tp0_logret_+0150bp_and_above', 'usdmxn_Tp0_logret_+0150bp_and_below',\n",
    "\n",
    "    '1','2','3','4','5','6','7','8','9','10',\n",
    "    '11','12','13','14','15','16','17','18','19','20',\n",
    "    '21','22','23','24','25','26','27','28','29','30',\n",
    "    '31','32','33','34','35','36','37','38','39','40',\n",
    "    '41','42','43','44','45','46','47','48','49','50',\n",
    "    '51','52','53','54','55','56','57','58','59','60',\n",
    "    '61','62','63','64','65','66','67','68','69','70',\n",
    "    '71','72','73','74','75','76','77','78','79','80',\n",
    "    '81','82','83','84','85','86','87'\n",
    "  ])\n",
    "\n",
    "training_test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(dvislr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data into variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bc_Tp1_up</th>\n",
       "      <th>bc_Tp1_dn</th>\n",
       "      <th>bc_Tp0_up</th>\n",
       "      <th>bc_Tp0_dn</th>\n",
       "      <th>usdmxn_Tp0_logret_-0150bp_and_above</th>\n",
       "      <th>usdmxn_Tp0_logret_-0150bp_and_below</th>\n",
       "      <th>usdmxn_Tp0_logret_-0140bp_and_above</th>\n",
       "      <th>usdmxn_Tp0_logret_-0140bp_and_below</th>\n",
       "      <th>usdmxn_Tp0_logret_-0130bp_and_above</th>\n",
       "      <th>usdmxn_Tp0_logret_-0130bp_and_below</th>\n",
       "      <th>...</th>\n",
       "      <th>78</th>\n",
       "      <th>79</th>\n",
       "      <th>80</th>\n",
       "      <th>81</th>\n",
       "      <th>82</th>\n",
       "      <th>83</th>\n",
       "      <th>84</th>\n",
       "      <th>85</th>\n",
       "      <th>86</th>\n",
       "      <th>87</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.029968</td>\n",
       "      <td>0.029456</td>\n",
       "      <td>0.222493</td>\n",
       "      <td>0.128977</td>\n",
       "      <td>0.062618</td>\n",
       "      <td>0.027276</td>\n",
       "      <td>0.023740</td>\n",
       "      <td>0.021459</td>\n",
       "      <td>0.020401</td>\n",
       "      <td>-0.003376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.005383</td>\n",
       "      <td>-0.005290</td>\n",
       "      <td>-0.012917</td>\n",
       "      <td>-0.007126</td>\n",
       "      <td>-0.006120</td>\n",
       "      <td>0.008725</td>\n",
       "      <td>0.006062</td>\n",
       "      <td>0.004955</td>\n",
       "      <td>0.004396</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012770</td>\n",
       "      <td>0.012063</td>\n",
       "      <td>0.092960</td>\n",
       "      <td>0.050359</td>\n",
       "      <td>0.022079</td>\n",
       "      <td>0.024121</td>\n",
       "      <td>0.021459</td>\n",
       "      <td>0.020217</td>\n",
       "      <td>0.018647</td>\n",
       "      <td>-0.000321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.019095</td>\n",
       "      <td>-0.019707</td>\n",
       "      <td>-0.109928</td>\n",
       "      <td>-0.060954</td>\n",
       "      <td>-0.033573</td>\n",
       "      <td>-0.023105</td>\n",
       "      <td>-0.020661</td>\n",
       "      <td>-0.020121</td>\n",
       "      <td>-0.018647</td>\n",
       "      <td>0.000803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015641</td>\n",
       "      <td>0.014382</td>\n",
       "      <td>-0.084643</td>\n",
       "      <td>-0.027146</td>\n",
       "      <td>-0.004751</td>\n",
       "      <td>0.013842</td>\n",
       "      <td>0.011033</td>\n",
       "      <td>0.009937</td>\n",
       "      <td>0.008563</td>\n",
       "      <td>-0.001204</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 153 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   bc_Tp1_up  bc_Tp1_dn  bc_Tp0_up  bc_Tp0_dn  \\\n",
       "0        0.0        1.0        1.0        0.0   \n",
       "1        1.0        0.0        0.0        1.0   \n",
       "2        0.0        1.0        1.0        0.0   \n",
       "3        1.0        0.0        1.0        0.0   \n",
       "4        1.0        0.0        0.0        1.0   \n",
       "\n",
       "   usdmxn_Tp0_logret_-0150bp_and_above  usdmxn_Tp0_logret_-0150bp_and_below  \\\n",
       "0                                  NaN                                  NaN   \n",
       "1                                  NaN                                  NaN   \n",
       "2                                  NaN                                  NaN   \n",
       "3                                  NaN                                  NaN   \n",
       "4                                  NaN                                  NaN   \n",
       "\n",
       "   usdmxn_Tp0_logret_-0140bp_and_above  usdmxn_Tp0_logret_-0140bp_and_below  \\\n",
       "0                                  NaN                                  NaN   \n",
       "1                                  NaN                                  NaN   \n",
       "2                                  NaN                                  NaN   \n",
       "3                                  NaN                                  NaN   \n",
       "4                                  NaN                                  NaN   \n",
       "\n",
       "   usdmxn_Tp0_logret_-0130bp_and_above  usdmxn_Tp0_logret_-0130bp_and_below  \\\n",
       "0                                  NaN                                  NaN   \n",
       "1                                  NaN                                  NaN   \n",
       "2                                  NaN                                  NaN   \n",
       "3                                  NaN                                  NaN   \n",
       "4                                  NaN                                  NaN   \n",
       "\n",
       "     ...           78        79        80        81        82        83  \\\n",
       "0    ...     0.029968  0.029456  0.222493  0.128977  0.062618  0.027276   \n",
       "1    ...    -0.005383 -0.005290 -0.012917 -0.007126 -0.006120  0.008725   \n",
       "2    ...     0.012770  0.012063  0.092960  0.050359  0.022079  0.024121   \n",
       "3    ...    -0.019095 -0.019707 -0.109928 -0.060954 -0.033573 -0.023105   \n",
       "4    ...     0.015641  0.014382 -0.084643 -0.027146 -0.004751  0.013842   \n",
       "\n",
       "         84        85        86        87  \n",
       "0  0.023740  0.021459  0.020401 -0.003376  \n",
       "1  0.006062  0.004955  0.004396  0.000000  \n",
       "2  0.021459  0.020217  0.018647 -0.000321  \n",
       "3 -0.020661 -0.020121 -0.018647  0.000803  \n",
       "4  0.011033  0.009937  0.008563 -0.001204  \n",
       "\n",
       "[5 rows x 153 columns]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# row 0      = header\n",
    "# row 1-4279 = data\n",
    "# NumRowsIncludeHeader = len(data_values_indexed_scaled_logret) = 4280 = 0-4279\n",
    "# Start from row 7, so we can have up to (7-1) lookback days\n",
    "\n",
    "# check rows results\n",
    "# dvislr_nr_m0010_z0000_p0010 rows =3554\n",
    "# dvislr_nr_m0020_z0000_p0020 rows =2904\n",
    "# dvislr_nr_m0030_z0000_p0030 rows =2316\n",
    "# dvislr_nr_m0040_z0000_p0040 rows =1837\n",
    "# dvislr_nr_m0050_z0000_p0050 rows =1437\n",
    "# dvislr_nr_m0060_z0000_p0060 rows =1135\n",
    "# dvislr_nr_m0070_z0000_p0070 rows =893\n",
    "# dvislr_nr_m0080_z0000_p0080 rows =697\n",
    "# dvislr_nr_m0090_z0000_p0090 rows =559\n",
    "# dvislr_nr_m0100_z0000_p0100 rows =439\n",
    "\n",
    "#==========================================================================================\n",
    "# Select the data\n",
    "#==========================================================================================\n",
    "# dvislr =  dvislr                          # default\n",
    "# dvislr =  dvislr_nr_m0010_z0000_p0010\n",
    "# dvislr =  dvislr_nr_m0020_z0000_p0020\n",
    "# dvislr =  dvislr_nr_m0030_z0000_p0030\n",
    "# dvislr =  dvislr_nr_m0040_z0000_p0040\n",
    "# dvislr =  dvislr_nr_m0050_z0000_p0050\n",
    "# dvislr =  dvislr_nr_m0060_z0000_p0060\n",
    "# dvislr =  dvislr_nr_m0070_z0000_p0070\n",
    "# dvislr =  dvislr_nr_m0080_z0000_p0080\n",
    "# dvislr =  dvislr_nr_m0090_z0000_p0090\n",
    "# dvislr =  dvislr_nr_m0100_z0000_p0100\n",
    "\n",
    "# for row i\n",
    "for i in range(7, len(dvislr)): # = range(7, 4280) =  [7, 4280) = [7,4279] \n",
    "\n",
    "  #==========================================================================================\n",
    "  usdmxn_Tp1_logret_positive = dvislr['usdmxn_Tp1_logret_positive'].ix[i] # col 1\n",
    "  usdmxn_Tp1_logret_negative = dvislr['usdmxn_Tp1_logret_negative'].ix[i] # col 2\n",
    "  usdmxn_Tp0_logret_positive = dvislr['usdmxn_Tp0_logret_positive'].ix[i] # col 3\n",
    "  usdmxn_Tp0_logret_negative = dvislr['usdmxn_Tp0_logret_negative'].ix[i] # col 4\n",
    "  #==========================================================================================\n",
    "  bc_Tp1_up = usdmxn_Tp1_logret_positive; # Tp1 up # col 1\n",
    "  bc_Tp1_dn = usdmxn_Tp1_logret_negative; # Tp1 dn # col 2\n",
    "  bc_Tp0_up = usdmxn_Tp0_logret_positive; # Tp0 up # col 3\n",
    "  bc_Tp0_dn = usdmxn_Tp0_logret_negative; # Tp0 dn # col 4\n",
    "  #==========================================================================================\n",
    "\n",
    "  # 02: stratification  \n",
    "  usdmxn_Tp0_logret_p0150bp_and_below = dvislr['usdmxn_Tp0_logret_+0150bp_and_below'].ix[i]; usdmxn_Tp0_logret_p0150bp_and_above = dvislr['usdmxn_Tp0_logret_+0150bp_and_above'].ix[i];\n",
    "  usdmxn_Tp0_logret_p0140bp_and_below = dvislr['usdmxn_Tp0_logret_+0140bp_and_below'].ix[i]; usdmxn_Tp0_logret_p0140bp_and_above = dvislr['usdmxn_Tp0_logret_+0140bp_and_above'].ix[i];\n",
    "  usdmxn_Tp0_logret_p0130bp_and_below = dvislr['usdmxn_Tp0_logret_+0130bp_and_below'].ix[i]; usdmxn_Tp0_logret_p0130bp_and_above = dvislr['usdmxn_Tp0_logret_+0130bp_and_above'].ix[i];\n",
    "  usdmxn_Tp0_logret_p0120bp_and_below = dvislr['usdmxn_Tp0_logret_+0120bp_and_below'].ix[i]; usdmxn_Tp0_logret_p0120bp_and_above = dvislr['usdmxn_Tp0_logret_+0120bp_and_above'].ix[i];\n",
    "  usdmxn_Tp0_logret_p0110bp_and_below = dvislr['usdmxn_Tp0_logret_+0110bp_and_below'].ix[i]; usdmxn_Tp0_logret_p0110bp_and_above = dvislr['usdmxn_Tp0_logret_+0110bp_and_above'].ix[i];\n",
    "  \n",
    "  usdmxn_Tp0_logret_p0100bp_and_below = dvislr['usdmxn_Tp0_logret_+0100bp_and_below'].ix[i]; usdmxn_Tp0_logret_p0100bp_and_above = dvislr['usdmxn_Tp0_logret_+0100bp_and_above'].ix[i];\n",
    "  usdmxn_Tp0_logret_p0090bp_and_below = dvislr['usdmxn_Tp0_logret_+0090bp_and_below'].ix[i]; usdmxn_Tp0_logret_p0090bp_and_above = dvislr['usdmxn_Tp0_logret_+0090bp_and_above'].ix[i];\n",
    "  usdmxn_Tp0_logret_p0080bp_and_below = dvislr['usdmxn_Tp0_logret_+0080bp_and_below'].ix[i]; usdmxn_Tp0_logret_p0080bp_and_above = dvislr['usdmxn_Tp0_logret_+0080bp_and_above'].ix[i];\n",
    "  usdmxn_Tp0_logret_p0070bp_and_below = dvislr['usdmxn_Tp0_logret_+0070bp_and_below'].ix[i]; usdmxn_Tp0_logret_p0070bp_and_above = dvislr['usdmxn_Tp0_logret_+0070bp_and_above'].ix[i];\n",
    "  usdmxn_Tp0_logret_p0060bp_and_below = dvislr['usdmxn_Tp0_logret_+0060bp_and_below'].ix[i]; usdmxn_Tp0_logret_p0060bp_and_above = dvislr['usdmxn_Tp0_logret_+0060bp_and_above'].ix[i];\n",
    "\n",
    "  usdmxn_Tp0_logret_p0050bp_and_below = dvislr['usdmxn_Tp0_logret_+0050bp_and_below'].ix[i]; usdmxn_Tp0_logret_p0050bp_and_above = dvislr['usdmxn_Tp0_logret_+0050bp_and_above'].ix[i];\n",
    "  usdmxn_Tp0_logret_p0040bp_and_below = dvislr['usdmxn_Tp0_logret_+0040bp_and_below'].ix[i]; usdmxn_Tp0_logret_p0040bp_and_above = dvislr['usdmxn_Tp0_logret_+0040bp_and_above'].ix[i];\n",
    "  usdmxn_Tp0_logret_p0030bp_and_below = dvislr['usdmxn_Tp0_logret_+0030bp_and_below'].ix[i]; usdmxn_Tp0_logret_p0030bp_and_above = dvislr['usdmxn_Tp0_logret_+0030bp_and_above'].ix[i];\n",
    "  usdmxn_Tp0_logret_p0020bp_and_below = dvislr['usdmxn_Tp0_logret_+0020bp_and_below'].ix[i]; usdmxn_Tp0_logret_p0020bp_and_above = dvislr['usdmxn_Tp0_logret_+0020bp_and_above'].ix[i];\n",
    "  usdmxn_Tp0_logret_p0010bp_and_below = dvislr['usdmxn_Tp0_logret_+0010bp_and_below'].ix[i]; usdmxn_Tp0_logret_p0010bp_and_above = dvislr['usdmxn_Tp0_logret_+0010bp_and_above'].ix[i];\n",
    "\n",
    "  usdmxn_Tp0_logret_p0000bp_and_below = dvislr['usdmxn_Tp0_logret_+0000bp_and_below'].ix[i]; usdmxn_Tp0_logret_p0000bp_and_above = dvislr['usdmxn_Tp0_logret_+0000bp_and_above'].ix[i];\n",
    "\n",
    "  usdmxn_Tp0_logret_m0010bp_and_below = dvislr['usdmxn_Tp0_logret_-0010bp_and_below'].ix[i]; usdmxn_Tp0_logret_m0010bp_and_above = dvislr['usdmxn_Tp0_logret_-0010bp_and_above'].ix[i];\n",
    "  usdmxn_Tp0_logret_m0020bp_and_below = dvislr['usdmxn_Tp0_logret_-0020bp_and_below'].ix[i]; usdmxn_Tp0_logret_m0020bp_and_above = dvislr['usdmxn_Tp0_logret_-0020bp_and_above'].ix[i];\n",
    "  usdmxn_Tp0_logret_m0030bp_and_below = dvislr['usdmxn_Tp0_logret_-0030bp_and_below'].ix[i]; usdmxn_Tp0_logret_m0030bp_and_above = dvislr['usdmxn_Tp0_logret_-0030bp_and_above'].ix[i];\n",
    "  usdmxn_Tp0_logret_m0040bp_and_below = dvislr['usdmxn_Tp0_logret_-0040bp_and_below'].ix[i]; usdmxn_Tp0_logret_m0040bp_and_above = dvislr['usdmxn_Tp0_logret_-0040bp_and_above'].ix[i];\n",
    "  usdmxn_Tp0_logret_m0050bp_and_below = dvislr['usdmxn_Tp0_logret_-0050bp_and_below'].ix[i]; usdmxn_Tp0_logret_m0050bp_and_above = dvislr['usdmxn_Tp0_logret_-0050bp_and_above'].ix[i];\n",
    "\n",
    "  usdmxn_Tp0_logret_m0060bp_and_below = dvislr['usdmxn_Tp0_logret_-0060bp_and_below'].ix[i]; usdmxn_Tp0_logret_m0060bp_and_above = dvislr['usdmxn_Tp0_logret_-0060bp_and_above'].ix[i];\n",
    "  usdmxn_Tp0_logret_m0070bp_and_below = dvislr['usdmxn_Tp0_logret_-0070bp_and_below'].ix[i]; usdmxn_Tp0_logret_m0070bp_and_above = dvislr['usdmxn_Tp0_logret_-0070bp_and_above'].ix[i];\n",
    "  usdmxn_Tp0_logret_m0080bp_and_below = dvislr['usdmxn_Tp0_logret_-0080bp_and_below'].ix[i]; usdmxn_Tp0_logret_m0080bp_and_above = dvislr['usdmxn_Tp0_logret_-0080bp_and_above'].ix[i];\n",
    "  usdmxn_Tp0_logret_m0090bp_and_below = dvislr['usdmxn_Tp0_logret_-0090bp_and_below'].ix[i]; usdmxn_Tp0_logret_m0090bp_and_above = dvislr['usdmxn_Tp0_logret_-0090bp_and_above'].ix[i];\n",
    "  usdmxn_Tp0_logret_m0100bp_and_below = dvislr['usdmxn_Tp0_logret_-0100bp_and_below'].ix[i]; usdmxn_Tp0_logret_m0100bp_and_above = dvislr['usdmxn_Tp0_logret_-0100bp_and_above'].ix[i];\n",
    "\n",
    "  usdmxn_Tp0_logret_m0110bp_and_below = dvislr['usdmxn_Tp0_logret_-0110bp_and_below'].ix[i]; usdmxn_Tp0_logret_m0110bp_and_above = dvislr['usdmxn_Tp0_logret_-0110bp_and_above'].ix[i];\n",
    "  usdmxn_Tp0_logret_m0120bp_and_below = dvislr['usdmxn_Tp0_logret_-0120bp_and_below'].ix[i]; usdmxn_Tp0_logret_m0120bp_and_above = dvislr['usdmxn_Tp0_logret_-0120bp_and_above'].ix[i];\n",
    "  usdmxn_Tp0_logret_m0130bp_and_below = dvislr['usdmxn_Tp0_logret_-0130bp_and_below'].ix[i]; usdmxn_Tp0_logret_m0130bp_and_above = dvislr['usdmxn_Tp0_logret_-0130bp_and_above'].ix[i];\n",
    "  usdmxn_Tp0_logret_m0140bp_and_below = dvislr['usdmxn_Tp0_logret_-0140bp_and_below'].ix[i]; usdmxn_Tp0_logret_m0140bp_and_above = dvislr['usdmxn_Tp0_logret_-0140bp_and_above'].ix[i];\n",
    "  usdmxn_Tp0_logret_m0150bp_and_below = dvislr['usdmxn_Tp0_logret_-0150bp_and_below'].ix[i]; usdmxn_Tp0_logret_m0150bp_and_above = dvislr['usdmxn_Tp0_logret_-0150bp_and_above'].ix[i];\n",
    "\n",
    "  #==========================================================================================  \n",
    "#   bc_Tp0_dn = usdmxn_Tp0_logret_m0010bp_and_below; bc_Tp0_up = usdmxn_Tp0_logret_p0010bp_and_above;\n",
    "#   bc_Tp0_dn = usdmxn_Tp0_logret_m0020bp_and_below; bc_Tp0_up = usdmxn_Tp0_logret_p0020bp_and_above;\n",
    "#   bc_Tp0_dn = usdmxn_Tp0_logret_m0030bp_and_below; bc_Tp0_up = usdmxn_Tp0_logret_p0030bp_and_above;\n",
    "#   bc_Tp0_dn = usdmxn_Tp0_logret_m0040bp_and_below; bc_Tp0_up = usdmxn_Tp0_logret_p0040bp_and_above;\n",
    "#   bc_Tp0_dn = usdmxn_Tp0_logret_m0050bp_and_below; bc_Tp0_up = usdmxn_Tp0_logret_p0050bp_and_above;\n",
    "#   bc_Tp0_dn = usdmxn_Tp0_logret_m0060bp_and_below; bc_Tp0_up = usdmxn_Tp0_logret_p0060bp_and_above;\n",
    "#   bc_Tp0_dn = usdmxn_Tp0_logret_m0070bp_and_below; bc_Tp0_up = usdmxn_Tp0_logret_p0070bp_and_above;\n",
    "#   bc_Tp0_dn = usdmxn_Tp0_logret_m0080bp_and_below; bc_Tp0_up = usdmxn_Tp0_logret_p0080bp_and_above;\n",
    "#   bc_Tp0_dn = usdmxn_Tp0_logret_m0090bp_and_below; bc_Tp0_up = usdmxn_Tp0_logret_p0090bp_and_above;\n",
    "#   bc_Tp0_dn = usdmxn_Tp0_logret_m0100bp_and_below; bc_Tp0_up = usdmxn_Tp0_logret_p0100bp_and_above;\n",
    "  \n",
    "  #   v_001_Tm005 = dvislr['1'].ix[i-5]  # lookback 1d\n",
    "  # col 067 = dvislr[1]\n",
    "  # col 068 = dvislr[2]\n",
    "  # ...\n",
    "  # col 153 = dvislr[87]\n",
    "  \n",
    "  v_001_Tm000 = dvislr[1].ix[i];  v_002_Tm000 = dvislr[2].ix[i];  v_003_Tm000 = dvislr[3].ix[i];  v_004_Tm000 = dvislr[4].ix[i];  v_005_Tm000 = dvislr[5].ix[i];\n",
    "  v_006_Tm000 = dvislr[6].ix[i];  v_007_Tm000 = dvislr[7].ix[i];  v_008_Tm000 = dvislr[8].ix[i];  v_009_Tm000 = dvislr[9].ix[i];  v_010_Tm000 = dvislr[10].ix[i];\n",
    "  v_011_Tm000 = dvislr[11].ix[i]; v_012_Tm000 = dvislr[12].ix[i]; v_013_Tm000 = dvislr[13].ix[i]; v_014_Tm000 = dvislr[14].ix[i]; v_015_Tm000 = dvislr[15].ix[i];\n",
    "  v_016_Tm000 = dvislr[16].ix[i]; v_017_Tm000 = dvislr[17].ix[i]; v_018_Tm000 = dvislr[18].ix[i]; v_019_Tm000 = dvislr[19].ix[i]; v_020_Tm000 = dvislr[20].ix[i];  \n",
    "  v_021_Tm000 = dvislr[21].ix[i]; v_022_Tm000 = dvislr[22].ix[i]; v_023_Tm000 = dvislr[23].ix[i]; v_024_Tm000 = dvislr[24].ix[i]; v_025_Tm000 = dvislr[25].ix[i];\n",
    "  v_026_Tm000 = dvislr[26].ix[i]; v_027_Tm000 = dvislr[27].ix[i]; v_028_Tm000 = dvislr[28].ix[i]; v_029_Tm000 = dvislr[29].ix[i]; v_030_Tm000 = dvislr[30].ix[i];\n",
    "  v_031_Tm000 = dvislr[31].ix[i]; v_032_Tm000 = dvislr[32].ix[i]; v_033_Tm000 = dvislr[33].ix[i]; v_034_Tm000 = dvislr[34].ix[i]; v_035_Tm000 = dvislr[35].ix[i];\n",
    "  v_036_Tm000 = dvislr[36].ix[i]; v_037_Tm000 = dvislr[37].ix[i]; v_038_Tm000 = dvislr[38].ix[i]; v_039_Tm000 = dvislr[39].ix[i]; v_040_Tm000 = dvislr[40].ix[i];\n",
    "  v_041_Tm000 = dvislr[41].ix[i]; v_042_Tm000 = dvislr[42].ix[i]; v_043_Tm000 = dvislr[43].ix[i]; v_044_Tm000 = dvislr[44].ix[i]; v_045_Tm000 = dvislr[45].ix[i];\n",
    "  v_046_Tm000 = dvislr[46].ix[i]; v_047_Tm000 = dvislr[47].ix[i]; v_048_Tm000 = dvislr[48].ix[i]; v_049_Tm000 = dvislr[49].ix[i]; v_050_Tm000 = dvislr[50].ix[i];\n",
    "  v_051_Tm000 = dvislr[51].ix[i]; v_052_Tm000 = dvislr[52].ix[i]; v_053_Tm000 = dvislr[53].ix[i]; v_054_Tm000 = dvislr[54].ix[i]; v_055_Tm000 = dvislr[55].ix[i];\n",
    "  v_056_Tm000 = dvislr[56].ix[i]; v_057_Tm000 = dvislr[57].ix[i]; v_058_Tm000 = dvislr[58].ix[i]; v_059_Tm000 = dvislr[59].ix[i]; v_060_Tm000 = dvislr[60].ix[i];\n",
    "  v_061_Tm000 = dvislr[61].ix[i]; v_062_Tm000 = dvislr[62].ix[i]; v_063_Tm000 = dvislr[63].ix[i]; v_064_Tm000 = dvislr[64].ix[i]; v_065_Tm000 = dvislr[65].ix[i];\n",
    "  v_066_Tm000 = dvislr[66].ix[i]; v_067_Tm000 = dvislr[67].ix[i]; v_068_Tm000 = dvislr[68].ix[i]; v_069_Tm000 = dvislr[69].ix[i]; v_070_Tm000 = dvislr[70].ix[i];\n",
    "  v_071_Tm000 = dvislr[71].ix[i]; v_072_Tm000 = dvislr[72].ix[i]; v_073_Tm000 = dvislr[73].ix[i]; v_074_Tm000 = dvislr[74].ix[i]; v_075_Tm000 = dvislr[75].ix[i];\n",
    "  v_076_Tm000 = dvislr[76].ix[i]; v_077_Tm000 = dvislr[77].ix[i]; v_078_Tm000 = dvislr[78].ix[i]; v_079_Tm000 = dvislr[79].ix[i]; v_080_Tm000 = dvislr[80].ix[i];\n",
    "  v_081_Tm000 = dvislr[81].ix[i]; v_082_Tm000 = dvislr[82].ix[i]; v_083_Tm000 = dvislr[83].ix[i]; v_084_Tm000 = dvislr[84].ix[i]; v_085_Tm000 = dvislr[85].ix[i];\n",
    "  v_086_Tm000 = dvislr[86].ix[i]; v_087_Tm000 = dvislr[87].ix[i];\n",
    "\n",
    "  # training_test_data: append data\n",
    "  training_test_data = training_test_data.append(\n",
    "    {\n",
    "#       'usdmxn_Tp0_logret_positive':usdmxn_Tp0_logret_positive,\n",
    "#       'usdmxn_Tp0_logret_negative':usdmxn_Tp0_logret_negative,\n",
    "#       'usdmxn_Tp0_logret_positive_0050bp':usdmxn_Tp0_logret_positive_0050bp,\n",
    "#       'usdmxn_Tp0_logret_negative_0050bp':usdmxn_Tp0_logret_negative_0050bp,\n",
    "      \n",
    "      'bc_Tp1_up':bc_Tp1_up,\n",
    "      'bc_Tp1_dn':bc_Tp1_dn,\n",
    "      \n",
    "      'bc_Tp0_up':bc_Tp0_up,\n",
    "      'bc_Tp0_dn':bc_Tp0_dn,\n",
    "      \n",
    "      # '1':v_001_Tm000 = USDMXN Curncy_Tp1\n",
    "      # '2':v_002_Tm000 = USDMXN Curncy_Tp0\n",
    "      \n",
    "                        '2':v_002_Tm000, '3':v_003_Tm000, '4':v_004_Tm000, '5':v_005_Tm000,\n",
    "      '6':v_006_Tm000,  '7':v_007_Tm000, '8':v_008_Tm000, '9':v_009_Tm000, '10':v_010_Tm000,\n",
    "\n",
    "      '11':v_011_Tm000, '12':v_012_Tm000, '13':v_013_Tm000, '14':v_014_Tm000, '15':v_015_Tm000,\n",
    "      '16':v_016_Tm000, '17':v_017_Tm000, '18':v_018_Tm000, '19':v_019_Tm000, '20':v_020_Tm000,\n",
    "\n",
    "      '21':v_021_Tm000, '22':v_022_Tm000, '23':v_023_Tm000, '24':v_024_Tm000, '25':v_025_Tm000,\n",
    "      '26':v_026_Tm000, '27':v_027_Tm000, '28':v_028_Tm000, '29':v_029_Tm000, '30':v_030_Tm000,\n",
    "\n",
    "      '31':v_031_Tm000, '32':v_032_Tm000, '33':v_033_Tm000, '34':v_034_Tm000, '35':v_035_Tm000,\n",
    "      '36':v_036_Tm000, '37':v_037_Tm000, '38':v_038_Tm000, '39':v_039_Tm000, '40':v_040_Tm000,\n",
    "\n",
    "      '41':v_041_Tm000, '42':v_042_Tm000, '43':v_043_Tm000, '44':v_044_Tm000, '45':v_045_Tm000,\n",
    "      '46':v_046_Tm000, '47':v_047_Tm000, '48':v_048_Tm000, '49':v_049_Tm000, '50':v_050_Tm000,\n",
    "\n",
    "      '51':v_051_Tm000, '52':v_052_Tm000, '53':v_053_Tm000, '54':v_054_Tm000, '55':v_055_Tm000,\n",
    "      '56':v_056_Tm000, '57':v_057_Tm000, '58':v_058_Tm000, '59':v_059_Tm000, '60':v_060_Tm000,\n",
    "\n",
    "      '61':v_061_Tm000, '62':v_062_Tm000, '63':v_063_Tm000, '64':v_064_Tm000, '65':v_065_Tm000,\n",
    "      '66':v_066_Tm000, '67':v_067_Tm000, '68':v_068_Tm000, '69':v_069_Tm000, '70':v_070_Tm000,\n",
    "\n",
    "      '71':v_071_Tm000, '72':v_072_Tm000, '73':v_073_Tm000, '74':v_074_Tm000, '75':v_075_Tm000,\n",
    "      '76':v_076_Tm000, '77':v_077_Tm000, '78':v_078_Tm000, '79':v_079_Tm000, '80':v_080_Tm000,\n",
    "      \n",
    "      '81':v_081_Tm000, '82':v_082_Tm000, '83':v_083_Tm000, '84':v_084_Tm000, '85':v_085_Tm000,\n",
    "      '86':v_086_Tm000, '87':v_087_Tm000, \n",
    "      \n",
    "    },\n",
    "    ignore_index=True)\n",
    "  \n",
    "# data_values_indexed_scaled_logret: row [7,4279] \n",
    "# training_test_data               : row [0,4272] = 4273 rows\n",
    "# training_test_data: col 01-02 = 02 cols = binary outputs\n",
    "# training_test_data: col 03-89 = 87 cols = inputs  \n",
    "\n",
    "training_test_data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bc_Tp1_up</th>\n",
       "      <th>bc_Tp1_dn</th>\n",
       "      <th>bc_Tp0_up</th>\n",
       "      <th>bc_Tp0_dn</th>\n",
       "      <th>usdmxn_Tp0_logret_-0150bp_and_above</th>\n",
       "      <th>usdmxn_Tp0_logret_-0150bp_and_below</th>\n",
       "      <th>usdmxn_Tp0_logret_-0140bp_and_above</th>\n",
       "      <th>usdmxn_Tp0_logret_-0140bp_and_below</th>\n",
       "      <th>usdmxn_Tp0_logret_-0130bp_and_above</th>\n",
       "      <th>usdmxn_Tp0_logret_-0130bp_and_below</th>\n",
       "      <th>...</th>\n",
       "      <th>78</th>\n",
       "      <th>79</th>\n",
       "      <th>80</th>\n",
       "      <th>81</th>\n",
       "      <th>82</th>\n",
       "      <th>83</th>\n",
       "      <th>84</th>\n",
       "      <th>85</th>\n",
       "      <th>86</th>\n",
       "      <th>87</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4268</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.004005</td>\n",
       "      <td>-0.002007</td>\n",
       "      <td>-0.022728</td>\n",
       "      <td>-0.011173</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.005398</td>\n",
       "      <td>-0.004040</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.004046</td>\n",
       "      <td>0.003066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4269</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004005</td>\n",
       "      <td>0.003348</td>\n",
       "      <td>0.011300</td>\n",
       "      <td>0.011173</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.005369</td>\n",
       "      <td>-0.005362</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.004030</td>\n",
       "      <td>0.001536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4270</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006711</td>\n",
       "      <td>0.004032</td>\n",
       "      <td>-0.022473</td>\n",
       "      <td>-0.022223</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.009415</td>\n",
       "      <td>0.009402</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.008075</td>\n",
       "      <td>-0.004602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4271</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.004032</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.045462</td>\n",
       "      <td>0.044951</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.008075</td>\n",
       "      <td>-0.008065</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.005391</td>\n",
       "      <td>-0.005698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4272</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 153 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      bc_Tp1_up  bc_Tp1_dn  bc_Tp0_up  bc_Tp0_dn  \\\n",
       "4268        0.0        1.0        0.0        1.0   \n",
       "4269        0.0        1.0        1.0        0.0   \n",
       "4270        1.0        0.0        0.0        1.0   \n",
       "4271        0.0        1.0        1.0        0.0   \n",
       "4272        1.0        0.0        1.0        0.0   \n",
       "\n",
       "      usdmxn_Tp0_logret_-0150bp_and_above  \\\n",
       "4268                                  NaN   \n",
       "4269                                  NaN   \n",
       "4270                                  NaN   \n",
       "4271                                  NaN   \n",
       "4272                                  NaN   \n",
       "\n",
       "      usdmxn_Tp0_logret_-0150bp_and_below  \\\n",
       "4268                                  NaN   \n",
       "4269                                  NaN   \n",
       "4270                                  NaN   \n",
       "4271                                  NaN   \n",
       "4272                                  NaN   \n",
       "\n",
       "      usdmxn_Tp0_logret_-0140bp_and_above  \\\n",
       "4268                                  NaN   \n",
       "4269                                  NaN   \n",
       "4270                                  NaN   \n",
       "4271                                  NaN   \n",
       "4272                                  NaN   \n",
       "\n",
       "      usdmxn_Tp0_logret_-0140bp_and_below  \\\n",
       "4268                                  NaN   \n",
       "4269                                  NaN   \n",
       "4270                                  NaN   \n",
       "4271                                  NaN   \n",
       "4272                                  NaN   \n",
       "\n",
       "      usdmxn_Tp0_logret_-0130bp_and_above  \\\n",
       "4268                                  NaN   \n",
       "4269                                  NaN   \n",
       "4270                                  NaN   \n",
       "4271                                  NaN   \n",
       "4272                                  NaN   \n",
       "\n",
       "      usdmxn_Tp0_logret_-0130bp_and_below    ...           78        79  \\\n",
       "4268                                  NaN    ...    -0.004005 -0.002007   \n",
       "4269                                  NaN    ...     0.004005  0.003348   \n",
       "4270                                  NaN    ...     0.006711  0.004032   \n",
       "4271                                  NaN    ...    -0.004032  0.000000   \n",
       "4272                                  NaN    ...     0.000000  0.000000   \n",
       "\n",
       "            80        81   82        83        84   85        86        87  \n",
       "4268 -0.022728 -0.011173  0.0 -0.005398 -0.004040  0.0 -0.004046  0.003066  \n",
       "4269  0.011300  0.011173  0.0 -0.005369 -0.005362  0.0 -0.004030  0.001536  \n",
       "4270 -0.022473 -0.022223  0.0  0.009415  0.009402  0.0  0.008075 -0.004602  \n",
       "4271  0.045462  0.044951  0.0 -0.008075 -0.008065  0.0 -0.005391 -0.005698  \n",
       "4272  0.000000  0.000000  0.0  0.000000  0.000000  0.0  0.000000  0.000000  \n",
       "\n",
       "[5 rows x 153 columns]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_test_data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bc_Tp1_up</th>\n",
       "      <th>bc_Tp1_dn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4273.000000</td>\n",
       "      <td>4273.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.495905</td>\n",
       "      <td>0.504095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.500042</td>\n",
       "      <td>0.500042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         bc_Tp1_up    bc_Tp1_dn\n",
       "count  4273.000000  4273.000000\n",
       "mean      0.495905     0.504095\n",
       "std       0.500042     0.500042\n",
       "min       0.000000     0.000000\n",
       "25%       0.000000     0.000000\n",
       "50%       0.000000     1.000000\n",
       "75%       1.000000     1.000000\n",
       "max       1.000000     1.000000"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training_test_data.describe()\n",
    "training_test_data[['bc_Tp1_up','bc_Tp1_dn']].describe()\n",
    "# training_test_data[['bc_Tp0_up','bc_Tp0_dn']].describe()\n",
    "\n",
    "# check rows results\n",
    "# dvislr_nr_m0010_z0000_p0010 rows =3554 - 7 = 3547\n",
    "# dvislr_nr_m0020_z0000_p0020 rows =2904 - 7 = 2897\n",
    "# dvislr_nr_m0030_z0000_p0030 rows =2316 - 7 = 2309\n",
    "# dvislr_nr_m0040_z0000_p0040 rows =1837 - 7 = 1830\n",
    "# dvislr_nr_m0050_z0000_p0050 rows =1437 - 7 = 1430\n",
    "# dvislr_nr_m0060_z0000_p0060 rows =1135 - 7 = 1128\n",
    "# dvislr_nr_m0070_z0000_p0070 rows = 893 - 7 =  886\n",
    "# dvislr_nr_m0080_z0000_p0080 rows = 697 - 7 =  690\n",
    "# dvislr_nr_m0090_z0000_p0090 rows = 559 - 7 =  552\n",
    "# dvislr_nr_m0100_z0000_p0100 rows = 439 - 7 =  432"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create the training and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>78</th>\n",
       "      <th>79</th>\n",
       "      <th>80</th>\n",
       "      <th>81</th>\n",
       "      <th>82</th>\n",
       "      <th>83</th>\n",
       "      <th>84</th>\n",
       "      <th>85</th>\n",
       "      <th>86</th>\n",
       "      <th>87</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4273.000000</td>\n",
       "      <td>4273.000000</td>\n",
       "      <td>4273.000000</td>\n",
       "      <td>4273.000000</td>\n",
       "      <td>4273.000000</td>\n",
       "      <td>4273.000000</td>\n",
       "      <td>4273.000000</td>\n",
       "      <td>4273.000000</td>\n",
       "      <td>4273.000000</td>\n",
       "      <td>4273.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>4273.000000</td>\n",
       "      <td>4273.000000</td>\n",
       "      <td>4273.000000</td>\n",
       "      <td>4273.000000</td>\n",
       "      <td>4273.000000</td>\n",
       "      <td>4273.000000</td>\n",
       "      <td>4273.000000</td>\n",
       "      <td>4273.000000</td>\n",
       "      <td>4273.000000</td>\n",
       "      <td>4273.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.000153</td>\n",
       "      <td>0.000155</td>\n",
       "      <td>0.000702</td>\n",
       "      <td>0.000568</td>\n",
       "      <td>0.000041</td>\n",
       "      <td>0.000619</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>-0.000026</td>\n",
       "      <td>0.000202</td>\n",
       "      <td>0.000106</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000297</td>\n",
       "      <td>-0.000292</td>\n",
       "      <td>-0.001013</td>\n",
       "      <td>-0.000442</td>\n",
       "      <td>0.003848</td>\n",
       "      <td>-0.000315</td>\n",
       "      <td>-0.000301</td>\n",
       "      <td>-0.000260</td>\n",
       "      <td>-0.000289</td>\n",
       "      <td>0.000054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.006769</td>\n",
       "      <td>0.006770</td>\n",
       "      <td>0.020769</td>\n",
       "      <td>0.018350</td>\n",
       "      <td>0.004673</td>\n",
       "      <td>0.041773</td>\n",
       "      <td>0.000858</td>\n",
       "      <td>0.010168</td>\n",
       "      <td>0.007852</td>\n",
       "      <td>0.026645</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014881</td>\n",
       "      <td>0.014750</td>\n",
       "      <td>0.099855</td>\n",
       "      <td>0.149762</td>\n",
       "      <td>0.305913</td>\n",
       "      <td>0.016318</td>\n",
       "      <td>0.015439</td>\n",
       "      <td>0.015259</td>\n",
       "      <td>0.015197</td>\n",
       "      <td>0.004519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-0.066527</td>\n",
       "      <td>-0.066527</td>\n",
       "      <td>-0.614143</td>\n",
       "      <td>-0.101624</td>\n",
       "      <td>-0.190591</td>\n",
       "      <td>-0.837715</td>\n",
       "      <td>-0.007372</td>\n",
       "      <td>-0.166175</td>\n",
       "      <td>-0.123921</td>\n",
       "      <td>-0.385704</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.136928</td>\n",
       "      <td>-0.131822</td>\n",
       "      <td>-3.135494</td>\n",
       "      <td>-3.277145</td>\n",
       "      <td>-3.624341</td>\n",
       "      <td>-0.112705</td>\n",
       "      <td>-0.105936</td>\n",
       "      <td>-0.114583</td>\n",
       "      <td>-0.109933</td>\n",
       "      <td>-0.178882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-0.003350</td>\n",
       "      <td>-0.003350</td>\n",
       "      <td>-0.005985</td>\n",
       "      <td>-0.008787</td>\n",
       "      <td>-0.001979</td>\n",
       "      <td>-0.003059</td>\n",
       "      <td>-0.000320</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.001615</td>\n",
       "      <td>-0.002253</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.007932</td>\n",
       "      <td>-0.007635</td>\n",
       "      <td>-0.014180</td>\n",
       "      <td>-0.015385</td>\n",
       "      <td>-0.017392</td>\n",
       "      <td>-0.009142</td>\n",
       "      <td>-0.008615</td>\n",
       "      <td>-0.007857</td>\n",
       "      <td>-0.008096</td>\n",
       "      <td>-0.000083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-0.000074</td>\n",
       "      <td>-0.000069</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000085</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000501</td>\n",
       "      <td>-0.000542</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000528</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.003310</td>\n",
       "      <td>0.003315</td>\n",
       "      <td>0.008079</td>\n",
       "      <td>0.009256</td>\n",
       "      <td>0.002247</td>\n",
       "      <td>0.002388</td>\n",
       "      <td>0.000407</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001505</td>\n",
       "      <td>0.002380</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007049</td>\n",
       "      <td>0.006831</td>\n",
       "      <td>0.013672</td>\n",
       "      <td>0.015038</td>\n",
       "      <td>0.018019</td>\n",
       "      <td>0.008157</td>\n",
       "      <td>0.007746</td>\n",
       "      <td>0.007061</td>\n",
       "      <td>0.007395</td>\n",
       "      <td>0.000349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.070259</td>\n",
       "      <td>0.070259</td>\n",
       "      <td>0.137621</td>\n",
       "      <td>0.133531</td>\n",
       "      <td>0.020071</td>\n",
       "      <td>0.700986</td>\n",
       "      <td>0.005774</td>\n",
       "      <td>0.229973</td>\n",
       "      <td>0.108471</td>\n",
       "      <td>0.697777</td>\n",
       "      <td>...</td>\n",
       "      <td>0.072736</td>\n",
       "      <td>0.082584</td>\n",
       "      <td>1.823012</td>\n",
       "      <td>4.235250</td>\n",
       "      <td>4.605170</td>\n",
       "      <td>0.081709</td>\n",
       "      <td>0.082212</td>\n",
       "      <td>0.079560</td>\n",
       "      <td>0.085061</td>\n",
       "      <td>0.176596</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 87 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 1            2            3            4            5  \\\n",
       "count  4273.000000  4273.000000  4273.000000  4273.000000  4273.000000   \n",
       "mean      0.000153     0.000155     0.000702     0.000568     0.000041   \n",
       "std       0.006769     0.006770     0.020769     0.018350     0.004673   \n",
       "min      -0.066527    -0.066527    -0.614143    -0.101624    -0.190591   \n",
       "25%      -0.003350    -0.003350    -0.005985    -0.008787    -0.001979   \n",
       "50%      -0.000074    -0.000069     0.000000     0.000000     0.000085   \n",
       "75%       0.003310     0.003315     0.008079     0.009256     0.002247   \n",
       "max       0.070259     0.070259     0.137621     0.133531     0.020071   \n",
       "\n",
       "                 6            7            8            9           10  \\\n",
       "count  4273.000000  4273.000000  4273.000000  4273.000000  4273.000000   \n",
       "mean      0.000619     0.000020    -0.000026     0.000202     0.000106   \n",
       "std       0.041773     0.000858     0.010168     0.007852     0.026645   \n",
       "min      -0.837715    -0.007372    -0.166175    -0.123921    -0.385704   \n",
       "25%      -0.003059    -0.000320     0.000000    -0.001615    -0.002253   \n",
       "50%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "75%       0.002388     0.000407     0.000000     0.001505     0.002380   \n",
       "max       0.700986     0.005774     0.229973     0.108471     0.697777   \n",
       "\n",
       "          ...                78           79           80           81  \\\n",
       "count     ...       4273.000000  4273.000000  4273.000000  4273.000000   \n",
       "mean      ...         -0.000297    -0.000292    -0.001013    -0.000442   \n",
       "std       ...          0.014881     0.014750     0.099855     0.149762   \n",
       "min       ...         -0.136928    -0.131822    -3.135494    -3.277145   \n",
       "25%       ...         -0.007932    -0.007635    -0.014180    -0.015385   \n",
       "50%       ...          0.000000     0.000000     0.000000     0.000000   \n",
       "75%       ...          0.007049     0.006831     0.013672     0.015038   \n",
       "max       ...          0.072736     0.082584     1.823012     4.235250   \n",
       "\n",
       "                82           83           84           85           86  \\\n",
       "count  4273.000000  4273.000000  4273.000000  4273.000000  4273.000000   \n",
       "mean      0.003848    -0.000315    -0.000301    -0.000260    -0.000289   \n",
       "std       0.305913     0.016318     0.015439     0.015259     0.015197   \n",
       "min      -3.624341    -0.112705    -0.105936    -0.114583    -0.109933   \n",
       "25%      -0.017392    -0.009142    -0.008615    -0.007857    -0.008096   \n",
       "50%       0.000000    -0.000501    -0.000542     0.000000    -0.000528   \n",
       "75%       0.018019     0.008157     0.007746     0.007061     0.007395   \n",
       "max       4.605170     0.081709     0.082212     0.079560     0.085061   \n",
       "\n",
       "                87  \n",
       "count  4273.000000  \n",
       "mean      0.000054  \n",
       "std       0.004519  \n",
       "min      -0.178882  \n",
       "25%      -0.000083  \n",
       "50%       0.000000  \n",
       "75%       0.000349  \n",
       "max       0.176596  \n",
       "\n",
       "[8 rows x 87 columns]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tp1=2, Tp0=2, strat=62, inputs=87\n",
    "# predictors_tf = training_test_data[training_test_data.columns[2+2+(31*2):2+2+(31*2)+87]]\n",
    "# predictors_tf.describe()\n",
    "# predictors_tf.head()\n",
    "\n",
    "tf_innput = training_test_data[ training_test_data.columns[2+2+(31*2) : 2+2+(31*2)+87] ]\n",
    "tf_innput.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 2 output columns\n",
    "# classes_tf = training_test_data[training_test_data.columns[0:2]] # col 0, 1\n",
    "# classes_tf = training_test_data[training_test_data.columns[2:4]] # col 2, 3\n",
    "# classes_tf.describe()\n",
    "# count = 4273"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bc_Tp1_up</th>\n",
       "      <th>bc_Tp1_dn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4273.000000</td>\n",
       "      <td>4273.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.495905</td>\n",
       "      <td>0.504095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.500042</td>\n",
       "      <td>0.500042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         bc_Tp1_up    bc_Tp1_dn\n",
       "count  4273.000000  4273.000000\n",
       "mean      0.495905     0.504095\n",
       "std       0.500042     0.500042\n",
       "min       0.000000     0.000000\n",
       "25%       0.000000     0.000000\n",
       "50%       0.000000     1.000000\n",
       "75%       1.000000     1.000000\n",
       "max       1.000000     1.000000"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_output_Tp1 = training_test_data[training_test_data.columns[0:2]] # col 0, 1\n",
    "tf_output_Tp1.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bc_Tp0_up</th>\n",
       "      <th>bc_Tp0_dn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4273.000000</td>\n",
       "      <td>4273.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.496139</td>\n",
       "      <td>0.503861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.500044</td>\n",
       "      <td>0.500044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         bc_Tp0_up    bc_Tp0_dn\n",
       "count  4273.000000  4273.000000\n",
       "mean      0.496139     0.503861\n",
       "std       0.500044     0.500044\n",
       "min       0.000000     0.000000\n",
       "25%       0.000000     0.000000\n",
       "50%       0.000000     1.000000\n",
       "75%       1.000000     1.000000\n",
       "max       1.000000     1.000000"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_output_Tp0 = training_test_data[training_test_data.columns[2:4]] # col 2, 3\n",
    "tf_output_Tp0.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size_tst = size of testing set = 854\n",
      "size_trn = size of training set = 3419\n"
     ]
    }
   ],
   "source": [
    "# Split: test data = 20%\n",
    "size_tst = int(len(training_test_data) * 0.2)\n",
    "print 'size_tst = size of testing set = ' + str(size_tst)\n",
    "# Split: train data = 80%\n",
    "size_trn = len(training_test_data) - size_tst\n",
    "print 'size_trn = size of training set = ' + str(size_trn)\n",
    "\n",
    "# noise remove stratification: [-0010,+0010]: training_set_size=2837, test_set_size=710\n",
    "# noise remove stratification: [-0020,+0020]: training_set_size=2317, test_set_size=580\n",
    "# noise remove stratification: [-0030,+0030]: training_set_size=1847, test_set_size=462\n",
    "# noise remove stratification: [-0040,+0040]: training_set_size=1464, test_set_size=366\n",
    "# noise remove stratification: [-0050,+0050]: training_set_size=1144, test_set_size=286\n",
    "# noise remove stratification: [-0060,+0060]: training_set_size=902,  test_set_size=226\n",
    "# noise remove stratification: [-0070,+0070]: training_set_size=708,  test_set_size=178\n",
    "# noise remove stratification: [-0080,+0080]: training_set_size=552,  test_set_size=138\n",
    "# noise remove stratification: [-0090,+0090]: training_set_size=441,  test_set_size=111\n",
    "# noise remove stratification: [-0100,+0100]: training_set_size=345,  test_set_size=87\n",
    "\n",
    "# size_trn = size of training set = 3418, size_tst = size of testing set = 855"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split into training and test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bc_Tp1_up</th>\n",
       "      <th>bc_Tp1_dn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>854.000000</td>\n",
       "      <td>854.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.523419</td>\n",
       "      <td>0.476581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.499744</td>\n",
       "      <td>0.499744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        bc_Tp1_up   bc_Tp1_dn\n",
       "count  854.000000  854.000000\n",
       "mean     0.523419    0.476581\n",
       "std      0.499744    0.499744\n",
       "min      0.000000    0.000000\n",
       "25%      0.000000    0.000000\n",
       "50%      1.000000    0.000000\n",
       "75%      1.000000    1.000000\n",
       "max      1.000000    1.000000"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Since data is reverse chronological (ie. lastest on top)\n",
    "# top (size_tst=855) rows\n",
    "tf_innput_tst     = tf_innput[:size_tst]\n",
    "tf_output_tst_Tp1 = tf_output_Tp1[:size_tst]\n",
    "tf_output_tst_Tp0 = tf_output_Tp0[:size_tst]\n",
    "\n",
    "tf_output_tst_Tp1.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bc_Tp1_up</th>\n",
       "      <th>bc_Tp1_dn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3419.000000</td>\n",
       "      <td>3419.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.489032</td>\n",
       "      <td>0.510968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.499953</td>\n",
       "      <td>0.499953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         bc_Tp1_up    bc_Tp1_dn\n",
       "count  3419.000000  3419.000000\n",
       "mean      0.489032     0.510968\n",
       "std       0.499953     0.499953\n",
       "min       0.000000     0.000000\n",
       "25%       0.000000     0.000000\n",
       "50%       0.000000     1.000000\n",
       "75%       1.000000     1.000000\n",
       "max       1.000000     1.000000"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Since data is reverse chronological (ie. lastest on top)\n",
    "# bottom (total-(size_tst=855)=3418) rows\n",
    "# ie. start from row 855 onwards\n",
    "tf_innput_trn     = tf_innput[size_tst:]      \n",
    "tf_output_trn_Tp1 = tf_output_Tp1[size_tst:]\n",
    "tf_output_trn_Tp0 = tf_output_Tp0[size_tst:]\n",
    "\n",
    "tf_output_trn_Tp1.describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define some metrics here to evaluate the models.\n",
    "\n",
    "* [Precision](https://en.wikipedia.org/wiki/Precision_and_recall#Precision) -  The ability of the classifier not to label as positive a sample that is negative.\n",
    "* [Recall](https://en.wikipedia.org/wiki/Precision_and_recall#Recall) - The ability of the classifier to find all the positive samples.\n",
    "* [F1 Score](https://en.wikipedia.org/wiki/F1_score) - A weighted average of the precision and recall, where an F1 score reaches its best value at 1 and worst score at 0.\n",
    "* Accuracy - The percentage correctly predicted in the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def tf_confusion_metrics(model, actual_classes, session, feed_dict):\n",
    "  predictions = tf.argmax(model, 1)\n",
    "  actuals = tf.argmax(actual_classes, 1)\n",
    "\n",
    "  ones_like_actuals = tf.ones_like(actuals)\n",
    "  zeros_like_actuals = tf.zeros_like(actuals)\n",
    "  ones_like_predictions = tf.ones_like(predictions)\n",
    "  zeros_like_predictions = tf.zeros_like(predictions)\n",
    "\n",
    "  tp_op = tf.reduce_sum(\n",
    "    tf.cast(\n",
    "      tf.logical_and(\n",
    "        tf.equal(actuals, ones_like_actuals), \n",
    "        tf.equal(predictions, ones_like_predictions)\n",
    "      ), \n",
    "      \"float\"\n",
    "    )\n",
    "  )\n",
    "\n",
    "  tn_op = tf.reduce_sum(\n",
    "    tf.cast(\n",
    "      tf.logical_and(\n",
    "        tf.equal(actuals, zeros_like_actuals), \n",
    "        tf.equal(predictions, zeros_like_predictions)\n",
    "      ), \n",
    "      \"float\"\n",
    "    )\n",
    "  )\n",
    "\n",
    "  fp_op = tf.reduce_sum(\n",
    "    tf.cast(\n",
    "      tf.logical_and(\n",
    "        tf.equal(actuals, zeros_like_actuals), \n",
    "        tf.equal(predictions, ones_like_predictions)\n",
    "      ), \n",
    "      \"float\"\n",
    "    )\n",
    "  )\n",
    "\n",
    "  fn_op = tf.reduce_sum(\n",
    "    tf.cast(\n",
    "      tf.logical_and(\n",
    "        tf.equal(actuals, ones_like_actuals), \n",
    "        tf.equal(predictions, zeros_like_predictions)\n",
    "      ), \n",
    "      \"float\"\n",
    "    )\n",
    "  )\n",
    "\n",
    "  tp, tn, fp, fn = \\\n",
    "    session.run(\n",
    "      [tp_op, tn_op, fp_op, fn_op], \n",
    "      feed_dict\n",
    "    )\n",
    "\n",
    "  tpr = float(tp)/(float(tp) + float(fn))\n",
    "  fpr = float(fp)/(float(tp) + float(fn))\n",
    "\n",
    "  accuracy = (float(tp) + float(tn))/(float(tp) + float(fp) + float(fn) + float(tn))\n",
    "\n",
    "  recall = tpr\n",
    "  precision = float(tp)/(float(tp) + float(fp))\n",
    "  \n",
    "  f1_score = (2 * (precision * recall)) / (precision + recall)\n",
    "  \n",
    "  print 'Precision = ', precision\n",
    "  print 'Recall = ', recall\n",
    "  print 'F1 Score = ', f1_score\n",
    "  print 'Accuracy = ', accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary classification with TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, get some tensors flowing. The model is binary classification expressed in TensorFlow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_innput     = 87\n",
      "num_output_Tp1 = 2\n",
      "num_output_Tp0 = 2\n"
     ]
    }
   ],
   "source": [
    "# tensorflow session\n",
    "sess_m01_BC = tf.Session()\n",
    "\n",
    "# Define variables for the number of predictors and number of classes to remove magic numbers from our code.\n",
    "num_innput     = len(tf_innput_trn.columns)     # 87\n",
    "num_output_Tp1 = len(tf_output_trn_Tp1.columns) # 02 = Tp1\n",
    "num_output_Tp0 = len(tf_output_trn_Tp0.columns) # 02 = Tp0\n",
    "\n",
    "print 'num_innput     = ' + str(num_innput)\n",
    "print 'num_output_Tp1 = ' + str(num_output_Tp1)\n",
    "print 'num_output_Tp0 = ' + str(num_output_Tp0)\n",
    "\n",
    "# Define placeholders for the data we feed into the process\n",
    "# data_innput     = feature_data\n",
    "# data_output_Tp1 = actual_classes\n",
    "# data_output_Tp0 = actual_classes\n",
    "data_innput     = tf.placeholder(\"float\", [None, num_innput])\n",
    "data_output_Tp1 = tf.placeholder(\"float\", [None, num_output_Tp1])\n",
    "data_output_Tp0 = tf.placeholder(\"float\", [None, num_output_Tp0])\n",
    "\n",
    "# Define a matrix of weights and initialize it with some small random values.\n",
    "weights_Tp1 = tf.Variable(tf.truncated_normal([num_innput, num_output_Tp1], stddev=0.0001))\n",
    "weights_Tp0 = tf.Variable(tf.truncated_normal([num_innput, num_output_Tp0], stddev=0.0001))\n",
    "\n",
    "biases_Tp1  = tf.Variable(tf.ones([num_output_Tp1]))\n",
    "biases_Tp0  = tf.Variable(tf.ones([num_output_Tp0]))\n",
    "\n",
    "# Define our model...\n",
    "# Here we take a softmax regression of the product of our data_innput and weights.\n",
    "model_Tp1 = tf.nn.softmax(tf.matmul(data_innput, weights_Tp1) + biases_Tp1)\n",
    "model_Tp0 = tf.nn.softmax(tf.matmul(data_innput, weights_Tp0) + biases_Tp0)\n",
    "# Define a cost function (we're using the cross entropy).\n",
    "cost_Tp1 = -tf.reduce_sum(data_output_Tp1 * tf.log(model_Tp1))\n",
    "cost_Tp0 = -tf.reduce_sum(data_output_Tp0 * tf.log(model_Tp0))\n",
    "# Define a training step...\n",
    "# Here we use gradient descent with a learning rate of 0.01 using the cost function we just defined.\n",
    "training_step_Tp1 = tf.train.AdamOptimizer(learning_rate=0.0001).minimize(cost_Tp1)\n",
    "training_step_Tp0 = tf.train.AdamOptimizer(learning_rate=0.0001).minimize(cost_Tp0)\n",
    "# Init\n",
    "init = tf.initialize_all_variables()\n",
    "# Run\n",
    "sess_m01_BC.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# BEFORE the model has been run, ie. not yet trained\n",
    "# display weights\n",
    "m01_BC_Tp1_w = sess_m01_BC.run(weights_Tp1)    \n",
    "m01_BC_Tp0_w = sess_m01_BC.run(weights_Tp0)    \n",
    "\n",
    "# print m01_BC_w_Tp1 \n",
    "# print m01_BC_w_Tp0 \n",
    "\n",
    "# Expect something like this (small numbers e-05): \n",
    "# [[ -6.66152628e-05  -6.49469657e-05]\n",
    "#  [  2.86067752e-05   3.27789494e-05]\n",
    "#  [  4.73170294e-05   1.85125769e-04]\n",
    "#  [ -6.42955492e-05   4.16024632e-05]\n",
    "#  [ -1.54488771e-05  -2.10903818e-05]\n",
    "#  [  4.62506796e-05  -2.98624745e-05]\n",
    "#  [ -5.35508079e-05  -1.30856875e-04]\n",
    "#  [ -1.70812025e-04   1.33106529e-04]\n",
    "#  [ -1.45097118e-04  -1.80459567e-04]\n",
    "#  [  1.02448161e-04   8.27739277e-05]\n",
    "#  [ -1.33277281e-04  -4.04360726e-05]\n",
    "#  [ -1.36186543e-04   8.62382149e-05]\n",
    "#  [  2.80324894e-05  -2.20580205e-05]\n",
    "#  [ -4.52000713e-05   2.54859442e-05]\n",
    "#  [ -6.20259525e-05  -6.95227573e-05]\n",
    "#  [  1.68935469e-04  -4.03221093e-05]\n",
    "#  [ -9.51008842e-05   4.10227585e-05]\n",
    "#  [ -5.90909331e-05  -9.20566745e-05]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll train our model in the following snippet. The approach of TensorFlow to executing graph operations allows fine-grained control over the process. Any operation you provide to the session as part of the run operation will be executed and the results returned. You can provide a list of multiple operations.\n",
    "\n",
    "You'll train the model over 30,000 iterations using the full dataset each time. Every thousandth iteration we'll assess the accuracy of the model on the training data to assess progress."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## m01_BC: On Data_Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Tp1_5000_0.560983; Tp0_5000_0.593448;\n",
      " Tp1_10000_0.584966; Tp0_10000_0.622112;\n",
      " Tp1_15000_0.598421; Tp0_15000_0.627961;\n",
      " Tp1_20000_0.603978; Tp0_20000_0.639076;\n",
      " Tp1_25000_0.613922; Tp0_25000_0.650775;\n",
      " Tp1_30000_0.621527; Tp0_30000_0.65487;\n"
     ]
    }
   ],
   "source": [
    "correct_prediction_Tp1 = tf.equal(tf.argmax(model_Tp1, 1), tf.argmax(data_output_Tp1, 1))\n",
    "correct_prediction_Tp0 = tf.equal(tf.argmax(model_Tp0, 1), tf.argmax(data_output_Tp0, 1))\n",
    "\n",
    "accuracy_Tp1 = tf.reduce_mean(tf.cast(correct_prediction_Tp1, \"float\"))\n",
    "accuracy_Tp0 = tf.reduce_mean(tf.cast(correct_prediction_Tp0, \"float\"))\n",
    "\n",
    "for i in range(1, 30001):\n",
    "  s = ''\n",
    "\n",
    "  # Tp1 ==================================================\n",
    "  sess_m01_BC.run(\n",
    "    training_step_Tp1, \n",
    "    feed_dict={\n",
    "      data_innput:     tf_innput_trn.values, \n",
    "      data_output_Tp1: tf_output_trn_Tp1.values.reshape(len(tf_output_trn_Tp1.values), 2)\n",
    "    }\n",
    "  )\n",
    "  if i%5000 == 0:\n",
    "    s = s + ' ' + 'Tp1'\n",
    "    s = s + '_' + str(i) \n",
    "    s = s + '_' + str(sess_m01_BC.run(\n",
    "      accuracy_Tp1,\n",
    "      feed_dict={\n",
    "        data_innput:     tf_innput_trn.values, \n",
    "        data_output_Tp1: tf_output_trn_Tp1.values.reshape(len(tf_output_trn_Tp1.values), 2)\n",
    "      }\n",
    "    ))\n",
    "    s = s + ';'\n",
    "\n",
    "  # Tp0 ==================================================\n",
    "  sess_m01_BC.run(\n",
    "    training_step_Tp0, \n",
    "    feed_dict={\n",
    "      data_innput:     tf_innput_trn.values, \n",
    "      data_output_Tp0: tf_output_trn_Tp0.values.reshape(len(tf_output_trn_Tp0.values), 2)\n",
    "    }\n",
    "  )\n",
    "  if i%5000 == 0:\n",
    "    s = s + ' ' + 'Tp0'\n",
    "    s = s + '_' + str(i) \n",
    "    s = s + '_' + str(sess_m01_BC.run(\n",
    "      accuracy_Tp0,\n",
    "      feed_dict={\n",
    "        data_innput:     tf_innput_trn.values, \n",
    "        data_output_Tp0: tf_output_trn_Tp0.values.reshape(len(tf_output_trn_Tp0.values), 2)\n",
    "      }\n",
    "    ))\n",
    "    s = s + ';'\n",
    "\n",
    "    print s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ## m01_BC: On Data_Training: Archive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Expect: \n",
    "# 5000 0.603862 # 10000 0.630778 # 15000 0.639555 # 20000 0.654184 # 25000 0.660035 # 30000 0.666764\n",
    "# After using < -0.5% and > +0.5%\n",
    "# 5000 0.928906 # 10000 0.928906 # 15000 0.928906 # 20000 0.928906 # 25000 0.928906 # 30000 0.928906\n",
    "# Noise Removal: [-0010, +0010]\n",
    "# 5000 0.617201 # 10000 0.64399 # 15000 0.665139 # 20000 0.683468 # 25000 0.688403 # 30000 0.691928\n",
    "# Noise Removal: [-0020, +0020]\n",
    "# 5000 0.625809 # 10000 0.6612 # 15000 0.680622 # 20000 0.695727 # 25000 0.708675 # 30000 0.712128\n",
    "# Noise Removal: [-0030, +0030]\n",
    "# 5000 0.635084 # 10000 0.677315 # 15000 0.701678 # 20000 0.714672 # 25000 0.729291 # 30000 0.738495\n",
    "# Noise Removal: [-0040, +0040]\n",
    "# 5000 0.645492 # 10000 0.689891 # 15000 0.717213 # 20000 0.734973 # 25000 0.748634 # 30000 0.758197\n",
    "# Noise Removal: [-0050, +0050]\n",
    "# 5000 0.653846 # 10000 0.702797 # 15000 0.730769 # 20000 0.740385 # 25000 0.75 # 30000 0.768357\n",
    "# Noise Removal: [-0060, +0060]\n",
    "# 5000 0.672949 # 10000 0.72173 # 15000 0.750554 # 20000 0.761641 # 25000 0.783814 # 30000 0.792683\n",
    "# Noise Removal: [-0070, +0070]\n",
    "# 5000 0.673729 # 10000 0.735876 # 15000 0.771186 # 20000 0.789548 # 25000 0.809322 # 30000 0.817797\n",
    "# Noise Removal: [-0080, +0080]\n",
    "# 5000 0.692029 # 10000 0.748188 # 15000 0.780797 # 20000 0.815217 # 25000 0.826087 # 30000 0.833333\n",
    "# Noise Removal: [-0090, +0090]\n",
    "# 5000 0.696145 # 10000 0.750567 # 15000 0.773243 # 20000 0.795918 # 25000 0.811791 # 30000 0.816327\n",
    "# Noise Removal: [-0100, +0100]\n",
    "# 5000 0.718841 # 10000 0.75942 # 15000 0.788406 # 20000 0.808696 # 25000 0.823188 # 30000 0.84058\n",
    "# inputs=87, output=2, no filters, Tp1\n",
    "# 5000 0.574605 # 10000 0.587771 # 15000 0.595377 # 20000 0.603277 # 25000 0.607958 # 30000 0.610591\n",
    "# ========================================\n",
    "# Tp1_05000_0.560983; Tp0_05000_0.593448;\n",
    "# Tp1_10000_0.584966; Tp0_10000_0.622112;\n",
    "# Tp1_15000_0.598421; Tp0_15000_0.627961;\n",
    "# Tp1_20000_0.603978; Tp0_20000_0.639076;\n",
    "# Tp1_25000_0.613922; Tp0_25000_0.650775;\n",
    "# Tp1_30000_0.621527; Tp0_30000_0.65487;    <= m01_BC: predicting (T+1) has 62% accu, predicting (T+0) has 65% accu\n",
    "# ========================================\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "m01_BC: predicting (T+1) has 62% accu, predicting (T+0) has 65% accu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using all data:\n",
    "Accuracy  66.6% on training data\n",
    "That is OK, better than random."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## m01_BC: On Data_Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Tp1: Performance on Test Data =====\n",
      "Precision =  0.507795100223\n",
      "Recall =  0.560196560197\n",
      "F1 Score =  0.532710280374\n",
      "Accuracy =  0.531615925059\n",
      "===== Tp0: Performance on Test Data =====\n",
      "Precision =  0.537209302326\n",
      "Recall =  0.567567567568\n",
      "F1 Score =  0.551971326165\n",
      "Accuracy =  0.560889929742\n"
     ]
    }
   ],
   "source": [
    "print '===== Tp1: Performance on Test Data ====='\n",
    "feed_dict= {\n",
    "  data_innput:     tf_innput_tst.values,\n",
    "  data_output_Tp1: tf_output_tst_Tp1.values.reshape(len(tf_output_tst_Tp1.values), 2)\n",
    "}\n",
    "tf_confusion_metrics(model_Tp1, data_output_Tp1, sess_m01_BC, feed_dict)\n",
    "\n",
    "print '===== Tp0: Performance on Test Data ====='\n",
    "feed_dict= {\n",
    "  data_innput:     tf_innput_tst.values,\n",
    "  data_output_Tp0: tf_output_tst_Tp0.values.reshape(len(tf_output_tst_Tp0.values), 2)\n",
    "}\n",
    "tf_confusion_metrics(model_Tp0, data_output_Tp0, sess_m01_BC, feed_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## m01_BC: On Data_Testing: Archive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Precision =  0.541176470588 # Recall =  0.555555555556 # F1 Score =  0.548271752086 # Accuracy =  0.556725146199\n",
    "# After removing noise\n",
    "# Precision =  0.684210526316 # Recall =  0.534246575342 # F1 Score =  0.6 # Accuracy =  0.636363636364\n",
    "# Noise Removal: [-0010, +0010]\n",
    "# Precision =  0.577235772358 # Recall =  0.605113636364 # F1 Score =  0.590846047157 # Accuracy =  0.584507042254\n",
    "# Noise Removal: [-0020, +0020]\n",
    "# Precision =  0.588850174216 # Recall =  0.603571428571 # F1 Score =  0.596119929453 # Accuracy =  0.605172413793\n",
    "# Noise Removal: [-0030, +0030]\n",
    "# Precision =  0.616740088106 # Recall =  0.619469026549 # F1 Score =  0.618101545254 # Accuracy =  0.625541125541\n",
    "# Noise Removal: [-0040, +0040]\n",
    "# Precision =  0.622093023256 # Recall =  0.58152173913 # F1 Score =  0.601123595506 # Accuracy =  0.612021857923\n",
    "# Noise Removal: [-0050, +0050]\n",
    "# Precision =  0.684210526316 # Recall =  0.534246575342 # F1 Score =  0.6 # Accuracy =  0.636363636364\n",
    "# Noise Removal: [-0060, +0060]\n",
    "# Precision =  0.644736842105 # Recall =  0.453703703704 # F1 Score =  0.532608695652 # Accuracy =  0.619469026549\n",
    "# Noise Removal: [-0070, +0070]\n",
    "# Precision =  0.666666666667 # Recall =  0.47619047619 # F1 Score =  0.555555555556 # Accuracy =  0.640449438202\n",
    "# Noise Removal: [-0080, +0080]\n",
    "# Precision =  0.6 # Recall =  0.406779661017 # F1 Score =  0.484848484848 # Accuracy =  0.630434782609\n",
    "# Noise Removal: [-0090, +0090]\n",
    "# Precision =  0.478260869565 # Recall =  0.261904761905 # F1 Score =  0.338461538462 # Accuracy =  0.612612612613\n",
    "# Noise Removal: [-0100, +0100]\n",
    "# Precision =  0.588235294118 # Recall =  0.37037037037 # F1 Score =  0.454545454545 # Accuracy =  0.724137931034\n",
    "\n",
    "# check rows results                         numrows   AccuOnTrainData    AccuOnTestData\n",
    "# dvislr_nr_m0010_z0000_p0010 rows =3554 - 7 = 3547    # 30000 0.691928   # Accuracy =  0.584507042254\n",
    "# dvislr_nr_m0020_z0000_p0020 rows =2904 - 7 = 2897    # 30000 0.712128   # Accuracy =  0.605172413793\n",
    "# dvislr_nr_m0030_z0000_p0030 rows =2316 - 7 = 2309    # 30000 0.738495   # Accuracy =  0.625541125541\n",
    "# dvislr_nr_m0040_z0000_p0040 rows =1837 - 7 = 1830    # 30000 0.758197   # Accuracy =  0.612021857923\n",
    "# dvislr_nr_m0050_z0000_p0050 rows =1437 - 7 = 1430    # 30000 0.768357   # Accuracy =  0.636363636364\n",
    "# dvislr_nr_m0060_z0000_p0060 rows =1135 - 7 = 1128    # 30000 0.792683   # Accuracy =  0.619469026549\n",
    "# dvislr_nr_m0070_z0000_p0070 rows = 893 - 7 =  886    # 30000 0.817797   # Accuracy =  0.640449438202\n",
    "# dvislr_nr_m0080_z0000_p0080 rows = 697 - 7 =  690    # 30000 0.833333   # Accuracy =  0.630434782609\n",
    "# dvislr_nr_m0090_z0000_p0090 rows = 559 - 7 =  552    # 30000 0.816327   # Accuracy =  0.612612612613\n",
    "# dvislr_nr_m0100_z0000_p0100 rows = 439 - 7 =  432    # 30000 0.84058    # Accuracy =  0.724137931034\n",
    "\n",
    "# ===== Tp1: Performance on Test Data =====\n",
    "# Precision =  0.507795100223\n",
    "# Recall =  0.560196560197\n",
    "# F1 Score =  0.532710280374\n",
    "# Accuracy =  0.531615925059\n",
    "# ===== Tp0: Performance on Test Data =====\n",
    "# Precision =  0.537209302326\n",
    "# Recall =  0.567567567568\n",
    "# F1 Score =  0.551971326165\n",
    "# Accuracy =  0.560889929742\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# m01_BC: on data_tst: Tp1.accu = 53% | Tp0.accu = 56%\n",
    "# m01_BC: on data_trn: Tp1.accu = 62% | Tp0.accu = 65%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# AFTER the model has been run, ie. trained\n",
    "# display weights\n",
    "m01_BC_Tp1_w = sess_m01_BC.run(weights_Tp1)    \n",
    "m01_BC_Tp0_w = sess_m01_BC.run(weights_Tp0)    \n",
    "# print m01_BC_w_Tp1 \n",
    "# print m01_BC_w_Tp0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uploaded: {u'icon': u'page_white_excel', u'bytes': 2844, u'thumb_exists': False, u'rev': u'5f4a1e9d47', u'modified': u'Wed, 24 Aug 2016 09:43:51 +0000', u'shareable': False, u'client_mtime': u'Wed, 24 Aug 2016 09:43:51 +0000', u'path': u'/m01_BC_Tp0_w.csv', u'is_dir': False, u'size': u'2.8 KB', u'root': u'dropbox', u'mime_type': u'text/csv', u'revision': 95}\n"
     ]
    }
   ],
   "source": [
    "# response = client.put_file('/output.csv', str(pd.DataFrame(npa).to_csv()), overwrite=True)\n",
    "# response = client.put_file('/output.csv', str(               df.to_csv()), overwrite=True)\n",
    "response = client.put_file('/m01_BC_Tp1_w.csv', str(pd.DataFrame(m01_BC_Tp1_w).to_csv()), overwrite=True)\n",
    "response = client.put_file('/m01_BC_Tp0_w.csv', str(pd.DataFrame(m01_BC_Tp0_w).to_csv()), overwrite=True)\n",
    "print \"uploaded:\", response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>78</th>\n",
       "      <th>79</th>\n",
       "      <th>80</th>\n",
       "      <th>81</th>\n",
       "      <th>82</th>\n",
       "      <th>83</th>\n",
       "      <th>84</th>\n",
       "      <th>85</th>\n",
       "      <th>86</th>\n",
       "      <th>87</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.001383</td>\n",
       "      <td>0.009640</td>\n",
       "      <td>-0.004322</td>\n",
       "      <td>-0.002986</td>\n",
       "      <td>-0.004742</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.000089</td>\n",
       "      <td>-0.005801</td>\n",
       "      <td>0.009516</td>\n",
       "      <td>0.013912</td>\n",
       "      <td>...</td>\n",
       "      <td>0.029968</td>\n",
       "      <td>0.029456</td>\n",
       "      <td>0.222493</td>\n",
       "      <td>0.128977</td>\n",
       "      <td>0.062618</td>\n",
       "      <td>0.027276</td>\n",
       "      <td>0.023740</td>\n",
       "      <td>0.021459</td>\n",
       "      <td>0.020401</td>\n",
       "      <td>-0.003376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.009640</td>\n",
       "      <td>-0.001815</td>\n",
       "      <td>-0.009415</td>\n",
       "      <td>0.043185</td>\n",
       "      <td>0.000636</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.001050</td>\n",
       "      <td>-0.003639</td>\n",
       "      <td>0.006765</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.005383</td>\n",
       "      <td>-0.005290</td>\n",
       "      <td>-0.012917</td>\n",
       "      <td>-0.007126</td>\n",
       "      <td>-0.006120</td>\n",
       "      <td>0.008725</td>\n",
       "      <td>0.006062</td>\n",
       "      <td>0.004955</td>\n",
       "      <td>0.004396</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.001815</td>\n",
       "      <td>0.007429</td>\n",
       "      <td>0.026992</td>\n",
       "      <td>0.004772</td>\n",
       "      <td>-0.003315</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.000045</td>\n",
       "      <td>-0.000521</td>\n",
       "      <td>-0.002741</td>\n",
       "      <td>0.039748</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012770</td>\n",
       "      <td>0.012063</td>\n",
       "      <td>0.092960</td>\n",
       "      <td>0.050359</td>\n",
       "      <td>0.022079</td>\n",
       "      <td>0.024121</td>\n",
       "      <td>0.021459</td>\n",
       "      <td>0.020217</td>\n",
       "      <td>0.018647</td>\n",
       "      <td>-0.000321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.007429</td>\n",
       "      <td>0.011079</td>\n",
       "      <td>-0.005542</td>\n",
       "      <td>0.002764</td>\n",
       "      <td>0.004163</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002623</td>\n",
       "      <td>0.002016</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.019095</td>\n",
       "      <td>-0.019707</td>\n",
       "      <td>-0.109928</td>\n",
       "      <td>-0.060954</td>\n",
       "      <td>-0.033573</td>\n",
       "      <td>-0.023105</td>\n",
       "      <td>-0.020661</td>\n",
       "      <td>-0.020121</td>\n",
       "      <td>-0.018647</td>\n",
       "      <td>0.000803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.011079</td>\n",
       "      <td>-0.001229</td>\n",
       "      <td>0.014858</td>\n",
       "      <td>-0.016469</td>\n",
       "      <td>-0.002401</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.001052</td>\n",
       "      <td>-0.018949</td>\n",
       "      <td>-0.006370</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015641</td>\n",
       "      <td>0.014382</td>\n",
       "      <td>-0.084643</td>\n",
       "      <td>-0.027146</td>\n",
       "      <td>-0.004751</td>\n",
       "      <td>0.013842</td>\n",
       "      <td>0.011033</td>\n",
       "      <td>0.009937</td>\n",
       "      <td>0.008563</td>\n",
       "      <td>-0.001204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.001229</td>\n",
       "      <td>0.000167</td>\n",
       "      <td>0.000465</td>\n",
       "      <td>0.002362</td>\n",
       "      <td>0.001836</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012164</td>\n",
       "      <td>-0.000266</td>\n",
       "      <td>0.005335</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.010292</td>\n",
       "      <td>-0.010567</td>\n",
       "      <td>-0.010572</td>\n",
       "      <td>-0.005682</td>\n",
       "      <td>-0.007241</td>\n",
       "      <td>-0.014537</td>\n",
       "      <td>-0.014021</td>\n",
       "      <td>-0.014508</td>\n",
       "      <td>-0.013996</td>\n",
       "      <td>0.000160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.000167</td>\n",
       "      <td>-0.011771</td>\n",
       "      <td>0.024019</td>\n",
       "      <td>0.011527</td>\n",
       "      <td>0.000141</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.000089</td>\n",
       "      <td>-0.037074</td>\n",
       "      <td>0.001296</td>\n",
       "      <td>-0.007589</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.002921</td>\n",
       "      <td>-0.002863</td>\n",
       "      <td>-0.006670</td>\n",
       "      <td>-0.010147</td>\n",
       "      <td>-0.009366</td>\n",
       "      <td>0.006704</td>\n",
       "      <td>0.004835</td>\n",
       "      <td>0.006020</td>\n",
       "      <td>0.004581</td>\n",
       "      <td>0.000401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-0.011771</td>\n",
       "      <td>0.018542</td>\n",
       "      <td>0.010193</td>\n",
       "      <td>0.016514</td>\n",
       "      <td>0.002902</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.001026</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.025475</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.008230</td>\n",
       "      <td>-0.007121</td>\n",
       "      <td>0.056656</td>\n",
       "      <td>0.030754</td>\n",
       "      <td>0.016291</td>\n",
       "      <td>-0.018765</td>\n",
       "      <td>-0.014977</td>\n",
       "      <td>-0.014482</td>\n",
       "      <td>-0.012792</td>\n",
       "      <td>0.001124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.018542</td>\n",
       "      <td>-0.001387</td>\n",
       "      <td>-0.003987</td>\n",
       "      <td>0.009399</td>\n",
       "      <td>0.000780</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000179</td>\n",
       "      <td>0.001545</td>\n",
       "      <td>0.004862</td>\n",
       "      <td>-0.001188</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002414</td>\n",
       "      <td>0.002842</td>\n",
       "      <td>0.058993</td>\n",
       "      <td>0.029941</td>\n",
       "      <td>0.011436</td>\n",
       "      <td>0.013933</td>\n",
       "      <td>0.013180</td>\n",
       "      <td>0.011877</td>\n",
       "      <td>0.011373</td>\n",
       "      <td>0.000402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-0.001387</td>\n",
       "      <td>0.005222</td>\n",
       "      <td>0.003469</td>\n",
       "      <td>-0.019451</td>\n",
       "      <td>0.004123</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000179</td>\n",
       "      <td>-0.002053</td>\n",
       "      <td>-0.008928</td>\n",
       "      <td>-0.007498</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.017725</td>\n",
       "      <td>-0.017400</td>\n",
       "      <td>0.133227</td>\n",
       "      <td>0.066527</td>\n",
       "      <td>0.032470</td>\n",
       "      <td>-0.021824</td>\n",
       "      <td>-0.020195</td>\n",
       "      <td>-0.015394</td>\n",
       "      <td>-0.018358</td>\n",
       "      <td>0.000884</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 87 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          1         2         3         4         5    6         7         8  \\\n",
       "0 -0.001383  0.009640 -0.004322 -0.002986 -0.004742  0.0 -0.000089 -0.005801   \n",
       "1  0.009640 -0.001815 -0.009415  0.043185  0.000636  0.0  0.000000 -0.001050   \n",
       "2 -0.001815  0.007429  0.026992  0.004772 -0.003315  0.0 -0.000045 -0.000521   \n",
       "3  0.007429  0.011079 -0.005542  0.002764  0.004163  0.0  0.000000  0.002623   \n",
       "4  0.011079 -0.001229  0.014858 -0.016469 -0.002401  0.0  0.000000 -0.001052   \n",
       "5 -0.001229  0.000167  0.000465  0.002362  0.001836  0.0  0.000000  0.012164   \n",
       "6  0.000167 -0.011771  0.024019  0.011527  0.000141  0.0 -0.000089 -0.037074   \n",
       "7 -0.011771  0.018542  0.010193  0.016514  0.002902  0.0  0.000045  0.001026   \n",
       "8  0.018542 -0.001387 -0.003987  0.009399  0.000780  0.0  0.000179  0.001545   \n",
       "9 -0.001387  0.005222  0.003469 -0.019451  0.004123  0.0  0.000179 -0.002053   \n",
       "\n",
       "          9        10    ...           78        79        80        81  \\\n",
       "0  0.009516  0.013912    ...     0.029968  0.029456  0.222493  0.128977   \n",
       "1 -0.003639  0.006765    ...    -0.005383 -0.005290 -0.012917 -0.007126   \n",
       "2 -0.002741  0.039748    ...     0.012770  0.012063  0.092960  0.050359   \n",
       "3  0.002016  0.000000    ...    -0.019095 -0.019707 -0.109928 -0.060954   \n",
       "4 -0.018949 -0.006370    ...     0.015641  0.014382 -0.084643 -0.027146   \n",
       "5 -0.000266  0.005335    ...    -0.010292 -0.010567 -0.010572 -0.005682   \n",
       "6  0.001296 -0.007589    ...    -0.002921 -0.002863 -0.006670 -0.010147   \n",
       "7  0.000000  0.025475    ...    -0.008230 -0.007121  0.056656  0.030754   \n",
       "8  0.004862 -0.001188    ...     0.002414  0.002842  0.058993  0.029941   \n",
       "9 -0.008928 -0.007498    ...    -0.017725 -0.017400  0.133227  0.066527   \n",
       "\n",
       "         82        83        84        85        86        87  \n",
       "0  0.062618  0.027276  0.023740  0.021459  0.020401 -0.003376  \n",
       "1 -0.006120  0.008725  0.006062  0.004955  0.004396  0.000000  \n",
       "2  0.022079  0.024121  0.021459  0.020217  0.018647 -0.000321  \n",
       "3 -0.033573 -0.023105 -0.020661 -0.020121 -0.018647  0.000803  \n",
       "4 -0.004751  0.013842  0.011033  0.009937  0.008563 -0.001204  \n",
       "5 -0.007241 -0.014537 -0.014021 -0.014508 -0.013996  0.000160  \n",
       "6 -0.009366  0.006704  0.004835  0.006020  0.004581  0.000401  \n",
       "7  0.016291 -0.018765 -0.014977 -0.014482 -0.012792  0.001124  \n",
       "8  0.011436  0.013933  0.013180  0.011877  0.011373  0.000402  \n",
       "9  0.032470 -0.021824 -0.020195 -0.015394 -0.018358  0.000884  \n",
       "\n",
       "[10 rows x 87 columns]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# grab a sample input row from the test set\n",
    "sample_input_1_87 = tf_innput_tst[:10]\n",
    "# HTML(pd.DataFrame(sample_input_1_87).to_html())\n",
    "sample_input_1_87"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>78</th>\n",
       "      <th>79</th>\n",
       "      <th>80</th>\n",
       "      <th>81</th>\n",
       "      <th>82</th>\n",
       "      <th>83</th>\n",
       "      <th>84</th>\n",
       "      <th>85</th>\n",
       "      <th>86</th>\n",
       "      <th>87</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.002927</td>\n",
       "      <td>0.003588</td>\n",
       "      <td>0.005673</td>\n",
       "      <td>0.005162</td>\n",
       "      <td>0.000412</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>-0.003019</td>\n",
       "      <td>-0.001683</td>\n",
       "      <td>0.006859</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000285</td>\n",
       "      <td>-0.000420</td>\n",
       "      <td>0.033960</td>\n",
       "      <td>0.019550</td>\n",
       "      <td>0.008384</td>\n",
       "      <td>0.001637</td>\n",
       "      <td>0.001045</td>\n",
       "      <td>0.000996</td>\n",
       "      <td>0.000417</td>\n",
       "      <td>-0.000113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.008668</td>\n",
       "      <td>0.008554</td>\n",
       "      <td>0.012800</td>\n",
       "      <td>0.017597</td>\n",
       "      <td>0.003068</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000095</td>\n",
       "      <td>0.012832</td>\n",
       "      <td>0.007826</td>\n",
       "      <td>0.015571</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015638</td>\n",
       "      <td>0.015333</td>\n",
       "      <td>0.100346</td>\n",
       "      <td>0.054012</td>\n",
       "      <td>0.026840</td>\n",
       "      <td>0.019378</td>\n",
       "      <td>0.017076</td>\n",
       "      <td>0.015713</td>\n",
       "      <td>0.015090</td>\n",
       "      <td>0.001328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-0.011771</td>\n",
       "      <td>-0.011771</td>\n",
       "      <td>-0.009415</td>\n",
       "      <td>-0.019451</td>\n",
       "      <td>-0.004742</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.000089</td>\n",
       "      <td>-0.037074</td>\n",
       "      <td>-0.018949</td>\n",
       "      <td>-0.007589</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.019095</td>\n",
       "      <td>-0.019707</td>\n",
       "      <td>-0.109928</td>\n",
       "      <td>-0.060954</td>\n",
       "      <td>-0.033573</td>\n",
       "      <td>-0.023105</td>\n",
       "      <td>-0.020661</td>\n",
       "      <td>-0.020121</td>\n",
       "      <td>-0.018647</td>\n",
       "      <td>-0.003376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-0.001386</td>\n",
       "      <td>-0.001348</td>\n",
       "      <td>-0.004238</td>\n",
       "      <td>-0.001649</td>\n",
       "      <td>-0.001766</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.000034</td>\n",
       "      <td>-0.001803</td>\n",
       "      <td>-0.003414</td>\n",
       "      <td>-0.005074</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009776</td>\n",
       "      <td>-0.009705</td>\n",
       "      <td>-0.012331</td>\n",
       "      <td>-0.009391</td>\n",
       "      <td>-0.006961</td>\n",
       "      <td>-0.017708</td>\n",
       "      <td>-0.014738</td>\n",
       "      <td>-0.014502</td>\n",
       "      <td>-0.013695</td>\n",
       "      <td>-0.000241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-0.000531</td>\n",
       "      <td>0.002695</td>\n",
       "      <td>0.001967</td>\n",
       "      <td>0.003768</td>\n",
       "      <td>0.000708</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000786</td>\n",
       "      <td>-0.000133</td>\n",
       "      <td>0.002668</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.004152</td>\n",
       "      <td>-0.004076</td>\n",
       "      <td>0.024993</td>\n",
       "      <td>0.012129</td>\n",
       "      <td>0.003343</td>\n",
       "      <td>0.007715</td>\n",
       "      <td>0.005448</td>\n",
       "      <td>0.005487</td>\n",
       "      <td>0.004488</td>\n",
       "      <td>0.000281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.009087</td>\n",
       "      <td>0.009087</td>\n",
       "      <td>0.013692</td>\n",
       "      <td>0.010995</td>\n",
       "      <td>0.002636</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.001415</td>\n",
       "      <td>0.001836</td>\n",
       "      <td>0.012125</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010181</td>\n",
       "      <td>0.009758</td>\n",
       "      <td>0.084468</td>\n",
       "      <td>0.045458</td>\n",
       "      <td>0.020632</td>\n",
       "      <td>0.013911</td>\n",
       "      <td>0.012643</td>\n",
       "      <td>0.011392</td>\n",
       "      <td>0.010670</td>\n",
       "      <td>0.000702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.018542</td>\n",
       "      <td>0.018542</td>\n",
       "      <td>0.026992</td>\n",
       "      <td>0.043185</td>\n",
       "      <td>0.004163</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000179</td>\n",
       "      <td>0.012164</td>\n",
       "      <td>0.009516</td>\n",
       "      <td>0.039748</td>\n",
       "      <td>...</td>\n",
       "      <td>0.029968</td>\n",
       "      <td>0.029456</td>\n",
       "      <td>0.222493</td>\n",
       "      <td>0.128977</td>\n",
       "      <td>0.062618</td>\n",
       "      <td>0.027276</td>\n",
       "      <td>0.023740</td>\n",
       "      <td>0.021459</td>\n",
       "      <td>0.020401</td>\n",
       "      <td>0.001124</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 87 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               1          2          3          4          5     6          7  \\\n",
       "count  10.000000  10.000000  10.000000  10.000000  10.000000  10.0  10.000000   \n",
       "mean    0.002927   0.003588   0.005673   0.005162   0.000412   0.0   0.000018   \n",
       "std     0.008668   0.008554   0.012800   0.017597   0.003068   0.0   0.000095   \n",
       "min    -0.011771  -0.011771  -0.009415  -0.019451  -0.004742   0.0  -0.000089   \n",
       "25%    -0.001386  -0.001348  -0.004238  -0.001649  -0.001766   0.0  -0.000034   \n",
       "50%    -0.000531   0.002695   0.001967   0.003768   0.000708   0.0   0.000000   \n",
       "75%     0.009087   0.009087   0.013692   0.010995   0.002636   0.0   0.000034   \n",
       "max     0.018542   0.018542   0.026992   0.043185   0.004163   0.0   0.000179   \n",
       "\n",
       "               8          9         10    ...             78         79  \\\n",
       "count  10.000000  10.000000  10.000000    ...      10.000000  10.000000   \n",
       "mean   -0.003019  -0.001683   0.006859    ...      -0.000285  -0.000420   \n",
       "std     0.012832   0.007826   0.015571    ...       0.015638   0.015333   \n",
       "min    -0.037074  -0.018949  -0.007589    ...      -0.019095  -0.019707   \n",
       "25%    -0.001803  -0.003414  -0.005074    ...      -0.009776  -0.009705   \n",
       "50%    -0.000786  -0.000133   0.002668    ...      -0.004152  -0.004076   \n",
       "75%     0.001415   0.001836   0.012125    ...       0.010181   0.009758   \n",
       "max     0.012164   0.009516   0.039748    ...       0.029968   0.029456   \n",
       "\n",
       "              80         81         82         83         84         85  \\\n",
       "count  10.000000  10.000000  10.000000  10.000000  10.000000  10.000000   \n",
       "mean    0.033960   0.019550   0.008384   0.001637   0.001045   0.000996   \n",
       "std     0.100346   0.054012   0.026840   0.019378   0.017076   0.015713   \n",
       "min    -0.109928  -0.060954  -0.033573  -0.023105  -0.020661  -0.020121   \n",
       "25%    -0.012331  -0.009391  -0.006961  -0.017708  -0.014738  -0.014502   \n",
       "50%     0.024993   0.012129   0.003343   0.007715   0.005448   0.005487   \n",
       "75%     0.084468   0.045458   0.020632   0.013911   0.012643   0.011392   \n",
       "max     0.222493   0.128977   0.062618   0.027276   0.023740   0.021459   \n",
       "\n",
       "              86         87  \n",
       "count  10.000000  10.000000  \n",
       "mean    0.000417  -0.000113  \n",
       "std     0.015090   0.001328  \n",
       "min    -0.018647  -0.003376  \n",
       "25%    -0.013695  -0.000241  \n",
       "50%     0.004488   0.000281  \n",
       "75%     0.010670   0.000702  \n",
       "max     0.020401   0.001124  \n",
       "\n",
       "[8 rows x 87 columns]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_input_1_87.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## m01_BC_Tp1: Make a prediction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[False  True]\n",
      " [False  True]\n",
      " [False  True]\n",
      " [ True False]\n",
      " [ True False]\n",
      " [ True False]\n",
      " [ True False]\n",
      " [False  True]\n",
      " [ True False]\n",
      " [False  True]]\n",
      "m01_BC_Tp1_usdmxn_pred = -1\n"
     ]
    }
   ],
   "source": [
    "# use the weights from our model to make a prediction\n",
    "m01_BC_Tp1_output_1_2 = np.dot(sample_input_1_87, m01_BC_Tp1_w) > 0\n",
    "print m01_BC_Tp1_output_1_2\n",
    "m01_BC_Tp1_usdmxn_up = m01_BC_Tp1_output_1_2[0,0]\n",
    "m01_BC_Tp1_usdmxn_dn = m01_BC_Tp1_output_1_2[0,1]\n",
    "\n",
    "if m01_BC_Tp1_usdmxn_up >= m01_BC_Tp1_usdmxn_dn:\n",
    "  m01_BC_Tp1_usdmxn_pred = 1 # up\n",
    "else:\n",
    "  m01_BC_Tp1_usdmxn_pred = -1 # down\n",
    "print 'm01_BC_Tp1_usdmxn_pred = '+ str(m01_BC_Tp1_usdmxn_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 2: Neural Network (NN): Feed-Forward with 2 Hidden Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll now build a proper feed-forward neural net with two hidden layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_innput     = 87\n",
      "num_output_Tp1 = 2\n",
      "num_output_Tp0 = 2\n"
     ]
    }
   ],
   "source": [
    "# tensorflow session (new)\n",
    "sess_m02_NN = tf.Session()\n",
    "\n",
    "# Define variables for the number of predictors and number of classes to remove magic numbers from our code.\n",
    "num_innput     = len(tf_innput_trn.columns)     # 87             # num_predictors = len(tf_innput_trn.columns)\n",
    "num_output_Tp1 = len(tf_output_trn_Tp1.columns) # 02 = Tp1       # num_classes = len(training_classes_tf.columns)\n",
    "num_output_Tp0 = len(tf_output_trn_Tp0.columns) # 02 = Tp0       # num_classes = len(training_classes_tf.columns)\n",
    "\n",
    "print 'num_innput     = ' + str(num_innput)\n",
    "print 'num_output_Tp1 = ' + str(num_output_Tp1)\n",
    "print 'num_output_Tp0 = ' + str(num_output_Tp0)\n",
    "\n",
    "# Define placeholders for the data we feed into the process\n",
    "# data_innput     = feature_data\n",
    "# data_output_Tp1 = actual_classes\n",
    "# data_output_Tp0 = actual_classes\n",
    "data_innput     = tf.placeholder(\"float\", [None, num_innput])\n",
    "data_output_Tp1 = tf.placeholder(\"float\", [None, num_output_Tp1])\n",
    "data_output_Tp0 = tf.placeholder(\"float\", [None, num_output_Tp0])\n",
    "\n",
    "# layer 1\n",
    "i_01_in = num_innput # 87\n",
    "i_01_out = 100\n",
    "NN_Tp1_w1 = tf.Variable(tf.truncated_normal([i_01_in, i_01_out], stddev=0.0001)) # w = weight\n",
    "NN_Tp1_b1 = tf.Variable(tf.ones([i_01_out]))                                     # b = bias\n",
    "NN_Tp0_w1 = tf.Variable(tf.truncated_normal([i_01_in, i_01_out], stddev=0.0001)) # w = weight\n",
    "NN_Tp0_b1 = tf.Variable(tf.ones([i_01_out]))                                     # b = bias\n",
    "\n",
    "# layer 2\n",
    "i_02_in = i_01_out\n",
    "i_02_out = 25\n",
    "NN_Tp1_w2 = tf.Variable(tf.truncated_normal([i_02_in, i_02_out], stddev=0.0001))\n",
    "NN_Tp1_b2 = tf.Variable(tf.ones([i_02_out]))\n",
    "NN_Tp0_w2 = tf.Variable(tf.truncated_normal([i_02_in, i_02_out], stddev=0.0001))\n",
    "NN_Tp0_b2 = tf.Variable(tf.ones([i_02_out]))\n",
    "\n",
    "# layer 3\n",
    "i_03_in = i_02_out\n",
    "if num_output_Tp1 == num_output_Tp0:\n",
    "  i_03_out = num_output_Tp1\n",
    "else:\n",
    "  i_03_out = 2\n",
    "NN_Tp1_w3 = tf.Variable(tf.truncated_normal([i_03_in, i_03_out], stddev=0.0001))\n",
    "NN_Tp1_b3 = tf.Variable(tf.ones([i_03_out]))\n",
    "NN_Tp0_w3 = tf.Variable(tf.truncated_normal([i_03_in, i_03_out], stddev=0.0001))\n",
    "NN_Tp0_b3 = tf.Variable(tf.ones([i_03_out]))\n",
    "\n",
    "# m02_NN_Tp1\n",
    "NN_Tp1_hidden_layer_0 = data_innput\n",
    "NN_Tp1_hidden_layer_1 = tf.nn.relu(tf.matmul(NN_Tp1_hidden_layer_0, NN_Tp1_w1) + NN_Tp1_b1)\n",
    "NN_Tp1_hidden_layer_2 = tf.nn.relu(tf.matmul(NN_Tp1_hidden_layer_1, NN_Tp1_w2) + NN_Tp1_b2)\n",
    "NN_Tp1_hidden_layer_3 = tf.matmul(           NN_Tp1_hidden_layer_2, NN_Tp1_w3) + NN_Tp1_b3\n",
    "\n",
    "# m02_NN_Tp0\n",
    "NN_Tp0_hidden_layer_0 = data_innput\n",
    "NN_Tp0_hidden_layer_1 = tf.nn.relu(tf.matmul(NN_Tp0_hidden_layer_0, NN_Tp0_w1) + NN_Tp0_b1)\n",
    "NN_Tp0_hidden_layer_2 = tf.nn.relu(tf.matmul(NN_Tp0_hidden_layer_1, NN_Tp0_w2) + NN_Tp0_b2)\n",
    "NN_Tp0_hidden_layer_3 = tf.matmul(           NN_Tp0_hidden_layer_2, NN_Tp0_w3) + NN_Tp0_b3\n",
    "\n",
    "# Define our model...\n",
    "# Here we take a softmax regression of the product of our data_innput and weights.\n",
    "NN_Tp1_model = tf.nn.softmax(NN_Tp1_hidden_layer_3)\n",
    "NN_Tp0_model = tf.nn.softmax(NN_Tp0_hidden_layer_3)\n",
    "\n",
    "# Define a cost function (we're using the cross entropy).\n",
    "NN_Tp1_cost = -tf.reduce_sum(data_output_Tp1 * tf.log(NN_Tp1_model))\n",
    "NN_Tp0_cost = -tf.reduce_sum(data_output_Tp0 * tf.log(NN_Tp0_model))\n",
    "\n",
    "# Define a training operation, or step...\n",
    "# Here we use gradient descent with a learning rate of 0.01 using the cost function we just defined.\n",
    "NN_Tp1_train_op = tf.train.AdamOptimizer(learning_rate=0.0001).minimize(NN_Tp1_cost)\n",
    "NN_Tp0_train_op = tf.train.AdamOptimizer(learning_rate=0.0001).minimize(NN_Tp0_cost)\n",
    "\n",
    "# Init\n",
    "init = tf.initialize_all_variables()\n",
    "# Run\n",
    "sess_m02_NN.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Tp1: BEFORE the model has been run, ie. trained\n",
    "m02_NN_Tp1_b1 = sess_m02_NN.run(NN_Tp1_b1)    \n",
    "m02_NN_Tp1_w1 = sess_m02_NN.run(NN_Tp1_w1)    \n",
    "m02_NN_Tp1_b2 = sess_m02_NN.run(NN_Tp1_b2)    \n",
    "m02_NN_Tp1_w2 = sess_m02_NN.run(NN_Tp1_w2)    \n",
    "m02_NN_Tp1_b3 = sess_m02_NN.run(NN_Tp1_b3) \n",
    "m02_NN_Tp1_w3 = sess_m02_NN.run(NN_Tp1_w3)    \n",
    "# Tp0: BEFORE the model has been run, ie. trained\n",
    "m02_NN_Tp0_b1 = sess_m02_NN.run(NN_Tp0_b1)    \n",
    "m02_NN_Tp0_w1 = sess_m02_NN.run(NN_Tp0_w1)    \n",
    "m02_NN_Tp0_b2 = sess_m02_NN.run(NN_Tp0_b2)    \n",
    "m02_NN_Tp0_w2 = sess_m02_NN.run(NN_Tp0_w2)    \n",
    "m02_NN_Tp0_b3 = sess_m02_NN.run(NN_Tp0_b3) \n",
    "m02_NN_Tp0_w3 = sess_m02_NN.run(NN_Tp0_w3)    \n",
    "\n",
    "# Tp1\n",
    "# HTML(pd.DataFrame(m02_NN_Tp1_b1).transpose().to_html())\n",
    "# HTML(pd.DataFrame(m02_NN_Tp1_w1).to_html())\n",
    "# HTML(pd.DataFrame(m02_NN_Tp1_b2).transpose().to_html())\n",
    "# HTML(pd.DataFrame(m02_NN_Tp1_w2).to_html())\n",
    "# HTML(pd.DataFrame(m02_NN_Tp1_b3).transpose().to_html())\n",
    "# HTML(pd.DataFrame(m02_NN_Tp1_w3).to_html())\n",
    "# Tp0\n",
    "# HTML(pd.DataFrame(m02_NN_Tp0_b1).transpose().to_html())\n",
    "# HTML(pd.DataFrame(m02_NN_Tp0_w1).to_html())\n",
    "# HTML(pd.DataFrame(m02_NN_Tp0_b2).transpose().to_html())\n",
    "# HTML(pd.DataFrame(m02_NN_Tp0_w2).to_html())\n",
    "# HTML(pd.DataFrame(m02_NN_Tp0_b3).transpose().to_html())\n",
    "# HTML(pd.DataFrame(m02_NN_Tp0_w3).to_html())\n",
    "\n",
    "# Tp1\n",
    "# pd.DataFrame(m02_NN_Tp1_b1).transpose()\n",
    "# pd.DataFrame(m02_NN_Tp1_w1)\n",
    "# pd.DataFrame(m02_NN_Tp1_b2).transpose()\n",
    "# pd.DataFrame(m02_NN_Tp1_w2)\n",
    "# pd.DataFrame(m02_NN_Tp1_b3).transpose()\n",
    "# pd.DataFrame(m02_NN_Tp1_w3)\n",
    "# Tp0\n",
    "# pd.DataFrame(m02_NN_Tp0_b1).transpose()\n",
    "# pd.DataFrame(m02_NN_Tp0_w1)\n",
    "# pd.DataFrame(m02_NN_Tp0_b2).transpose()\n",
    "# pd.DataFrame(m02_NN_Tp0_w2)\n",
    "# pd.DataFrame(m02_NN_Tp0_b3).transpose()\n",
    "# pd.DataFrame(m02_NN_Tp0_w3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, you'll train the model over 10,000 iterations using the full dataset each time. Every thousandth iteration, you'll assess the accuracy of the model on the training data to assess progress."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## m02_NN: On Data_Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " m02_NN_Tp1_1000_0.583796; m02_NN_Tp0_1000_0.631764;\n",
      " m02_NN_Tp1_2000_0.717754; m02_NN_Tp0_2000_0.740567;\n",
      " m02_NN_Tp1_3000_0.905235; m02_NN_Tp0_3000_0.869845;\n",
      " m02_NN_Tp1_4000_0.954958; m02_NN_Tp0_4000_0.937409;\n",
      " m02_NN_Tp1_5000_0.969582; m02_NN_Tp0_5000_0.964317;\n"
     ]
    }
   ],
   "source": [
    "NN_Tp1_correct_prediction = tf.equal(tf.argmax(NN_Tp1_model, 1), tf.argmax(data_output_Tp1, 1))\n",
    "NN_Tp0_correct_prediction = tf.equal(tf.argmax(NN_Tp0_model, 1), tf.argmax(data_output_Tp0, 1))\n",
    "\n",
    "NN_Tp1_accuracy = tf.reduce_mean(tf.cast(NN_Tp1_correct_prediction, \"float\"))\n",
    "NN_Tp0_accuracy = tf.reduce_mean(tf.cast(NN_Tp0_correct_prediction, \"float\"))\n",
    "\n",
    "num_iterations = 5001\n",
    "num_iterations_until_display = 1000\n",
    "\n",
    "for i in range(1, num_iterations):\n",
    "  s = ''\n",
    "  \n",
    "  # Tp1 ==================================================\n",
    "  sess_m02_NN.run(\n",
    "    NN_Tp1_train_op, \n",
    "    feed_dict={\n",
    "      data_innput:     tf_innput_trn.values, \n",
    "      data_output_Tp1: tf_output_trn_Tp1.values.reshape(len(tf_output_trn_Tp1.values), 2)\n",
    "    }\n",
    "  )\n",
    "  if i%num_iterations_until_display == 0:\n",
    "    s = s + ' ' + 'm02_NN'\n",
    "    s = s + '_' + 'Tp1'\n",
    "    s = s + '_' + str(i) \n",
    "    s = s + '_' + str(sess_m02_NN.run(\n",
    "      NN_Tp1_accuracy,\n",
    "      feed_dict={\n",
    "        data_innput:     tf_innput_trn.values, \n",
    "        data_output_Tp1: tf_output_trn_Tp1.values.reshape(len(tf_output_trn_Tp1.values), 2)\n",
    "      }\n",
    "    ))\n",
    "    s = s + ';'\n",
    "\n",
    "  # Tp0 ==================================================\n",
    "  sess_m02_NN.run(\n",
    "    NN_Tp0_train_op, \n",
    "    feed_dict={\n",
    "      data_innput:     tf_innput_trn.values, \n",
    "      data_output_Tp0: tf_output_trn_Tp0.values.reshape(len(tf_output_trn_Tp0.values), 2)\n",
    "    }\n",
    "  )\n",
    "  if i%num_iterations_until_display == 0:\n",
    "    s = s + ' ' + 'm02_NN'\n",
    "    s = s + '_' + 'Tp0'\n",
    "    s = s + '_' + str(i) \n",
    "    s = s + '_' + str(sess_m02_NN.run(\n",
    "      NN_Tp0_accuracy,\n",
    "      feed_dict={\n",
    "        data_innput:     tf_innput_trn.values, \n",
    "        data_output_Tp0: tf_output_trn_Tp0.values.reshape(len(tf_output_trn_Tp0.values), 2)\n",
    "      }\n",
    "    ))\n",
    "    s = s + ';'\n",
    "    \n",
    "    print s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## m02_NN: On Data_Training: Archive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Original\n",
    "# 1000 0.676126 # 2000 0.768578 # 3000 0.83031 # 4000 0.891164 # 5000 0.93739# 6000 0.956115 # 7000 0.972206 # 8000 0.978935 # 9000 0.985079 # 10000 0.987127\n",
    "# Original\n",
    "# 1000 0.685489 # 2000 0.777063 # 3000 0.84055 # 4000 0.902867 # 5000 0.941486 # 6000 0.957578 # 7000 0.974254 # 8000 0.980105 # 9000 0.985079 # 10000 0.987712\n",
    "# 87 to 2\n",
    "# 1000 0.595963 # 2000 0.731714 # 3000 0.870392 # 4000 0.928613 # 5000 0.9567\n",
    "# ======================================================\n",
    "# 87 inputs = X(t+1), X(t), v1, .., v85\n",
    "# ======================================================\n",
    "#  m02_NN_Tp1_1000_0.583796; m02_NN_Tp0_1000_0.631764;\n",
    "#  m02_NN_Tp1_2000_0.717754; m02_NN_Tp0_2000_0.740567;\n",
    "#  m02_NN_Tp1_3000_0.905235; m02_NN_Tp0_3000_0.869845;\n",
    "#  m02_NN_Tp1_4000_0.954958; m02_NN_Tp0_4000_0.937409;\n",
    "#  m02_NN_Tp1_5000_0.969582; m02_NN_Tp0_5000_0.964317;\n",
    "# ======================================================\n",
    "# ======================================================\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A significant improvement in accuracy with the training data shows that the hidden layers are adding additional capacity for learning to the model.\n",
    "\n",
    "Looking at precision, recall, and accuracy, you can see a measurable improvement in performance, but certainly not a [step function](https://wikipedia.org/wiki/Step_function). This indicates that we're likely reaching the limits of this relatively simple feature set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## m02_NN: On Data_Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== m02_NN_Tp1: Performance on Test Data =====\n",
      "Precision =  0.893719806763\n",
      "Recall =  0.909090909091\n",
      "F1 Score =  0.901339829476\n",
      "Accuracy =  0.905152224824\n",
      "===== m02_NN_Tp0: Performance on Test Data =====\n",
      "Precision =  0.844660194175\n",
      "Recall =  0.855036855037\n",
      "F1 Score =  0.849816849817\n",
      "Accuracy =  0.855971896956\n"
     ]
    }
   ],
   "source": [
    "print '===== m02_NN_Tp1: Performance on Test Data ====='\n",
    "feed_dict= {\n",
    "  data_innput:     tf_innput_tst.values,\n",
    "  data_output_Tp1: tf_output_tst_Tp1.values.reshape(len(tf_output_tst_Tp1.values), 2)\n",
    "}\n",
    "tf_confusion_metrics(NN_Tp1_model, data_output_Tp1, sess_m02_NN, feed_dict)\n",
    "\n",
    "print '===== m02_NN_Tp0: Performance on Test Data ====='\n",
    "feed_dict= {\n",
    "  data_innput:     tf_innput_tst.values,\n",
    "  data_output_Tp0: tf_output_tst_Tp0.values.reshape(len(tf_output_tst_Tp0.values), 2)\n",
    "}\n",
    "tf_confusion_metrics(NN_Tp0_model, data_output_Tp0, sess_m02_NN, feed_dict)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## m02_NN: On Data_Testing: Archive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Original\n",
    "# Precision =  0.934731934732\n",
    "# Recall =  0.968599033816\n",
    "# F1 Score =  0.951364175563\n",
    "# Accuracy =  0.952046783626\n",
    "\n",
    "# 87:2 Tp1\n",
    "# Precision =  0.857446808511\n",
    "# Recall =  0.973429951691\n",
    "# F1 Score =  0.911764705882\n",
    "# Accuracy =  0.908771929825\n",
    "\n",
    "# ======================================================\n",
    "# 87 inputs = X(t+1), X(t), v1, .., v85\n",
    "# ======================================================\n",
    "# ===== m02_NN_Tp1: Performance on Test Data =====\n",
    "# Precision =  0.893719806763\n",
    "# Recall =  0.909090909091\n",
    "# F1 Score =  0.901339829476\n",
    "# Accuracy =  0.905152224824\n",
    "# ===== m02_NN_Tp0: Performance on Test Data =====\n",
    "# Precision =  0.844660194175\n",
    "# Recall =  0.855036855037\n",
    "# F1 Score =  0.849816849817\n",
    "# Accuracy =  0.855971896956\n",
    "# ======================================================\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ======================================================\n",
    "# 87 inputs = X(t+1), X(t), v1, .., v85\n",
    "# ======================================================\n",
    "# m01_BC: on data_tst: Tp1.accu = 53% | Tp0.accu = 56%\n",
    "# m01_BC: on data_trn: Tp1.accu = 62% | Tp0.accu = 65%\n",
    "# m02_NN: on data_tst: Tp1.accu = 90% | Tp0.accu = 85%\n",
    "# m02_NN: on data_trn: Tp1.accu = 97% | Tp0.accu = 96%\n",
    "\n",
    "# ======================================================\n",
    "# 86 inputs =       X(t), v1, .., v85\n",
    "# ======================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Tp1: AFTER the model has been run, ie. trained\n",
    "m02_NN_Tp1_b1 = sess_m02_NN.run(NN_Tp1_b1)    \n",
    "m02_NN_Tp1_w1 = sess_m02_NN.run(NN_Tp1_w1)    \n",
    "m02_NN_Tp1_b2 = sess_m02_NN.run(NN_Tp1_b2)    \n",
    "m02_NN_Tp1_w2 = sess_m02_NN.run(NN_Tp1_w2)    \n",
    "m02_NN_Tp1_b3 = sess_m02_NN.run(NN_Tp1_b3) \n",
    "m02_NN_Tp1_w3 = sess_m02_NN.run(NN_Tp1_w3)    \n",
    "# Tp0: AFTER the model has been run, ie. trained\n",
    "m02_NN_Tp0_b1 = sess_m02_NN.run(NN_Tp0_b1)    \n",
    "m02_NN_Tp0_w1 = sess_m02_NN.run(NN_Tp0_w1)    \n",
    "m02_NN_Tp0_b2 = sess_m02_NN.run(NN_Tp0_b2)    \n",
    "m02_NN_Tp0_w2 = sess_m02_NN.run(NN_Tp0_w2)    \n",
    "m02_NN_Tp0_b3 = sess_m02_NN.run(NN_Tp0_b3) \n",
    "m02_NN_Tp0_w3 = sess_m02_NN.run(NN_Tp0_w3)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uploaded: {u'icon': u'page_white_excel', u'bytes': 1765, u'thumb_exists': False, u'rev': u'604a1e9d47', u'modified': u'Wed, 24 Aug 2016 10:52:20 +0000', u'shareable': False, u'client_mtime': u'Wed, 24 Aug 2016 10:52:20 +0000', u'path': u'/m02_NN_Tp1_b1.csv', u'is_dir': False, u'size': u'1.7 KB', u'root': u'dropbox', u'mime_type': u'text/csv', u'revision': 96}\n",
      "uploaded: {u'icon': u'page_white_excel', u'bytes': 143369, u'thumb_exists': False, u'rev': u'614a1e9d47', u'modified': u'Wed, 24 Aug 2016 10:52:21 +0000', u'shareable': False, u'client_mtime': u'Wed, 24 Aug 2016 10:52:21 +0000', u'path': u'/m02_NN_Tp1_w1.csv', u'is_dir': False, u'size': u'140 KB', u'root': u'dropbox', u'mime_type': u'text/csv', u'revision': 97}\n",
      "uploaded: {u'icon': u'page_white_excel', u'bytes': 421, u'thumb_exists': False, u'rev': u'624a1e9d47', u'modified': u'Wed, 24 Aug 2016 10:52:21 +0000', u'shareable': False, u'client_mtime': u'Wed, 24 Aug 2016 10:52:21 +0000', u'path': u'/m02_NN_Tp1_b2.csv', u'is_dir': False, u'size': u'421 bytes', u'root': u'dropbox', u'mime_type': u'text/csv', u'revision': 98}\n",
      "uploaded: {u'icon': u'page_white_excel', u'bytes': 39019, u'thumb_exists': False, u'rev': u'634a1e9d47', u'modified': u'Wed, 24 Aug 2016 10:52:22 +0000', u'shareable': False, u'client_mtime': u'Wed, 24 Aug 2016 10:52:22 +0000', u'path': u'/m02_NN_Tp1_w2.csv', u'is_dir': False, u'size': u'38.1 KB', u'root': u'dropbox', u'mime_type': u'text/csv', u'revision': 99}\n",
      "uploaded: {u'icon': u'page_white_excel', u'bytes': 36, u'thumb_exists': False, u'rev': u'644a1e9d47', u'modified': u'Wed, 24 Aug 2016 10:52:23 +0000', u'shareable': False, u'client_mtime': u'Wed, 24 Aug 2016 10:52:22 +0000', u'path': u'/m02_NN_Tp1_b3.csv', u'is_dir': False, u'size': u'36 bytes', u'root': u'dropbox', u'mime_type': u'text/csv', u'revision': 100}\n",
      "uploaded: {u'icon': u'page_white_excel', u'bytes': 837, u'thumb_exists': False, u'rev': u'654a1e9d47', u'modified': u'Wed, 24 Aug 2016 10:52:23 +0000', u'shareable': False, u'client_mtime': u'Wed, 24 Aug 2016 10:52:23 +0000', u'path': u'/m02_NN_Tp1_w3.csv', u'is_dir': False, u'size': u'837 bytes', u'root': u'dropbox', u'mime_type': u'text/csv', u'revision': 101}\n",
      "uploaded: {u'icon': u'page_white_excel', u'bytes': 1748, u'thumb_exists': False, u'rev': u'664a1e9d47', u'modified': u'Wed, 24 Aug 2016 10:52:24 +0000', u'shareable': False, u'client_mtime': u'Wed, 24 Aug 2016 10:52:24 +0000', u'path': u'/m02_NN_Tp0_b1.csv', u'is_dir': False, u'size': u'1.7 KB', u'root': u'dropbox', u'mime_type': u'text/csv', u'revision': 102}\n",
      "uploaded: {u'icon': u'page_white_excel', u'bytes': 138080, u'thumb_exists': False, u'rev': u'674a1e9d47', u'modified': u'Wed, 24 Aug 2016 10:52:25 +0000', u'shareable': False, u'client_mtime': u'Wed, 24 Aug 2016 10:52:24 +0000', u'path': u'/m02_NN_Tp0_w1.csv', u'is_dir': False, u'size': u'134.8 KB', u'root': u'dropbox', u'mime_type': u'text/csv', u'revision': 103}\n",
      "uploaded: {u'icon': u'page_white_excel', u'bytes': 421, u'thumb_exists': False, u'rev': u'684a1e9d47', u'modified': u'Wed, 24 Aug 2016 10:52:25 +0000', u'shareable': False, u'client_mtime': u'Wed, 24 Aug 2016 10:52:25 +0000', u'path': u'/m02_NN_Tp0_b2.csv', u'is_dir': False, u'size': u'421 bytes', u'root': u'dropbox', u'mime_type': u'text/csv', u'revision': 104}\n",
      "uploaded: {u'icon': u'page_white_excel', u'bytes': 39008, u'thumb_exists': False, u'rev': u'694a1e9d47', u'modified': u'Wed, 24 Aug 2016 10:52:26 +0000', u'shareable': False, u'client_mtime': u'Wed, 24 Aug 2016 10:52:26 +0000', u'path': u'/m02_NN_Tp0_w2.csv', u'is_dir': False, u'size': u'38.1 KB', u'root': u'dropbox', u'mime_type': u'text/csv', u'revision': 105}\n",
      "uploaded: {u'icon': u'page_white_excel', u'bytes': 36, u'thumb_exists': False, u'rev': u'6a4a1e9d47', u'modified': u'Wed, 24 Aug 2016 10:52:27 +0000', u'shareable': False, u'client_mtime': u'Wed, 24 Aug 2016 10:52:27 +0000', u'path': u'/m02_NN_Tp0_b3.csv', u'is_dir': False, u'size': u'36 bytes', u'root': u'dropbox', u'mime_type': u'text/csv', u'revision': 106}\n",
      "uploaded: {u'icon': u'page_white_excel', u'bytes': 840, u'thumb_exists': False, u'rev': u'6b4a1e9d47', u'modified': u'Wed, 24 Aug 2016 10:52:28 +0000', u'shareable': False, u'client_mtime': u'Wed, 24 Aug 2016 10:52:27 +0000', u'path': u'/m02_NN_Tp0_w3.csv', u'is_dir': False, u'size': u'840 bytes', u'root': u'dropbox', u'mime_type': u'text/csv', u'revision': 107}\n"
     ]
    }
   ],
   "source": [
    "# response = client.put_file('/output.csv', str(pd.DataFrame(npa).to_csv()), overwrite=True)\n",
    "# response = client.put_file('/output.csv', str(               df.to_csv()), overwrite=True)\n",
    "\n",
    "# Tp1\n",
    "response = client.put_file('/m02_NN_Tp1_b1.csv', str(pd.DataFrame(m02_NN_Tp1_b1).to_csv()), overwrite=True); print \"uploaded:\", response\n",
    "response = client.put_file('/m02_NN_Tp1_w1.csv', str(pd.DataFrame(m02_NN_Tp1_w1).to_csv()), overwrite=True); print \"uploaded:\", response\n",
    "response = client.put_file('/m02_NN_Tp1_b2.csv', str(pd.DataFrame(m02_NN_Tp1_b2).to_csv()), overwrite=True); print \"uploaded:\", response\n",
    "response = client.put_file('/m02_NN_Tp1_w2.csv', str(pd.DataFrame(m02_NN_Tp1_w2).to_csv()), overwrite=True); print \"uploaded:\", response\n",
    "response = client.put_file('/m02_NN_Tp1_b3.csv', str(pd.DataFrame(m02_NN_Tp1_b3).to_csv()), overwrite=True); print \"uploaded:\", response\n",
    "response = client.put_file('/m02_NN_Tp1_w3.csv', str(pd.DataFrame(m02_NN_Tp1_w3).to_csv()), overwrite=True); print \"uploaded:\", response\n",
    "# Tp0\n",
    "response = client.put_file('/m02_NN_Tp0_b1.csv', str(pd.DataFrame(m02_NN_Tp0_b1).to_csv()), overwrite=True); print \"uploaded:\", response\n",
    "response = client.put_file('/m02_NN_Tp0_w1.csv', str(pd.DataFrame(m02_NN_Tp0_w1).to_csv()), overwrite=True); print \"uploaded:\", response\n",
    "response = client.put_file('/m02_NN_Tp0_b2.csv', str(pd.DataFrame(m02_NN_Tp0_b2).to_csv()), overwrite=True); print \"uploaded:\", response\n",
    "response = client.put_file('/m02_NN_Tp0_w2.csv', str(pd.DataFrame(m02_NN_Tp0_w2).to_csv()), overwrite=True); print \"uploaded:\", response\n",
    "response = client.put_file('/m02_NN_Tp0_b3.csv', str(pd.DataFrame(m02_NN_Tp0_b3).to_csv()), overwrite=True); print \"uploaded:\", response\n",
    "response = client.put_file('/m02_NN_Tp0_w3.csv', str(pd.DataFrame(m02_NN_Tp0_w3).to_csv()), overwrite=True); print \"uploaded:\", response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You've covered a lot of ground. You moved from sourcing five years of financial time-series data, to munging that data into a more suitable form. You explored and visualized that data with exploratory data analysis and then decided on a machine learning model and the features for that model. You engineered those features, built a binary classifier in TensorFlow, and analyzed its performance. You built a feed forward neural net with two hidden layers in TensorFlow and analyzed its performance.\n",
    "\n",
    "How did the technology fare? It should take most people 1.5 to 3 hours to extract the juice from this solution, and none of that time is spent waiting for infrastructure or software; it's spent reading and thinking. In many organizations, it can take anywhere from days to months to do this sort of data analysis, depending on whether you need to procure any hardware. And you didn't need to do anything with infrastructure or additional software. Rather, you used a web-based console to direct GCP to set up systems on your behalf, which it did—fully managed, maintained, and supported—freeing you up to spend your time analyzing. \n",
    "\n",
    "It was also cost effective. If you took your time with this solution and spent three hours to go through it, the cost would be a few pennies. \n",
    "\n",
    "Cloud Datalab worked admirably, too. iPython/Jupyter has always been a great platform for interactive, iterative work and a fully-managed version of that platform on GCP, with connectors to other GCP technologies such as BigQuery and Google Cloud Storage, is a force multiplier for your analysis needs.  If you haven't used iPython before, this solution might have been eye opening, for you. If you're already familiar with iPython, then you'll love the connectors to other GCP technologies.\n",
    "\n",
    "Of course, R and Matlab are popular tools in machine learning, and we've made no mention either in this solution. Neither R nor Matlab are available as managed services on GCP. Both can be hosted in GCP and accessed through a cloud-friendly, web frontend.\n",
    "\n",
    "TensorFlow is a special piece of technology. It is expressive, performs well, and comes with the weight of Google's machine learning history and expertise to back it up and support it. We've only scratched the surface, but you can already see that within a handful of lines of code we've been able to write two models. Neither of them is cutting edge, by design, but neither of them is trivial either. With some additional tuning they would suit a whole spectrum of machine learning tasks. \n",
    "\n",
    "Finally, how did we do with the data analysis? We did well: over 70% accuracy in predicting the close of the S&P 500 is the highest we've seen achieved on this dataset, so with few steps and a few lines of code we've produced a full-on machine learning model. The reason for the relatively modest accuracy achieved is the dataset itself; there isn't enough signal there to do significantly better. But 7 times out of 10, we were able to correctly determine if the S&P 500 index would close up or down on the day, and that's objectively good."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**When you're finished, shut down the managed VM you used for Cloud Datalab to avoid incurring costs.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
